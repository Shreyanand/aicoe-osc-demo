{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0734a0d8-819e-4058-9723-179f0fbcf910",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Inference with transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae0cb087-0340-45cf-94fd-5eff01f3e516",
   "metadata": {
    "papermill": {
     "duration": 3.30074,
     "end_time": "2022-10-07T19:33:18.885511",
     "exception": false,
     "start_time": "2022-10-07T19:33:15.584771",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "from dotenv import load_dotenv\n",
    "from datasets import Dataset, DatasetDict\n",
    "import pandas as pd\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from src.data.s3_communication import S3Communication, S3FileType\n",
    "from src.components.utils.kpi_mapping import get_kpi_mapping_category\n",
    "import json\n",
    "import time\n",
    "import config\n",
    "from transformers import AutoTokenizer\n",
    "from torch import cuda\n",
    "import torch\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "local_model_path = '/opt/app-root/src/aicoe-osc-demo/models/transformers/RELEVANCE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2f3fab7-c574-4a9a-b1b8-fa2ae11bb104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load credentials\n",
    "dotenv_dir = os.environ.get(\n",
    "    \"CREDENTIAL_DOTENV_DIR\", os.environ.get(\"PWD\", \"/opt/app-root/src\")\n",
    ")\n",
    "dotenv_path = pathlib.Path(dotenv_dir) / \"credentials.env\"\n",
    "if os.path.exists(dotenv_path):\n",
    "    load_dotenv(dotenv_path=dotenv_path, override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffc914d3-f48d-4936-91df-6425115cacbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init s3 connector\n",
    "s3c = S3Communication(\n",
    "    s3_endpoint_url=os.getenv(\"S3_ENDPOINT\"),\n",
    "    aws_access_key_id=os.getenv(\"AWS_ACCESS_KEY_ID\"),\n",
    "    aws_secret_access_key=os.getenv(\"AWS_SECRET_ACCESS_KEY\"),\n",
    "    s3_bucket=os.getenv(\"S3_BUCKET\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee555c33-c823-447a-9774-6a3cf2a6241b",
   "metadata": {
    "papermill": {
     "duration": 0.003808,
     "end_time": "2022-10-07T19:33:19.004776",
     "exception": false,
     "start_time": "2022-10-07T19:33:19.000968",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Retrieve the test dataset and the trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bea6125-906c-4e8b-bd90-73021dad8195",
   "metadata": {
    "papermill": {
     "duration": 0.886377,
     "end_time": "2022-10-07T19:33:19.895045",
     "exception": false,
     "start_time": "2022-10-07T19:33:19.008668",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3c.download_files_in_prefix_to_dir(\n",
    "    config.BASE_TRAIN_TEST_DATASET_S3_PREFIX,\n",
    "    config.BASE_PROCESSED_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4718ed6-3be9-4f1d-8aa6-fb3d2f8123e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_path = str(config.BASE_PROCESSED_DATA)+'/rel_test_split.csv'\n",
    "test_data = pd.read_csv(test_data_path, index_col=0)\n",
    "test_data.rename(columns={'text': 'question', 'text_b':'sentence'}, inplace=True)\n",
    "\n",
    "train_data_path = str(config.BASE_PROCESSED_DATA)+'/rel_train_split.csv'\n",
    "train_data = pd.read_csv(train_data_path, index_col=0)\n",
    "train_data.rename(columns={'text': 'question', 'text_b':'sentence'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b7d2fa3-6d7d-4037-be15-a5a07adb96a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "trds = Dataset.from_pandas(train_data)\n",
    "teds = Dataset.from_pandas(test_data.drop('label', axis=1))\n",
    "\n",
    "climate_dataset = DatasetDict()\n",
    "\n",
    "climate_dataset['train'] = trds\n",
    "climate_dataset['test'] = teds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c55a45ca-a774-485e-85ab-5902e4a24107",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(local_model_path, use_fast=True)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(local_model_path).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49bea969-d101-47fc-b29a-0a8f42b3e43a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_batches(data_df, batch_size=32):\n",
    "    encoded_dataset = list()\n",
    "    batch = list()\n",
    "    for df, row in data_df.iterrows():\n",
    "        if len(batch) < batch_size:\n",
    "            batch.append([row['question'], row['sentence']])\n",
    "        else:\n",
    "            encoded_dataset.append(tokenizer(batch,\n",
    "                                             truncation=True,\n",
    "                                             return_tensors='pt',\n",
    "                                             padding=True))\n",
    "            batch = [[row['question'], row['sentence']]]\n",
    "\n",
    "    if batch:\n",
    "        encoded_dataset.append(tokenizer(batch,\n",
    "                                         truncation=True,\n",
    "                                         return_tensors='pt',\n",
    "                                         padding=True))\n",
    "    return encoded_dataset\n",
    "\n",
    "\n",
    "encoded_dataset = create_batches(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7fd14ef3-5f8d-4d31-a679-337f2450aab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(encoded_dataset):\n",
    "    outputs = list()\n",
    "    for batch in encoded_dataset:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        with torch.no_grad():\n",
    "            outs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            outputs.extend(outs.logits.argmax(axis=1).tolist())\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e03c31-8aac-436c-905c-85d00bcaa64c",
   "metadata": {},
   "source": [
    "# Infer for all pdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c93d7d5-e4fe-40e1-8e20-5a46331e682c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_data(pdf_name, pdf_path):\n",
    "    pdf_content = read_text_from_json(file_path)\n",
    "    text_data = []\n",
    "    # Build all possible combinations of paragraphs and  questions\n",
    "    # Keep track of page number which the text is extracted from and\n",
    "    # the pdf it belongs to.\n",
    "    for kpi_question in questions:\n",
    "        text_data.extend([{\n",
    "            \"page\": page_num,\n",
    "            \"pdf_name\": pdf_name,\n",
    "            \"question\": kpi_question,\n",
    "            \"sentence\": paragraph}\n",
    "            for page_num, page_content in pdf_content.items()\n",
    "            for paragraph in page_content])\n",
    "    return text_data\n",
    "\n",
    "\n",
    "def read_text_from_json(file):\n",
    "    \"\"\"Read text from json.\"\"\"\n",
    "\n",
    "    with open(file) as f:\n",
    "        text = json.load(f)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c2eaf42-4a8f-4eba-995b-94f4aa862dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "BENCHMARK_FOLDER = config.DATA_FOLDER / \"benchmark\"\n",
    "if not os.path.exists(BENCHMARK_FOLDER):\n",
    "    BENCHMARK_FOLDER.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "BENCHMARK_EXTRACTION_FOLDER = BENCHMARK_FOLDER / \"extraction\"\n",
    "if not os.path.exists(BENCHMARK_EXTRACTION_FOLDER):\n",
    "    pathlib.Path(BENCHMARK_EXTRACTION_FOLDER).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7098b9-2c9b-4128-8c92-b057c2205596",
   "metadata": {},
   "outputs": [],
   "source": [
    "kpi_df = s3c.download_df_from_s3(\n",
    "    \"aicoe-osc-demo/kpi_mapping\",\n",
    "    \"kpi_mapping.csv\",\n",
    "    filetype=S3FileType.CSV,\n",
    "    header=0)\n",
    "\n",
    "kmc = get_kpi_mapping_category(kpi_df)\n",
    "questions = [q_text for q_id, (q_text, sect) in kmc[\"KPI_MAPPING_MODEL\"].items()\n",
    "             if len(set(sect).intersection({\"OG\", \"CM\", \"CU\"})) > 0\n",
    "             and \"TEXT\" in kmc[\"KPI_CATEGORY\"][q_id]]\n",
    "\n",
    "text_paths = sorted(BENCHMARK_EXTRACTION_FOLDER.rglob(\"*.json\"))\n",
    "all_text_path_dict = {os.path.splitext(os.path.basename(file_path))[0]:\n",
    "                      file_path for file_path in text_paths\n",
    "                      if \"table_meta\" not in str(file_path)}\n",
    "\n",
    "df_list = []\n",
    "metrics_df_list = []\n",
    "num_pdfs = len(all_text_path_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3b929d-8205-4064-b72a-4a6407e23ce7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 0/144, 04_NOVATEK_AR_2016_ENG_11\n",
      "Processing 1/144, 04_NOVATEK_AR_2018_ENG_15\n",
      "Processing 2/144, 2013_book_mol_ar_eng_fin\n",
      "Processing 3/144, 2015_BASF_Report\n",
      "Processing 4/144, 2017 Sustainability Report\n",
      "Processing 5/144, 2017-Sustainability-Report\n",
      "Processing 6/144, 2017_SustainabilityReport_2_9_Web\n",
      "Processing 7/144, 2017_sustainability_report\n",
      "Processing 8/144, 2017_sustainability_report_tcm14-130393\n",
      "Processing 9/144, 2018 Annual Report\n",
      "Processing 10/144, 2018_sustainability_report\n",
      "Processing 11/144, 2019 Annual Report\n",
      "Processing 12/144, 2019_global_sustainability_plan_tcm14-148662\n",
      "Processing 13/144, 28022019-Repsol-Annual-Financial-Report-2018_tcm14-147383\n",
      "Processing 14/144, 2_LOTOS_Group Directors Report 2019\n",
      "Processing 15/144, AGL Energy Ltd Annual Report 2019\n",
      "Processing 16/144, AGL Energy Ltd FY19 Carbon Scenario Analysis\n",
      "Processing 17/144, AKERBP-Annual-Report-2016\n",
      "Processing 18/144, AKERBP-Annual-Report-2017\n",
      "Processing 19/144, ANNUAL REPORT 2017\n",
      "Processing 20/144, AR_FS_2017_ENG\n",
      "Processing 21/144, Adani Group Adani Enterprises Annual Report 2019\n",
      "Processing 22/144, Adani Group Adani Power Annual Report 2019\n",
      "Processing 23/144, Aker-BP-Annual-report-2018\n",
      "Processing 24/144, Aker-BP-Sustainability-Report-2018-1\n",
      "Processing 25/144, Aker-BP-Sustainability-Report-2019\n",
      "Processing 26/144, Aker-BP-Sustainability-report-2017\n",
      "Processing 27/144, Ameren Corporation Climate Risk Report 2019\n",
      "Processing 28/144, Ameren Corporation Sustainability Template 2019\n",
      "Processing 29/144, Ameren_2019_Annual_Report\n"
     ]
    }
   ],
   "source": [
    "for i, (pdf_name, file_path) in enumerate(all_text_path_dict.items()):\n",
    "    print(f'Processing {i}/{len(all_text_path_dict)}, {pdf_name}')\n",
    "    data = gather_data(pdf_name, file_path)\n",
    "    num_data_points = len(data)\n",
    "    num_pages = data[len(data)-1]['page']\n",
    "    chunk_size = 1000\n",
    "    chunk_idx = 0\n",
    "    total_file_time = 0\n",
    "\n",
    "    predictions = list()\n",
    "    while chunk_idx * chunk_size < num_data_points:\n",
    "        chunk_start = time.time()\n",
    "        data_chunk = data[chunk_idx * chunk_size:(chunk_idx + 1) * chunk_size]\n",
    "        temp_df = pd.DataFrame(data_chunk).drop(['pdf_name', 'page'], axis=1)\n",
    "        encoded_dataset = create_batches(temp_df,\n",
    "                                         batch_size=128)\n",
    "        predictions.extend(predict(encoded_dataset))\n",
    "        chunk_idx += 1\n",
    "\n",
    "        chunk_end = time.time()\n",
    "        total_file_time += (chunk_end - chunk_start)\n",
    "\n",
    "    time_per_data_point = total_file_time / num_data_points\n",
    "    data_points_per_sec = 1/time_per_data_point\n",
    "\n",
    "    metrics_list = [\n",
    "        [pdf_name,\n",
    "         int(num_pages),\n",
    "         num_data_points,\n",
    "         total_file_time,\n",
    "         time_per_data_point,\n",
    "         data_points_per_sec]]\n",
    "\n",
    "    metrics_df = pd.DataFrame(\n",
    "        metrics_list, columns=['PDF Name', 'Number of Pages', 'Number of Data Points',\n",
    "                               'Total Inference Time', 'Time per data point', 'Data points per sec'])\n",
    "    metrics_df_list.append(metrics_df)\n",
    "\n",
    "\n",
    "concatenated_dfs = pd.concat(df_list) if len(df_list) > 0 else pd.DataFrame()\n",
    "metrics_df = pd.DataFrame()\n",
    "if len(metrics_df_list) > 0:\n",
    "    metrics_df = pd.concat(metrics_df_list) if len(metrics_df_list) > 0 else pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78238496-1d06-4319-bf79-b6651058a849",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df.to_pickle('../../reports/benchmarks/distilbert_relevance.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "185092cb-413d-4e7b-b079-32e44ebf0b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df = pd.read_pickle('../../reports/benchmarks/distilbert_relevance.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cd395f16-7a26-4fba-be63-e6f6a962c748",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    144.000000\n",
       "mean       0.002231\n",
       "std        0.000903\n",
       "min        0.000556\n",
       "25%        0.001702\n",
       "50%        0.002161\n",
       "75%        0.002694\n",
       "max        0.004880\n",
       "Name: Time per data point, dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_df['Time per data point'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f636ee0c-89cb-4db7-a906-5bcbbdf20d0f",
   "metadata": {},
   "source": [
    "The average time per data point is 0.002231 seconds. A pdf with on average 157 pages, and 360 data points per page, will take 125 seconds or 2.1min to execute that is almost 3 times faster than farm model that takes 6.5mins for the same task."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
