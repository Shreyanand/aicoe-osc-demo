{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "484782ad-a76f-44f4-8be3-b2a9e5a97088",
   "metadata": {},
   "source": [
    "# Model Benchmarks\n",
    "This notebook benchmarks the models used for inferring relevant texts from the PDFs and determining exact answer to the KPI questions from the relevant texts. It works by downloading the models and data from inferencing in the scratch directories under the `ROOT/data/benchmark` folder and running the inferencer on that data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25054789-3a79-4d61-b2c3-31b9f07335eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import shutil\n",
    "import pathlib\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b9dd11e-8709-4e93-97f9-c7c80f528352",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09/27/2022 18:42:21 - INFO - farm.modeling.prediction_head -   Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import config\n",
    "import logging\n",
    "from config_farm_train import InferConfig\n",
    "from src.data.s3_communication import S3Communication, S3FileType\n",
    "from src.models.text_kpi_infer import TextKPIInfer, aggregate_result\n",
    "from src.models.relevance_infer import TextRelevanceInfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47192225-f899-4565-b807-b7949174cabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.components.utils.kpi_mapping import get_kpi_mapping_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8f47b11-76dd-47aa-b5b1-8b28157fb39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from collections import defaultdict\n",
    "from config_qa_farm_train import QAFileConfig, QAInferConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef85257c-4eae-46ad-8e6d-be36a1c1457e",
   "metadata": {},
   "source": [
    "## 1. Setup for benchmark runs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70445754-6b25-449d-95ef-e874934e1fa7",
   "metadata": {},
   "source": [
    "### 1.1. Load S3 credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ec3c193-d14b-4338-827d-20a47153a00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load credentials\n",
    "dotenv_dir = os.environ.get(\n",
    "    \"CREDENTIAL_DOTENV_DIR\", os.environ.get(\"PWD\", \"/opt/app-root/src\"))\n",
    "dotenv_path = pathlib.Path(dotenv_dir) / \"credentials.env\"\n",
    "if os.path.exists(dotenv_path):\n",
    "    load_dotenv(dotenv_path=dotenv_path, override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d37df1-d489-452e-a7a7-61921fc7c0ed",
   "metadata": {},
   "source": [
    "### 1.2. Creating S3 connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44b4a612-75ac-4159-8cae-9a7154e38a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3c = S3Communication(\n",
    "    s3_endpoint_url=os.getenv(\"S3_ENDPOINT\"),\n",
    "    aws_access_key_id=os.getenv(\"AWS_ACCESS_KEY_ID\"),\n",
    "    aws_secret_access_key=os.getenv(\"AWS_SECRET_ACCESS_KEY\"),\n",
    "    s3_bucket=os.getenv(\"S3_BUCKET\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614f05c6-5f32-4d57-9954-cdcb731d6b3b",
   "metadata": {},
   "source": [
    "### 1.3. Base Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "26fd21b9-b3f5-40aa-b202-d5ee5794e217",
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_config = InferConfig(\"infer_demo\")\n",
    "_logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd10c8e5-66d7-4015-beb8-3389a7107f32",
   "metadata": {},
   "source": [
    "### 1.4. Downloading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68abca80-b77f-4889-a9cc-6583bef0ac74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the pretrained model\n",
    "model_root = pathlib.Path(infer_config.load_dir['Text']).parent\n",
    "model_rel_zip = pathlib.Path(model_root, \"relevance_roberta.zip\")\n",
    "\n",
    "s3c.download_file_from_s3(\n",
    "    model_rel_zip, config.CHECKPOINT_S3_PREFIX, \"relevance_roberta.zip\")\n",
    "\n",
    "with zipfile.ZipFile(pathlib.Path(model_root, \"relevance_roberta.zip\"), \"r\") as z:\n",
    "    z.extractall(model_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62984a2-811b-4563-9acf-8a70f0ba52e2",
   "metadata": {},
   "source": [
    "### 1.5. Loading KPIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7e325371-4d8c-4471-af2e-706a0b4e6f1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>kpi_id</th>\n",
       "      <th>question</th>\n",
       "      <th>sectors</th>\n",
       "      <th>add_year</th>\n",
       "      <th>kpi_category</th>\n",
       "      <th>Unnamed: 5</th>\n",
       "      <th>Unnamed: 6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>What is the company name?</td>\n",
       "      <td>OG, CM, CU</td>\n",
       "      <td>False</td>\n",
       "      <td>TEXT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>In which year was the annual report or the sus...</td>\n",
       "      <td>OG, CM, CU</td>\n",
       "      <td>False</td>\n",
       "      <td>TEXT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>What is the total volume of proven and probabl...</td>\n",
       "      <td>OG</td>\n",
       "      <td>True</td>\n",
       "      <td>TEXT, TABLE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.1</td>\n",
       "      <td>What is the volume of estimated proven hydroca...</td>\n",
       "      <td>OG</td>\n",
       "      <td>True</td>\n",
       "      <td>TEXT, TABLE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.2</td>\n",
       "      <td>What is the volume of estimated probable hydro...</td>\n",
       "      <td>OG</td>\n",
       "      <td>True</td>\n",
       "      <td>TEXT, TABLE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3.0</td>\n",
       "      <td>What is the total volume of hydrocarbons produ...</td>\n",
       "      <td>OG</td>\n",
       "      <td>True</td>\n",
       "      <td>TEXT, TABLE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3.1</td>\n",
       "      <td>What is the total volume of crude oil liquid p...</td>\n",
       "      <td>OG</td>\n",
       "      <td>True</td>\n",
       "      <td>TEXT, TABLE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3.2</td>\n",
       "      <td>What is the total volume of natural gas liquid...</td>\n",
       "      <td>OG</td>\n",
       "      <td>True</td>\n",
       "      <td>TEXT, TABLE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3.3</td>\n",
       "      <td>What is the total volume of natural gas produc...</td>\n",
       "      <td>OG</td>\n",
       "      <td>True</td>\n",
       "      <td>TEXT, TABLE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4.0</td>\n",
       "      <td>What is the annual total production from coal?</td>\n",
       "      <td>CU</td>\n",
       "      <td>True</td>\n",
       "      <td>TEXT, TABLE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4.1</td>\n",
       "      <td>What is the annual total production from ligni...</td>\n",
       "      <td>CU</td>\n",
       "      <td>True</td>\n",
       "      <td>TEXT, TABLE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4.2</td>\n",
       "      <td>What is the annual total production from hard ...</td>\n",
       "      <td>CU</td>\n",
       "      <td>True</td>\n",
       "      <td>TEXT, TABLE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>5.0</td>\n",
       "      <td>What is the total installed capacity from coal?</td>\n",
       "      <td>CU</td>\n",
       "      <td>True</td>\n",
       "      <td>TEXT, TABLE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>5.1</td>\n",
       "      <td>What is the total installed capacity from lign...</td>\n",
       "      <td>CU</td>\n",
       "      <td>True</td>\n",
       "      <td>TEXT, TABLE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5.2</td>\n",
       "      <td>What is the total installed capacity from hard...</td>\n",
       "      <td>CU</td>\n",
       "      <td>True</td>\n",
       "      <td>TEXT, TABLE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>6.0</td>\n",
       "      <td>What is the total amount of direct greenhouse ...</td>\n",
       "      <td>CU, OG</td>\n",
       "      <td>True</td>\n",
       "      <td>TEXT, TABLE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>7.0</td>\n",
       "      <td>What is the total amount of energy indirect gr...</td>\n",
       "      <td>CU, OG</td>\n",
       "      <td>True</td>\n",
       "      <td>TEXT, TABLE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>8.0</td>\n",
       "      <td>What is the total amount of upstream energy in...</td>\n",
       "      <td>CU, OG</td>\n",
       "      <td>True</td>\n",
       "      <td>TEXT, TABLE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>9.0</td>\n",
       "      <td>What is the base year for carbon reduction com...</td>\n",
       "      <td>OG, CM, CU</td>\n",
       "      <td>False</td>\n",
       "      <td>TEXT, TABLE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>10.0</td>\n",
       "      <td>What is the climate commitment scenario consid...</td>\n",
       "      <td>OG, CM, CU</td>\n",
       "      <td>True</td>\n",
       "      <td>TEXT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>11.0</td>\n",
       "      <td>What is the target year for climate commitment?</td>\n",
       "      <td>OG, CM, CU</td>\n",
       "      <td>False</td>\n",
       "      <td>TEXT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>12.0</td>\n",
       "      <td>What is the target carbon reduction in percent...</td>\n",
       "      <td>OG, CM, CU</td>\n",
       "      <td>True</td>\n",
       "      <td>TEXT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>13.0</td>\n",
       "      <td>What is the total amount of scope 1 and 2 gree...</td>\n",
       "      <td>CU</td>\n",
       "      <td>True</td>\n",
       "      <td>TEXT, TABLE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>14.0</td>\n",
       "      <td>What is the total amount of scope 1, scope 2 a...</td>\n",
       "      <td>OG</td>\n",
       "      <td>True</td>\n",
       "      <td>TEXT, TABLE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    kpi_id                                           question     sectors  \\\n",
       "0      0.0                          What is the company name?  OG, CM, CU   \n",
       "1      1.0  In which year was the annual report or the sus...  OG, CM, CU   \n",
       "2      2.0  What is the total volume of proven and probabl...          OG   \n",
       "3      2.1  What is the volume of estimated proven hydroca...          OG   \n",
       "4      2.2  What is the volume of estimated probable hydro...          OG   \n",
       "5      3.0  What is the total volume of hydrocarbons produ...          OG   \n",
       "6      3.1  What is the total volume of crude oil liquid p...          OG   \n",
       "7      3.2  What is the total volume of natural gas liquid...          OG   \n",
       "8      3.3  What is the total volume of natural gas produc...          OG   \n",
       "9      4.0     What is the annual total production from coal?          CU   \n",
       "10     4.1  What is the annual total production from ligni...          CU   \n",
       "11     4.2  What is the annual total production from hard ...          CU   \n",
       "12     5.0    What is the total installed capacity from coal?          CU   \n",
       "13     5.1  What is the total installed capacity from lign...          CU   \n",
       "14     5.2  What is the total installed capacity from hard...          CU   \n",
       "15     6.0  What is the total amount of direct greenhouse ...      CU, OG   \n",
       "16     7.0  What is the total amount of energy indirect gr...      CU, OG   \n",
       "17     8.0  What is the total amount of upstream energy in...      CU, OG   \n",
       "18     9.0  What is the base year for carbon reduction com...  OG, CM, CU   \n",
       "19    10.0  What is the climate commitment scenario consid...  OG, CM, CU   \n",
       "20    11.0    What is the target year for climate commitment?  OG, CM, CU   \n",
       "21    12.0  What is the target carbon reduction in percent...  OG, CM, CU   \n",
       "22    13.0  What is the total amount of scope 1 and 2 gree...          CU   \n",
       "23    14.0  What is the total amount of scope 1, scope 2 a...          OG   \n",
       "\n",
       "    add_year kpi_category  Unnamed: 5  Unnamed: 6  \n",
       "0      False         TEXT         NaN         NaN  \n",
       "1      False         TEXT         NaN         NaN  \n",
       "2       True  TEXT, TABLE         NaN         NaN  \n",
       "3       True  TEXT, TABLE         NaN         NaN  \n",
       "4       True  TEXT, TABLE         NaN         NaN  \n",
       "5       True  TEXT, TABLE         NaN         NaN  \n",
       "6       True  TEXT, TABLE         NaN         NaN  \n",
       "7       True  TEXT, TABLE         NaN         NaN  \n",
       "8       True  TEXT, TABLE         NaN         NaN  \n",
       "9       True  TEXT, TABLE         NaN         NaN  \n",
       "10      True  TEXT, TABLE         NaN         NaN  \n",
       "11      True  TEXT, TABLE         NaN         NaN  \n",
       "12      True  TEXT, TABLE         NaN         NaN  \n",
       "13      True  TEXT, TABLE         NaN         NaN  \n",
       "14      True  TEXT, TABLE         NaN         NaN  \n",
       "15      True  TEXT, TABLE         NaN         NaN  \n",
       "16      True  TEXT, TABLE         NaN         NaN  \n",
       "17      True  TEXT, TABLE         NaN         NaN  \n",
       "18     False  TEXT, TABLE         NaN         NaN  \n",
       "19      True         TEXT         NaN         NaN  \n",
       "20     False         TEXT         NaN         NaN  \n",
       "21      True         TEXT         NaN         NaN  \n",
       "22      True  TEXT, TABLE         NaN         NaN  \n",
       "23      True  TEXT, TABLE         NaN         NaN  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kpi_df = s3c.download_df_from_s3(\n",
    "    \"aicoe-osc-demo/kpi_mapping\",\n",
    "    \"kpi_mapping.csv\",\n",
    "    filetype=S3FileType.CSV,\n",
    "    header=0)\n",
    "kpi_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb21ff4b-8620-4a53-bb0e-b918166b16e6",
   "metadata": {},
   "source": [
    "## 2. Running Benchmarks on Relevance Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71beed6-2e42-4a81-a3b9-ffbe1fc300a9",
   "metadata": {},
   "source": [
    "### 2.1. Setting up scratch directories and files for benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "204236e3-c544-4fe7-afa0-a37019afb7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "BENCHMARK_FOLDER = config.DATA_FOLDER / \"benchmark\"\n",
    "if not os.path.exists(BENCHMARK_FOLDER):\n",
    "    BENCHMARK_FOLDER.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if not os.path.exists(BENCHMARK_FOLDER / \"extraction\"):\n",
    "    pathlib.Path(BENCHMARK_FOLDER / \"extraction\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if not os.path.exists(BENCHMARK_FOLDER / \"infer_relevance\"):\n",
    "    pathlib.Path(BENCHMARK_FOLDER / \"infer_relevance\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "s3c.download_files_in_prefix_to_dir(\n",
    "    config.BASE_EXTRACTION_S3_PREFIX,\n",
    "    BENCHMARK_FOLDER / \"extraction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a24c72-4258-4b57-949f-93085c2c8eab",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.2. Relevance Infer Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "02c92125-8b40-4dda-8520-ad4db328eea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Text': '/opt/app-root/src/aicoe-osc-demo/models/RELEVANCE'}\n",
      "/opt/app-root/src/data/benchmark/extraction\n",
      "{'Text': PosixPath('/opt/app-root/src/data/benchmark/infer_relevance')}\n"
     ]
    }
   ],
   "source": [
    "infer_config.extracted_dir = BENCHMARK_FOLDER / \"extraction\"\n",
    "infer_config.result_dir['Text'] = BENCHMARK_FOLDER / \"infer_relevance\"\n",
    "print(infer_config.load_dir)\n",
    "print(infer_config.extracted_dir)\n",
    "print(infer_config.result_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9224841-88af-4da4-8089-055e15a341f9",
   "metadata": {},
   "source": [
    "### 2.3. Defining Methods from the TextRelevanceInfer class for benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "46f87db0-2ade-4bbb-93fe-0fdaf70a05b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_folder(tri):\n",
    "    \"\"\"Make prediction on all the data (csv files or json) inside a folder.\n",
    "\n",
    "    It also saves the relevant tables or\n",
    "    paragraphs for questions inside a csv file.\n",
    "    \"\"\"\n",
    "    all_text_path_dict = tri._gather_extracted_files()\n",
    "    df_list = []\n",
    "    metrics_df_list = []\n",
    "    num_pdfs = len(all_text_path_dict)\n",
    "    _logger.info(\n",
    "        \"{} Starting Relevence Inference for the following extracted pdf files found in {}:\\n{} \".format(\n",
    "            \"#\" * 20, tri.result_dir, [pdf for pdf in all_text_path_dict.keys()]))\n",
    "    for i, (pdf_name, file_path) in enumerate(all_text_path_dict.items()):\n",
    "        _logger.info(\"{} {}/{} PDFs\".format(\"#\" * 20, i + 1, num_pdfs))\n",
    "        predictions_file_name = \"{}_{}\".format(pdf_name, \"predictions_relevant.csv\")\n",
    "        if (tri.infer_config.skip_processed_files\n",
    "                and predictions_file_name in os.listdir(tri.result_dir)):\n",
    "            _logger.info(\n",
    "                \"The relevance infer results for {} already exists. Skipping.\".format(\n",
    "                    pdf_name))\n",
    "            _logger.info(\n",
    "                \"If you would like to re-process the already processed files, set \"\n",
    "                \"`skip_processed_files` to False in the config file. \")\n",
    "            continue\n",
    "        _logger.info(\"Running inference for {}:\".format(pdf_name))\n",
    "\n",
    "        try:\n",
    "            start = time.time()\n",
    "            data = tri._gather_data(pdf_name, file_path)\n",
    "            num_data_points = len(data)\n",
    "            num_pages = data[len(data)-1]['page']\n",
    "            _logger.info(\n",
    "                \"Gathered the extracted data ({} points) from the file {} in {} sec.\".format(\n",
    "                    num_data_points, pdf_name, str(time.time() - start)))\n",
    "            predictions = []\n",
    "            chunk_size = 1000\n",
    "            chunk_idx = 0\n",
    "            total_file_time = 0\n",
    "\n",
    "            while chunk_idx * chunk_size < num_data_points:\n",
    "                chunk_start = time.time()\n",
    "                data_chunk = data[\n",
    "                    chunk_idx * chunk_size : (chunk_idx + 1) * chunk_size]\n",
    "                predictions_chunk = tri.model.inference_from_dicts(\n",
    "                    dicts=data_chunk)\n",
    "\n",
    "                predictions.extend(predictions_chunk)\n",
    "                chunk_idx += 1\n",
    "\n",
    "                chunk_end = time.time()\n",
    "                total_file_time += (chunk_end - chunk_start)\n",
    "\n",
    "            time_per_data_point = total_file_time / num_data_points\n",
    "            data_points_per_sec = 1/time_per_data_point\n",
    "            _logger.info(\n",
    "                \"Ran inference on file {} with {} pages and {} data points in {} sec ({} sec per data point,\"\n",
    "                \" {} data points per second)\".format(\n",
    "                    pdf_name, num_pages, num_data_points, total_file_time, time_per_data_point, data_points_per_sec))\n",
    "            metrics_list = [\n",
    "                [pdf_name, int(num_pages), num_data_points, total_file_time, time_per_data_point, data_points_per_sec]]\n",
    "\n",
    "            metrics_df = pd.DataFrame(\n",
    "                metrics_list, columns=['PDF Name', 'Number of Pages', 'Number of Data Points',\n",
    "                                       'Total Inference Time', 'Time per data point', 'Data points per sec'])\n",
    "            metrics_df_list.append(metrics_df)\n",
    "            flat_predictions = [\n",
    "                example for batch in predictions for example in batch[\"predictions\"]]\n",
    "            positive_examples = [\n",
    "                data[index]\n",
    "                for index, pred_example in enumerate(flat_predictions)\n",
    "                if pred_example[\"label\"] == \"1\"]\n",
    "\n",
    "            df = pd.DataFrame(positive_examples)\n",
    "            df[\"source\"] = tri.data_type\n",
    "\n",
    "            df_list.append(df)\n",
    "            predictions_file_path = os.path.join(\n",
    "                tri.result_dir, predictions_file_name\n",
    "            )\n",
    "            df.to_csv(predictions_file_path)\n",
    "            _logger.info(\n",
    "                \"Saved {} relevant {} examples for {} in {}\".format(\n",
    "                    len(df), tri.data_type, pdf_name, predictions_file_path))\n",
    "        except Exception as exc:\n",
    "            _logger.warning(exc)\n",
    "            e = sys.exc_info()[0]\n",
    "            _logger.warning(\n",
    "                \"There was an error making inference (RELEVANCE) on {}\".format(\n",
    "                    pdf_name))\n",
    "            _logger.warning(\"The error is\\n{}\\nSkipping this pdf\".format(e))\n",
    "\n",
    "    concatenated_dfs = pd.concat(df_list) if len(df_list) > 0 else pd.DataFrame()\n",
    "    metrics_df = pd.DataFrame()\n",
    "    if len(metrics_df_list) > 0:\n",
    "        metrics_df = pd.concat(metrics_df_list) if len(metrics_df_list) > 0 else pd.DataFrame()\n",
    "        _logger.info(\n",
    "            \"Metrics for inferring paragraphs relevant to KPI are: \"\n",
    "            \"\\nTotal Number of Data Points Processed = {}\"\n",
    "            \"\\nTotal Inference Time = {}\"\n",
    "            \"\\nAverage Number of Pages Per PDF = {}\"\n",
    "            \"\\nAverage Inference Time Per PDF = {}\"\n",
    "            \"\\nMinimum Inference Time of PDF = {}\"\n",
    "            \"\\nMaximum Inference Time of PDF = {}\"\n",
    "            \"\\nStd of Inference Times of PDFs = {}\"\n",
    "            \"\\nAverage Time Per Data Point Processed= {}\"\n",
    "            \"\\nAverage Data Points Processed Per Second = {} \\n\"\n",
    "            .format(metrics_df['Number of Data Points'].sum(),\n",
    "                    metrics_df['Total Inference Time'].sum(),\n",
    "                    int(metrics_df['Number of Pages'].mean()),\n",
    "                    metrics_df['Total Inference Time'].mean(),\n",
    "                    metrics_df['Total Inference Time'].min(),\n",
    "                    metrics_df['Total Inference Time'].max(),\n",
    "                    metrics_df['Total Inference Time'].std(),\n",
    "                    metrics_df['Time per data point'].mean(),\n",
    "                    metrics_df['Data points per sec'].mean()))\n",
    "\n",
    "    tri.model.close_multiprocessing_pool()\n",
    "    return concatenated_dfs, metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4841d686-85a8-4d21-a226-562da87f3c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_for_relevance():\n",
    "    if not os.path.exists(BENCHMARK_FOLDER / \"infer_relevance\"):\n",
    "        pathlib.Path(BENCHMARK_FOLDER / \"infer_relevance\").mkdir(parents=True, exist_ok=True)\n",
    "    for filename in os.listdir(BENCHMARK_FOLDER / \"infer_relevance\"):\n",
    "        file_path = os.path.join(BENCHMARK_FOLDER / \"infer_relevance\", filename)\n",
    "        try:\n",
    "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                os.unlink(file_path)\n",
    "            elif os.path.isdir(file_path):\n",
    "                shutil.rmtree(file_path)\n",
    "        except Exception as e:\n",
    "            print('Failed to delete %s. Reason: %s' % (file_path, e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5153cc-78d5-4065-a6f5-c2c466a6da44",
   "metadata": {},
   "source": [
    "### 2.4. Running benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1f20cb29-768e-47c8-bd99-1d656ba6a13b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09/26/2022 16:21:27 - WARNING - farm.modeling.prediction_head -   `layer_dims` will be deprecated in future releases\n",
      "09/26/2022 16:21:27 - INFO - __main__ -   #################### Starting Relevence Inference for the following extracted pdf files found in /opt/app-root/src/data/benchmark/infer_relevance:\n",
      "['sustainability-report-2019'] \n",
      "09/26/2022 16:21:27 - INFO - __main__ -   #################### 1/1 PDFs\n",
      "09/26/2022 16:21:27 - INFO - __main__ -   Running inference for sustainability-report-2019:\n",
      "09/26/2022 16:21:27 - INFO - src.models.relevance_infer -   ###### Received 726 examples for Text, number of questions: 24\n",
      "09/26/2022 16:21:27 - INFO - __main__ -   Gathered the extracted data (17424 points) from the file sustainability-report-2019 in 0.013759613037109375 sec.\n",
      "/opt/app-root/lib64/python3.8/site-packages/transformers/tokenization_utils.py:458: FutureWarning: `is_pretokenized` is deprecated and will be removed in a future version, use `is_split_into_words` instead.\n",
      "  warnings.warn(\n",
      "/opt/app-root/lib64/python3.8/site-packages/transformers/tokenization_utils.py:458: FutureWarning: `is_pretokenized` is deprecated and will be removed in a future version, use `is_split_into_words` instead.\n",
      "  warnings.warn(\n",
      "/opt/app-root/lib64/python3.8/site-packages/transformers/tokenization_utils.py:458: FutureWarning: `is_pretokenized` is deprecated and will be removed in a future version, use `is_split_into_words` instead.\n",
      "  warnings.warn(\n",
      "/opt/app-root/lib64/python3.8/site-packages/transformers/tokenization_utils.py:458: FutureWarning: `is_pretokenized` is deprecated and will be removed in a future version, use `is_split_into_words` instead.\n",
      "  warnings.warn(\n",
      "/opt/app-root/lib64/python3.8/site-packages/transformers/tokenization_utils.py:458: FutureWarning: `is_pretokenized` is deprecated and will be removed in a future version, use `is_split_into_words` instead.\n",
      "  warnings.warn(\n",
      "/opt/app-root/lib64/python3.8/site-packages/transformers/tokenization_utils.py:458: FutureWarning: `is_pretokenized` is deprecated and will be removed in a future version, use `is_split_into_words` instead.\n",
      "  warnings.warn(\n",
      "/opt/app-root/lib64/python3.8/site-packages/transformers/tokenization_utils.py:458: FutureWarning: `is_pretokenized` is deprecated and will be removed in a future version, use `is_split_into_words` instead.\n",
      "  warnings.warn(\n",
      "09/26/2022 16:23:27 - INFO - __main__ -   Ran inference on file sustainability-report-2019 with 32 pages and 17424 data points in 120.05442547798157 sec (0.006890175934227592 sec per data point, 145.1341750262728 data points per second)\n",
      "09/26/2022 16:23:28 - INFO - __main__ -   Saved 2788 relevant Text examples for sustainability-report-2019 in /opt/app-root/src/data/benchmark/infer_relevance/sustainability-report-2019_predictions_relevant.csv\n",
      "09/26/2022 16:23:28 - INFO - __main__ -   Metrics for inferring paragraphs relevant to KPI are: \n",
      "Total Number of Data Points Processed = 17424\n",
      "Total Inference Time = 120.05442547798157\n",
      "Average Number of Pages Per PDF = 32\n",
      "Average Inference Time Per PDF = 120.05442547798157\n",
      "Minimum Inference Time of PDF = 120.05442547798157\n",
      "Maximum Inference Time of PDF = 120.05442547798157\n",
      "Std of Inference Times of PDFs = nan\n",
      "Average Time Per Data Point Processed= 0.006890175934227592\n",
      "Average Data Points Processed Per Second = 145.1341750262728 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_runs = 1\n",
    "metrics_dfs = []\n",
    "for i in range(num_runs):\n",
    "    cleanup_for_relevance()\n",
    "    tri = TextRelevanceInfer(infer_config, kpi_df)\n",
    "    result_df, metrics_df = run_folder(tri)\n",
    "    metrics_dfs.append(metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "abdad085-786a-4402-bac7-2765f7e6d47e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmark Iteration 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PDF Name</th>\n",
       "      <th>Number of Pages</th>\n",
       "      <th>Number of Data Points</th>\n",
       "      <th>Total Inference Time</th>\n",
       "      <th>Time per data point</th>\n",
       "      <th>Data points per sec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sustainability-report-2019</td>\n",
       "      <td>32</td>\n",
       "      <td>17424</td>\n",
       "      <td>121.7939</td>\n",
       "      <td>0.00699</td>\n",
       "      <td>143.061353</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     PDF Name  Number of Pages  Number of Data Points  \\\n",
       "0  sustainability-report-2019               32                  17424   \n",
       "\n",
       "   Total Inference Time  Time per data point  Data points per sec  \n",
       "0              121.7939              0.00699           143.061353  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Statistics for Iteration 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Number of Pages</th>\n",
       "      <th>Number of Data Points</th>\n",
       "      <th>Total Inference Time</th>\n",
       "      <th>Time per data point</th>\n",
       "      <th>Data points per sec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>32.0</td>\n",
       "      <td>17424.0</td>\n",
       "      <td>121.7939</td>\n",
       "      <td>0.00699</td>\n",
       "      <td>143.061353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>32.0</td>\n",
       "      <td>17424.0</td>\n",
       "      <td>121.7939</td>\n",
       "      <td>0.00699</td>\n",
       "      <td>143.061353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>32.0</td>\n",
       "      <td>17424.0</td>\n",
       "      <td>121.7939</td>\n",
       "      <td>0.00699</td>\n",
       "      <td>143.061353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>32.0</td>\n",
       "      <td>17424.0</td>\n",
       "      <td>121.7939</td>\n",
       "      <td>0.00699</td>\n",
       "      <td>143.061353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>32.0</td>\n",
       "      <td>17424.0</td>\n",
       "      <td>121.7939</td>\n",
       "      <td>0.00699</td>\n",
       "      <td>143.061353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>32.0</td>\n",
       "      <td>17424.0</td>\n",
       "      <td>121.7939</td>\n",
       "      <td>0.00699</td>\n",
       "      <td>143.061353</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Number of Pages  Number of Data Points  Total Inference Time  \\\n",
       "count              1.0                    1.0                1.0000   \n",
       "mean              32.0                17424.0              121.7939   \n",
       "std                NaN                    NaN                   NaN   \n",
       "min               32.0                17424.0              121.7939   \n",
       "25%               32.0                17424.0              121.7939   \n",
       "50%               32.0                17424.0              121.7939   \n",
       "75%               32.0                17424.0              121.7939   \n",
       "max               32.0                17424.0              121.7939   \n",
       "\n",
       "       Time per data point  Data points per sec  \n",
       "count              1.00000             1.000000  \n",
       "mean               0.00699           143.061353  \n",
       "std                    NaN                  NaN  \n",
       "min                0.00699           143.061353  \n",
       "25%                0.00699           143.061353  \n",
       "50%                0.00699           143.061353  \n",
       "75%                0.00699           143.061353  \n",
       "max                0.00699           143.061353  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmark Iteration 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PDF Name</th>\n",
       "      <th>Number of Pages</th>\n",
       "      <th>Number of Data Points</th>\n",
       "      <th>Total Inference Time</th>\n",
       "      <th>Time per data point</th>\n",
       "      <th>Data points per sec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sustainability-report-2019</td>\n",
       "      <td>32</td>\n",
       "      <td>17424</td>\n",
       "      <td>123.214288</td>\n",
       "      <td>0.007072</td>\n",
       "      <td>141.412171</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     PDF Name  Number of Pages  Number of Data Points  \\\n",
       "0  sustainability-report-2019               32                  17424   \n",
       "\n",
       "   Total Inference Time  Time per data point  Data points per sec  \n",
       "0            123.214288             0.007072           141.412171  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Statistics for Iteration 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Number of Pages</th>\n",
       "      <th>Number of Data Points</th>\n",
       "      <th>Total Inference Time</th>\n",
       "      <th>Time per data point</th>\n",
       "      <th>Data points per sec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>32.0</td>\n",
       "      <td>17424.0</td>\n",
       "      <td>123.214288</td>\n",
       "      <td>0.007072</td>\n",
       "      <td>141.412171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>32.0</td>\n",
       "      <td>17424.0</td>\n",
       "      <td>123.214288</td>\n",
       "      <td>0.007072</td>\n",
       "      <td>141.412171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>32.0</td>\n",
       "      <td>17424.0</td>\n",
       "      <td>123.214288</td>\n",
       "      <td>0.007072</td>\n",
       "      <td>141.412171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>32.0</td>\n",
       "      <td>17424.0</td>\n",
       "      <td>123.214288</td>\n",
       "      <td>0.007072</td>\n",
       "      <td>141.412171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>32.0</td>\n",
       "      <td>17424.0</td>\n",
       "      <td>123.214288</td>\n",
       "      <td>0.007072</td>\n",
       "      <td>141.412171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>32.0</td>\n",
       "      <td>17424.0</td>\n",
       "      <td>123.214288</td>\n",
       "      <td>0.007072</td>\n",
       "      <td>141.412171</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Number of Pages  Number of Data Points  Total Inference Time  \\\n",
       "count              1.0                    1.0              1.000000   \n",
       "mean              32.0                17424.0            123.214288   \n",
       "std                NaN                    NaN                   NaN   \n",
       "min               32.0                17424.0            123.214288   \n",
       "25%               32.0                17424.0            123.214288   \n",
       "50%               32.0                17424.0            123.214288   \n",
       "75%               32.0                17424.0            123.214288   \n",
       "max               32.0                17424.0            123.214288   \n",
       "\n",
       "       Time per data point  Data points per sec  \n",
       "count             1.000000             1.000000  \n",
       "mean              0.007072           141.412171  \n",
       "std                    NaN                  NaN  \n",
       "min               0.007072           141.412171  \n",
       "25%               0.007072           141.412171  \n",
       "50%               0.007072           141.412171  \n",
       "75%               0.007072           141.412171  \n",
       "max               0.007072           141.412171  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(num_runs):\n",
    "    print(\"Benchmark Iteration\",i+1)\n",
    "    display(metrics_dfs[i])\n",
    "    print(\"\\nStatistics for Iteration\",i+1)\n",
    "    display(metrics_dfs[i].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0275679-d184-4cdb-8b3c-3e0e45bbff78",
   "metadata": {},
   "source": [
    "## 3. Running Benchmarks on KPI Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a2ed55-11a7-46fc-8338-5942992b1dfc",
   "metadata": {},
   "source": [
    "### 3.1. Setting up scratch directories and model for benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "397ee606-3e80-4c6c-b484-3d862ad0ca92",
   "metadata": {},
   "outputs": [],
   "source": [
    "BENCHMARK_FOLDER = config.DATA_FOLDER / \"benchmark\"\n",
    "\n",
    "if not os.path.exists(BENCHMARK_FOLDER / \"infer_kpi\"):\n",
    "    pathlib.Path(BENCHMARK_FOLDER / \"infer_kpi\").mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05eb2f72-e636-4d0b-8c22-1c6df08a056e",
   "metadata": {},
   "source": [
    "### 3.2. KPI Infer Configurations and Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7327374d-572c-4a11-b268-1718f7ec09fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Text': '/opt/app-root/src/aicoe-osc-demo/models/KPI_EXTRACTION'}\n",
      "{'Text': PosixPath('/opt/app-root/src/data/benchmark/infer_relevance')}\n",
      "{'Text': PosixPath('/opt/app-root/src/data/benchmark/infer_kpi')}\n"
     ]
    }
   ],
   "source": [
    "file_config = QAFileConfig(\"infer_demo\")\n",
    "infer_config = QAInferConfig(\"infer_demo\")\n",
    "infer_config.relevance_dir['Text'] = BENCHMARK_FOLDER / \"infer_relevance\"\n",
    "infer_config.result_dir['Text'] = BENCHMARK_FOLDER / \"infer_kpi\"\n",
    "print(infer_config.load_dir)\n",
    "print(infer_config.relevance_dir)\n",
    "print(infer_config.result_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b6a7bf7-a2ab-45b1-9cc9-e4afbd346904",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_root = pathlib.Path(file_config.saved_models_dir).parent\n",
    "model_rel_zip = pathlib.Path(model_root, 'KPI_EXTRACTION.zip')\n",
    "s3c.download_file_from_s3(model_rel_zip, config.CHECKPOINT_S3_PREFIX, \"KPI_EXTRACTION.zip\")\n",
    "with zipfile.ZipFile(pathlib.Path(model_root, 'KPI_EXTRACTION.zip'), 'r') as z:\n",
    "    z.extractall(model_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7f77cf-1d01-4e16-890c-56b2576d3067",
   "metadata": {},
   "source": [
    "### 3.3. Defining Methods from the TextKPIInfer class for benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f7d7b298-bf75-41cb-9d8a-faa104c6bf33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_on_relevance_results(tki, kpi_df):\n",
    "    \"\"\"Make inference using the qa model on the relevant paragraphs.\n",
    "\n",
    "    Args:\n",
    "        relevance_results_dir (str): path to the directory where the csv file containing the relevant paragraphs\n",
    "        and KPIs for text are stored (output from the relevance stage).\n",
    "        kpi_df (Pandas.DataFrame): A dataframe with kpi questions\n",
    "    Returns:\n",
    "        span_df (Pandas.DataFrame): A dataframe, containing best n answers for each KPI question for each pdf.\n",
    "            The n is defined by top_k. The following columns are added:\n",
    "                `answer_span`: answer span\n",
    "                `score`: The score of span from qa model\n",
    "                `rank`: for the given context and question, what is the rank of score for answer_span. For examples,\n",
    "                    if rank of a span is rank_1, it means that for the give context and question,\n",
    "                    the qa model gives the highest score to that span. while rank_2 means, the best guess of model\n",
    "                    is either`no_answer` or another span.\n",
    "\n",
    "    Note: The  result data frame will be saved in the `result_dir` directory.\n",
    "    \"\"\"\n",
    "    all_relevance_results_paths = glob.glob(\n",
    "        os.path.join(tki.infer_config.relevance_dir['Text'], \"*.csv\"))\n",
    "    all_span_dfs = []\n",
    "    num_csvs = len(all_relevance_results_paths)\n",
    "    metrics_df_list = []\n",
    "    _logger.info(\n",
    "        \"{} Starting KPI Inference for the following relevance CSV files found in {}:\\n{} \".format(\n",
    "            \"#\" * 20,\n",
    "            tki.infer_config.relevance_dir['Text'],\n",
    "            [\n",
    "                os.path.basename(relevance_results_path)\n",
    "                for relevance_results_path in all_relevance_results_paths]))\n",
    "    for i, relevance_results_path in enumerate(all_relevance_results_paths):\n",
    "        _logger.info(\"{} {}/{}\".format(\"#\" * 20, i + 1, num_csvs))\n",
    "        pdf_name = os.path.basename(relevance_results_path).split(\n",
    "            \"_predictions_relevant\")[0]\n",
    "        predictions_file_name = \"{}_{}\".format(pdf_name, \"predictions_kpi.csv\")\n",
    "        if (tki.infer_config.skip_processed_files\n",
    "                and predictions_file_name in os.listdir(tki.result_dir)):\n",
    "            _logger.info(\n",
    "                \"The KPI infer results for {} already exists. Skipping.\".format(\n",
    "                    pdf_name))\n",
    "            _logger.info(\n",
    "                \"If you would like to re-process the already processed files, set \"\n",
    "                \"`skip_processed_files` to False in the config file. \")\n",
    "            continue\n",
    "        _logger.info(\"Starting KPI Extraction for {}\".format(pdf_name))\n",
    "\n",
    "        input_df = pd.read_csv(relevance_results_path)\n",
    "        column_names = [\"text_b\", \"text\", \"page\", \"pdf_name\", \"source\"]\n",
    "\n",
    "        if len(input_df) == 0:\n",
    "            _logger.info(\n",
    "                \"The received relevance file is empty for {}\".format(pdf_name))\n",
    "            df_empty = pd.DataFrame([])\n",
    "            df_empty.to_csv(os.path.join(tki.result_dir, predictions_file_name))\n",
    "            continue\n",
    "\n",
    "        assert set(column_names).issubset(\n",
    "            set(input_df.columns)), \"\"\"The result of relevance detector has {} columns,while expected {}\"\"\".format(\n",
    "                input_df.columns, column_names)\n",
    "\n",
    "        qa_dict = [\n",
    "            {\"qas\": [question], \"context\": context}\n",
    "            for question, context in zip(input_df[\"text\"], input_df[\"text_b\"])]\n",
    "        num_data_points = len(qa_dict)\n",
    "        result = []\n",
    "        chunk_size = 1000\n",
    "        chunk_idx = 0\n",
    "        total_file_time = 0\n",
    "        while chunk_idx * chunk_size < num_data_points:\n",
    "            chunk_start = time.time()\n",
    "\n",
    "            data_chunk = qa_dict[\n",
    "                chunk_idx * chunk_size : (chunk_idx + 1) * chunk_size]\n",
    "            predictions_chunk = tki.model.inference_from_dicts(dicts=data_chunk)\n",
    "            result.extend(predictions_chunk)\n",
    "            chunk_idx += 1\n",
    "\n",
    "            chunk_end = time.time()\n",
    "            total_file_time += (chunk_end - chunk_start)\n",
    "        # result = self.model.inference_from_dicts(dicts=qa_dict)\n",
    "\n",
    "        time_per_data_point = total_file_time / num_data_points\n",
    "        data_points_per_sec = 1/time_per_data_point\n",
    "\n",
    "        _logger.info(\"Ran inference on the file {} with {} relevant data points in {} sec.\"\n",
    "                     \"({} sec per data point, {} data points per sec)\"\n",
    "                     .format(pdf_name, num_data_points, total_file_time, time_per_data_point, data_points_per_sec))\n",
    "\n",
    "        metrics_list = [[pdf_name, num_data_points, total_file_time, time_per_data_point, data_points_per_sec]]\n",
    "        metrics_df = pd.DataFrame(metrics_list, columns=[\n",
    "            'PDF Name', 'Number of Data Points', 'Total Inference Time', 'Time per data point', 'Data points per sec'])\n",
    "        metrics_df_list.append(metrics_df)\n",
    "\n",
    "        head_num = 0\n",
    "        num_answers = tki.model.model.prediction_heads[0].n_best_per_sample + 1\n",
    "        answers_dict = defaultdict(list)\n",
    "\n",
    "        for exp in result:\n",
    "            preds = exp[\"predictions\"][head_num][\"answers\"]\n",
    "            # Get the no_answer_score\n",
    "            no_answer_score = [\n",
    "                p[\"score\"] for p in preds if p[\"answer\"] == \"no_answer\"]\n",
    "            if (len(no_answer_score) == 0):  # Happens if no answer is not among the n_best predictions.\n",
    "                no_answer_score = (\n",
    "                    preds[0][\"score\"] - exp[\"predictions\"][head_num][\"no_ans_gap\"])\n",
    "            else:\n",
    "                no_answer_score = no_answer_score[0]\n",
    "\n",
    "            # Based on Farm implementation, no_answer_score already is equal = \"CLS score\" + no_ans_boost\n",
    "            # https://github.com/deepset-ai/FARM/blob/978da5d7600c48be458688996538770e9334e71b/farm/modeling/prediction_head.py#L1348\n",
    "            pure_no_ans_score = no_answer_score - infer_config.no_ans_boost\n",
    "\n",
    "            for i in range(num_answers):  # This param is not exactly representative, n_best mostly defines num answers.\n",
    "                answers_dict[f\"rank_{i+1}\"].append(\n",
    "                    (\n",
    "                        preds[i][\"answer\"],\n",
    "                        preds[i][\"score\"],\n",
    "                        pure_no_ans_score,\n",
    "                        no_answer_score))\n",
    "        for i in range(num_answers):\n",
    "            input_df[f\"rank_{i+1}\"] = answers_dict[f\"rank_{i+1}\"]\n",
    "\n",
    "        # Let's put different kpi predictions and their scores into one column so we can sort them.\n",
    "        var_cols = [i for i in list(input_df.columns) if i.startswith(\"rank_\")]\n",
    "        id_vars = [i for i in list(input_df.columns) if not i.startswith(\"rank_\")]\n",
    "        input_df = pd.melt(\n",
    "            input_df,\n",
    "            id_vars=id_vars,\n",
    "            value_vars=var_cols,\n",
    "            var_name=\"rank\",\n",
    "            value_name=\"answer_score\")\n",
    "\n",
    "        # Separate a column with tuple value into two columns\n",
    "        input_df[[\"answer\", \"score\", \"no_ans_score\", \"no_answer_score_plus_boost\"]] = pd.DataFrame(\n",
    "            input_df[\"answer_score\"].tolist(), index=input_df.index)\n",
    "        input_df = input_df.drop(columns=[\"answer_score\"], axis=1)\n",
    "\n",
    "        no_answerables = (\n",
    "            input_df.groupby([\"pdf_name\", \"text\"])\n",
    "            .apply(lambda grp: aggregate_result(grp))\n",
    "            .dropna(how=\"all\"))\n",
    "        no_answerables = pd.DataFrame(\n",
    "            no_answerables, columns=[\"score\"]).reset_index()\n",
    "        no_answerables[\"answer\"] = \"no_answer\"\n",
    "        no_answerables[\"source\"] = \"Text\"\n",
    "\n",
    "        # Filter to span-based answers\n",
    "        span_df = input_df[input_df[\"answer\"] != \"no_answer\"]\n",
    "        # Concatenate the result of span answers with non answerable examples.\n",
    "        span_df = pd.concat([span_df, no_answerables], ignore_index=True)\n",
    "\n",
    "        # Get the predictions with n highest score for each pdf and question.\n",
    "        # If the question is considered unanswerable, the best prediction is \"no_answer\", but the best span-based answer\n",
    "        # is also returned. if the question is answerable, the best span-based answers are returned.\n",
    "        span_df = (\n",
    "            span_df.groupby([\"pdf_name\", \"text\"])\n",
    "            .apply(lambda grp: grp.nlargest(infer_config.top_k, \"score\"))\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "\n",
    "        # Final cleaning on the dataframe, removing unnecessary columns and renaming `text` and `text_b` columns.\n",
    "        unnecessary_cols = [\"rank\"] + [\n",
    "            i for i in list(span_df.columns) if i.startswith(\"Unnamed\")]\n",
    "        span_df = span_df.drop(columns=unnecessary_cols, axis=1)\n",
    "        span_df.rename(columns={\"text\": \"kpi\", \"text_b\": \"paragraph\"}, inplace=True)\n",
    "\n",
    "        # Add the kpi id\n",
    "        reversed_kpi_mapping = {\n",
    "            value[0]: key\n",
    "            for key, value in get_kpi_mapping_category(kpi_df)[\n",
    "                \"KPI_MAPPING\"].items()}\n",
    "        span_df[\"kpi_id\"] = span_df[\"kpi\"].map(reversed_kpi_mapping)\n",
    "\n",
    "        # Change the order of columns\n",
    "        first_cols = [\"pdf_name\", \"kpi\", \"kpi_id\", \"answer\", \"page\"]\n",
    "        column_order = first_cols + [\n",
    "            col for col in span_df.columns if col not in first_cols]\n",
    "        span_df = span_df[column_order]\n",
    "\n",
    "        result_path = os.path.join(tki.result_dir, predictions_file_name)\n",
    "        span_df.to_csv(result_path)\n",
    "        _logger.info(\"Save the result of KPI extraction to {}\".format(result_path))\n",
    "        all_span_dfs.append(span_df)\n",
    "    concatenated_dfs = (\n",
    "        pd.concat(all_span_dfs) if len(all_span_dfs) > 0 else pd.DataFrame())\n",
    "    metrics_df = pd.DataFrame()\n",
    "    if len(metrics_df_list) > 0:\n",
    "        metrics_df = pd.concat(metrics_df_list) if len(metrics_df_list) > 0 else pd.DataFrame()\n",
    "        _logger.info(\n",
    "            \"Metrics for KPI from revelant paragraphs are:\"\n",
    "            \"\\nTotal Number of Data Points Processed = {}\"\n",
    "            \"\\nTotal Inference Time = {}\"\n",
    "            \"\\nAverage Inference Time Per CSV = {}\"\n",
    "            \"\\nMinimum Inference Time of CSV = {}\"\n",
    "            \"\\nMaximum Inference Time of CSV = {}\"\n",
    "            \"\\nStd of Inference Times of CSVs = {}\"\n",
    "            \"\\nAverage Time Per Data Point Processed= {}\"\n",
    "            \"\\nAverage Data Points Processed Per Second = {} \\n\"\n",
    "            .format(metrics_df['Number of Data Points'].sum(),\n",
    "                    metrics_df['Total Inference Time'].sum(),\n",
    "                    metrics_df['Total Inference Time'].mean(),\n",
    "                    metrics_df['Total Inference Time'].min(),\n",
    "                    metrics_df['Total Inference Time'].max(),\n",
    "                    metrics_df['Total Inference Time'].std(),\n",
    "                    metrics_df['Time per data point'].mean(),\n",
    "                    metrics_df['Data points per sec'].mean()))\n",
    "    tki.model.close_multiprocessing_pool()\n",
    "    return concatenated_dfs, metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dfc298b5-e6fb-48eb-bbf8-b601fdbda672",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_for_kpi():\n",
    "    if not os.path.exists(BENCHMARK_FOLDER / \"infer_kpi\"):\n",
    "        pathlib.Path(BENCHMARK_FOLDER / \"infer_kpi\").mkdir(parents=True, exist_ok=True)\n",
    "    for filename in os.listdir(BENCHMARK_FOLDER / \"infer_kpi\"):\n",
    "        file_path = os.path.join(BENCHMARK_FOLDER / \"infer_kpi\", filename)\n",
    "        try:\n",
    "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                os.unlink(file_path)\n",
    "            elif os.path.isdir(file_path):\n",
    "                shutil.rmtree(file_path)\n",
    "        except Exception as e:\n",
    "            print('Failed to delete %s. Reason: %s' % (file_path, e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33149241-ca0b-4331-8b6f-17514cbc9a4c",
   "metadata": {},
   "source": [
    "### 3.4. Running Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f1452526-c75e-42be-999c-7a9339d84bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09/26/2022 19:35:24 - WARNING - farm.modeling.prediction_head -   Some unused parameters are passed to the QuestionAnsweringHead. Might not be a problem. Params: {\"training\": false, \"num_labels\": 2, \"ph_output_type\": \"per_token_squad\", \"model_type\": \"span_classification\", \"label_tensor_name\": \"question_answering_label_ids\", \"label_list\": [\"start_token\", \"end_token\"], \"metric\": \"squad\", \"name\": \"QuestionAnsweringHead\"}\n",
      "09/26/2022 19:35:25 - WARNING - farm.infer -   QAInferencer always has task_type='question_answering' even if another value is provided to Inferencer.load() or QAInferencer()\n",
      "09/26/2022 19:35:25 - INFO - __main__ -   #################### Starting KPI Inference for the following relevance CSV files found in /opt/app-root/src/data/benchmark/infer_relevance:\n",
      "['sustainability-report-2019_predictions_relevant.csv'] \n",
      "09/26/2022 19:35:25 - INFO - __main__ -   #################### 1/1\n",
      "09/26/2022 19:35:25 - INFO - __main__ -   Starting KPI Extraction for sustainability-report-2019\n",
      "Inferencing Samples: 100%|██████████| 8/8 [00:01<00:00,  5.80 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 8/8 [00:01<00:00,  6.96 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 8/8 [00:01<00:00,  7.95 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 8/8 [00:01<00:00,  7.54 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 8/8 [00:01<00:00,  7.48 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 8/8 [00:00<00:00,  8.26 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 8/8 [00:00<00:00,  9.49 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 8/8 [00:00<00:00, 16.95 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 8/8 [00:00<00:00, 16.88 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 8/8 [00:00<00:00, 17.02 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 8/8 [00:00<00:00, 17.09 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 8/8 [00:00<00:00, 16.97 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 8/8 [00:00<00:00, 16.88 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 8/8 [00:00<00:00, 16.94 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 8/8 [00:00<00:00, 17.04 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 8/8 [00:00<00:00, 16.92 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 8/8 [00:00<00:00, 16.79 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 8/8 [00:00<00:00, 17.05 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 8/8 [00:00<00:00, 17.05 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 8/8 [00:00<00:00, 16.65 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 8/8 [00:00<00:00, 16.82 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 8/8 [00:00<00:00, 16.89 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 8/8 [00:00<00:00, 16.76 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 8/8 [00:00<00:00, 17.03 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 8/8 [00:00<00:00, 17.05 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 8/8 [00:00<00:00, 17.05 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 8/8 [00:00<00:00, 16.74 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 8/8 [00:00<00:00, 17.13 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 8/8 [00:00<00:00, 16.94 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 8/8 [00:00<00:00, 16.78 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 8/8 [00:00<00:00, 16.71 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 8/8 [00:00<00:00, 16.94 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 8/8 [00:00<00:00, 16.81 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 8/8 [00:00<00:00, 16.59 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 4/4 [00:00<00:00, 17.46 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 8/8 [00:01<00:00,  7.24 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 8/8 [00:01<00:00,  7.46 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 8/8 [00:01<00:00,  6.98 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 8/8 [00:01<00:00,  7.65 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 8/8 [00:01<00:00,  7.87 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 8/8 [00:01<00:00,  6.92 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 8/8 [00:00<00:00, 10.54 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 8/8 [00:00<00:00, 16.75 Batches/s]\n",
      "09/26/2022 19:35:55 - ERROR - farm.modeling.predictions -   Both start and end offsets should be 0: \n",
      "86, 86 with a no_answer. \n",
      "09/26/2022 19:35:55 - ERROR - farm.modeling.predictions -   Both start and end offsets should be 0: \n",
      "66, 66 with a no_answer. \n",
      "Inferencing Samples: 100%|██████████| 8/8 [00:00<00:00, 16.38 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 8/8 [00:00<00:00, 16.41 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 8/8 [00:00<00:00, 16.72 Batches/s]\n",
      "09/26/2022 19:35:57 - ERROR - farm.modeling.predictions -   Both start and end offsets should be 0: \n",
      "65, 65 with a no_answer. \n",
      "Inferencing Samples: 100%|██████████| 8/8 [00:00<00:00, 16.75 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 8/8 [00:00<00:00, 16.68 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 8/8 [00:00<00:00, 16.80 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 8/8 [00:00<00:00, 16.78 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 8/8 [00:00<00:00, 16.93 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 8/8 [00:00<00:00, 16.94 Batches/s]\n",
      "09/26/2022 19:35:59 - ERROR - farm.modeling.predictions -   Both start and end offsets should be 0: \n",
      "86, 86 with a no_answer. \n",
      "09/26/2022 19:35:59 - ERROR - farm.modeling.predictions -   Both start and end offsets should be 0: \n",
      "66, 66 with a no_answer. \n",
      "Inferencing Samples: 100%|██████████| 8/8 [00:00<00:00, 16.81 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 8/8 [00:00<00:00, 16.94 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 8/8 [00:00<00:00, 16.83 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 8/8 [00:00<00:00, 16.54 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 8/8 [00:00<00:00, 16.58 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 8/8 [00:00<00:00, 16.89 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 8/8 [00:00<00:00, 16.55 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 8/8 [00:00<00:00, 16.79 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 8/8 [00:00<00:00, 16.78 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 8/8 [00:00<00:00, 16.69 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 8/8 [00:00<00:00, 17.00 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 8/8 [00:00<00:00, 16.93 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 8/8 [00:00<00:00, 16.64 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 8/8 [00:00<00:00, 16.95 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 8/8 [00:00<00:00, 17.04 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 8/8 [00:00<00:00, 16.97 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 8/8 [00:00<00:00, 16.95 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 4/4 [00:00<00:00, 17.81 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 6/6 [00:00<00:00,  6.24 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 6/6 [00:00<00:00,  7.18 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 6/6 [00:00<00:00,  6.59 Batches/s]\n",
      "09/26/2022 19:36:11 - ERROR - farm.modeling.predictions -   Both start and end offsets should be 0: \n",
      "86, 86 with a no_answer. \n",
      "Inferencing Samples: 100%|██████████| 6/6 [00:00<00:00,  6.93 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 6/6 [00:00<00:00,  7.37 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 6/6 [00:00<00:00,  7.51 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 6/6 [00:00<00:00,  7.33 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 6/6 [00:00<00:00,  6.12 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 6/6 [00:00<00:00, 16.30 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 6/6 [00:00<00:00, 16.25 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 6/6 [00:00<00:00, 16.32 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 6/6 [00:00<00:00, 16.24 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 6/6 [00:00<00:00, 16.18 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 6/6 [00:00<00:00, 16.27 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 6/6 [00:00<00:00, 16.29 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 6/6 [00:00<00:00, 16.23 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 6/6 [00:00<00:00, 16.27 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 6/6 [00:00<00:00, 16.33 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 6/6 [00:00<00:00, 16.48 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 6/6 [00:00<00:00, 16.40 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 6/6 [00:00<00:00, 16.01 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 6/6 [00:00<00:00, 16.13 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 6/6 [00:00<00:00, 16.25 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 6/6 [00:00<00:00, 16.29 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 6/6 [00:00<00:00, 16.23 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 6/6 [00:00<00:00, 16.29 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 6/6 [00:00<00:00, 16.31 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 6/6 [00:00<00:00, 16.13 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 6/6 [00:00<00:00, 16.24 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 6/6 [00:00<00:00, 15.59 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 6/6 [00:00<00:00, 16.12 Batches/s]\n",
      "09/26/2022 19:36:24 - ERROR - farm.modeling.predictions -   Both start and end offsets should be 0: \n",
      "66, 66 with a no_answer. \n",
      "Inferencing Samples: 100%|██████████| 6/6 [00:00<00:00, 16.21 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 6/6 [00:00<00:00, 16.23 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 6/6 [00:00<00:00, 15.65 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 2/2 [00:00<00:00, 19.48 Batches/s]\n",
      "09/26/2022 19:36:26 - INFO - __main__ -   Ran inference on the file sustainability-report-2019 with 2788 relevant data points in 60.177449464797974 sec.(0.0215844510275459 sec per data point, 46.329647148487034 data points per sec)\n",
      "09/26/2022 19:36:26 - INFO - __main__ -   Save the result of KPI extraction to /opt/app-root/src/data/benchmark/infer_kpi/sustainability-report-2019_predictions_kpi.csv\n",
      "09/26/2022 19:36:26 - INFO - __main__ -   Metrics for KPI from revelant paragraphs are:\n",
      "Total Number of Data Points Processed = 2788\n",
      "Total Inference Time = 60.177449464797974\n",
      "Average Inference Time Per CSV = 60.177449464797974\n",
      "Minimum Inference Time of CSV = 60.177449464797974\n",
      "Maximum Inference Time of CSV = 60.177449464797974\n",
      "Std of Inference Times of CSVs = nan\n",
      "Average Time Per Data Point Processed= 0.0215844510275459\n",
      "Average Data Points Processed Per Second = 46.329647148487034 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_runs = 1\n",
    "kpi_metrics_dfs = []\n",
    "for i in range(num_runs):\n",
    "    cleanup_for_kpi()\n",
    "    tki = TextKPIInfer(infer_config)\n",
    "    result_df, kpi_metrics_df = infer_on_relevance_results(tki, kpi_df)\n",
    "    kpi_metrics_dfs.append(kpi_metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1e60b46a-3bef-428d-81c6-3f26c04f1063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmark Iteration 1\n",
      "                     PDF Name  Number of Data Points  Total Inference Time  \\\n",
      "0  sustainability-report-2019                   2788             60.177449   \n",
      "\n",
      "   Time per data point  Data points per sec  \n",
      "0             0.021584            46.329647  \n",
      "\n",
      "Statistics for Iteration 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Number of Data Points</th>\n",
       "      <th>Total Inference Time</th>\n",
       "      <th>Time per data point</th>\n",
       "      <th>Data points per sec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2788.0</td>\n",
       "      <td>60.177449</td>\n",
       "      <td>0.021584</td>\n",
       "      <td>46.329647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2788.0</td>\n",
       "      <td>60.177449</td>\n",
       "      <td>0.021584</td>\n",
       "      <td>46.329647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2788.0</td>\n",
       "      <td>60.177449</td>\n",
       "      <td>0.021584</td>\n",
       "      <td>46.329647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2788.0</td>\n",
       "      <td>60.177449</td>\n",
       "      <td>0.021584</td>\n",
       "      <td>46.329647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2788.0</td>\n",
       "      <td>60.177449</td>\n",
       "      <td>0.021584</td>\n",
       "      <td>46.329647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2788.0</td>\n",
       "      <td>60.177449</td>\n",
       "      <td>0.021584</td>\n",
       "      <td>46.329647</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Number of Data Points  Total Inference Time  Time per data point  \\\n",
       "count                    1.0              1.000000             1.000000   \n",
       "mean                  2788.0             60.177449             0.021584   \n",
       "std                      NaN                   NaN                  NaN   \n",
       "min                   2788.0             60.177449             0.021584   \n",
       "25%                   2788.0             60.177449             0.021584   \n",
       "50%                   2788.0             60.177449             0.021584   \n",
       "75%                   2788.0             60.177449             0.021584   \n",
       "max                   2788.0             60.177449             0.021584   \n",
       "\n",
       "       Data points per sec  \n",
       "count             1.000000  \n",
       "mean             46.329647  \n",
       "std                    NaN  \n",
       "min              46.329647  \n",
       "25%              46.329647  \n",
       "50%              46.329647  \n",
       "75%              46.329647  \n",
       "max              46.329647  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(num_runs):\n",
    "    print(\"Benchmark Iteration\",i+1)\n",
    "    print(kpi_metrics_dfs[i])\n",
    "    print(\"\\nStatistics for Iteration\",i+1)\n",
    "    display(kpi_metrics_dfs[i].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041c983b-f0cc-463d-9101-6d1925390e6b",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "We ran benchmarks on the following models:<br>\n",
    "    - `RELEVANCE`<br>\n",
    "    - `KPI`<br>\n",
    "For both the models, we set up a scratch directory, downloaded the data for the models to run inference on and executed the inferencing step `num_runs` times. Finally, the run times on datasets and their related statistics are displayed for both models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
