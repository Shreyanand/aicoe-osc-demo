{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05dcb4f5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Relevance with Sparse Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1230654c",
   "metadata": {
    "papermill": {
     "duration": 3.30074,
     "end_time": "2022-10-07T19:33:18.885511",
     "exception": false,
     "start_time": "2022-10-07T19:33:15.584771",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import config\n",
    "import os\n",
    "import pathlib\n",
    "from dotenv import load_dotenv\n",
    "from sklearn.metrics import recall_score, precision_score, f1_score, accuracy_score\n",
    "from src.data.s3_communication import S3Communication\n",
    "from sparsezoo import Model\n",
    "import zipfile\n",
    "from io import BytesIO\n",
    "import evaluate\n",
    "from datasets import Dataset, DatasetDict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import AutoTokenizer\n",
    "import shutil\n",
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings('always')  # \"error\", \"ignore\", \"always\", \"default\", \"module\" or \"once\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6fe41e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PYTORCH_NO_CUDA_MEMORY_CACHING=1\n"
     ]
    }
   ],
   "source": [
    "%env PYTORCH_NO_CUDA_MEMORY_CACHING=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a2fcfde",
   "metadata": {
    "papermill": {
     "duration": 0.011639,
     "end_time": "2022-10-07T19:33:18.901426",
     "exception": false,
     "start_time": "2022-10-07T19:33:18.889787",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load credentials\n",
    "dotenv_dir = os.environ.get(\n",
    "    \"CREDENTIAL_DOTENV_DIR\", os.environ.get(\"PWD\", \"/opt/app-root/src\")\n",
    ")\n",
    "dotenv_path = pathlib.Path(dotenv_dir) / \"credentials.env\"\n",
    "if os.path.exists(dotenv_path):\n",
    "    load_dotenv(dotenv_path=dotenv_path, override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "128a55ac",
   "metadata": {
    "papermill": {
     "duration": 0.090141,
     "end_time": "2022-10-07T19:33:18.997018",
     "exception": false,
     "start_time": "2022-10-07T19:33:18.906877",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# init s3 connector\n",
    "s3c = S3Communication(\n",
    "    s3_endpoint_url=os.getenv(\"S3_ENDPOINT\"),\n",
    "    aws_access_key_id=os.getenv(\"AWS_ACCESS_KEY_ID\"),\n",
    "    aws_secret_access_key=os.getenv(\"AWS_SECRET_ACCESS_KEY\"),\n",
    "    s3_bucket=os.getenv(\"S3_BUCKET\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae786b6b",
   "metadata": {
    "papermill": {
     "duration": 0.003808,
     "end_time": "2022-10-07T19:33:19.004776",
     "exception": false,
     "start_time": "2022-10-07T19:33:19.000968",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Retrieve the test dataset and the trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad0a89ab",
   "metadata": {
    "papermill": {
     "duration": 0.886377,
     "end_time": "2022-10-07T19:33:19.895045",
     "exception": false,
     "start_time": "2022-10-07T19:33:19.008668",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3c.download_files_in_prefix_to_dir(\n",
    "    config.BASE_TRAIN_TEST_DATASET_S3_PREFIX,\n",
    "    config.BASE_PROCESSED_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0eea6b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_path = str(config.BASE_PROCESSED_DATA)+'/rel_test_split.csv'\n",
    "test_data = pd.read_csv(test_data_path, index_col=0)\n",
    "test_data.rename(columns={'text':'question', 'text_b':'sentence'}, inplace=True)\n",
    "\n",
    "train_data_path = str(config.BASE_PROCESSED_DATA)+'/rel_train_split.csv'\n",
    "train_data = pd.read_csv(train_data_path, index_col=0)\n",
    "train_data.rename(columns={'text':'question', 'text_b':'sentence'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca46d1c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>question</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>0</td>\n",
       "      <td>What is the annual total production from ligni...</td>\n",
       "      <td>PJM's Operating ORDC Filing — On March 29, 201...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>1</td>\n",
       "      <td>What is the annual total production from ligni...</td>\n",
       "      <td>64.8 million metric tons of lignite produced</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     label                                           question  \\\n",
       "403      0  What is the annual total production from ligni...   \n",
       "402      1  What is the annual total production from ligni...   \n",
       "\n",
       "                                              sentence  \n",
       "403  PJM's Operating ORDC Filing — On March 29, 201...  \n",
       "402      64.8 million metric tons of lignite produced   "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[train_data['question']=='What is the annual total production from lignite (brown coal)?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71cbc227",
   "metadata": {},
   "outputs": [],
   "source": [
    "trds = Dataset.from_pandas(train_data)\n",
    "teds = Dataset.from_pandas(test_data.drop('label', axis=1))\n",
    "\n",
    "climate_dataset = DatasetDict()\n",
    "\n",
    "climate_dataset['train'] = trds\n",
    "climate_dataset['test'] = teds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09cfffc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'question', 'sentence', '__index_level_0__'],\n",
       "        num_rows: 2033\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['question', 'sentence', '__index_level_0__'],\n",
       "        num_rows: 509\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "climate_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d62e749",
   "metadata": {},
   "source": [
    "## Sparse model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f96acafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_path = str(config.BASE_PROCESSED_DATA)+'/rel_test_split.csv'\n",
    "test_data = pd.read_csv(test_data_path, index_col=0)\n",
    "test_data.rename(columns={'text':'question', 'text_b':'sentence'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e5a9ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = 2\n",
    "batch_size = 16 #16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "28e2a88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(local_path, model_name):\n",
    "    trainer.save_model(local_path)\n",
    "    shutil.make_archive(local_path, 'zip', local_path)\n",
    "    buffer = BytesIO()\n",
    "    with zipfile.ZipFile(buffer, 'a') as z:\n",
    "        for dirname, _, files in os.walk(local_path):\n",
    "            for f in files:\n",
    "                f_path = os.path.join(dirname, f)\n",
    "                with open (f_path, 'rb') as file_content:\n",
    "                    z.writestr(f\"{model_name}/{f}\", file_content.read())\n",
    "    buffer.seek(0)\n",
    "    # upload model to s3\n",
    "    s3c._upload_bytes(\n",
    "        buffer_bytes=buffer,\n",
    "        prefix=config.BASE_SAVED_MODELS_S3_PREFIX,\n",
    "        key=f\"{model_name}.zip\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f19d5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load('f1')\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples['question'],\n",
    "                     examples['sentence'],\n",
    "                     truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5763f9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scores_fn(groups):\n",
    "    for group, data in groups:\n",
    "        pred = data.pred\n",
    "        true = data.label\n",
    "        scores[group] = {}\n",
    "        scores[group][\"accuracy\"] = accuracy_score(true, pred)\n",
    "        scores[group][\"f1_score\"] = f1_score(true, pred)\n",
    "        scores[group][\"recall_score\"] = recall_score(true, pred)\n",
    "        scores[group][\"precision_score\"] = precision_score(true, pred)\n",
    "        scores[group][\"support\"] = len(pred)\n",
    "    scores_df = pd.DataFrame(scores)\n",
    "    return scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c4f2615",
   "metadata": {},
   "outputs": [],
   "source": [
    "stubs=['zoo:nlp/text_classification/distilbert-none/pytorch/huggingface/mnli/pruned80_quant-none-vnni',\n",
    "       'zoo:nlp/text_classification/distilbert-none/pytorch/huggingface/qqp/pruned80_quant-none-vnni',\n",
    "       'zoo:nlp/text_classification/obert-base/pytorch/huggingface/mnli/pruned90_quant-none']\n",
    "\n",
    "paths=['/opt/app-root/src/aicoe-osc-demo/models/distilbert_mnli_pruned80',\n",
    "       '/opt/app-root/src/aicoe-osc-demo/models/distilbert_qqp_pruned80',\n",
    "       '/opt/app-root/src/aicoe-osc-demo/models/obert_mnli_pruned90']\n",
    "\n",
    "model_name_4_s3=['distilbert_mnli_pruned80', 'distilbert_qqp_pruned80', 'obert_mnli_pruned90']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "863feaf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d795afe59da54f7f930721ea41e5c6c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "downloading...:   0%|          | 0.00/198 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21a00747ba194a92bc961798585110a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "downloading...:   0%|          | 0.00/655 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49f2f1a118fa4d0cbdbd83e48e7a5e47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "downloading...:   0%|          | 0.00/8.47k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4d6587fb52d42478f76eaf734ad27e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "downloading...:   0%|          | 0.00/3.68k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "534b9cb5d28d4310b41432c565fe3d12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "downloading...:   0%|          | 0.00/3.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb8a0c6e63324d7e8815b89f4213b4f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "downloading...:   0%|          | 0.00/256M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20d0b81d887840c7ac0478ee52a05845",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "downloading...:   0%|          | 0.00/867 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb4c409c4c62484080c3d5db049f4246",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "downloading...:   0%|          | 0.00/478 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64cc47c29f014730aefd6bcf56d0cc84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "downloading...:   0%|          | 0.00/695k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44eea0a75d0c4cf0a3be70a821e558e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "downloading...:   0%|          | 0.00/348 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "796c0dec551a4efa908cc142644378f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "downloading...:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7aa7895bc87b4eb7b2ad8a53cda4fb71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "downloading...:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d9a901c4bb44dbe9bd6caef75d2f990",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "downloading...:   0%|          | 0.00/64.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0ce590ce6c047609b51977c25d850a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "downloading...:   0%|          | 0.00/695k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1db9672b34824a1c837ba83d6620183b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "downloading...:   0%|          | 0.00/867 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5ce1885ebe842c4bc67414631ac75be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "downloading...:   0%|          | 0.00/348 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38a4566deb074a04980ee84a0c4001ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "downloading...:   0%|          | 0.00/4.13k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bed505779f448e6ab1a241e26b21c1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "downloading...:   0%|          | 0.00/1.30k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19e1b66d41144b42b00203f7e77745a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "downloading...:   0%|          | 0.00/1.05k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91242577ec874cf1be4a21c46bd492d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "downloading...:   0%|          | 0.00/4.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "521a52896b0446499261abf1179691f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "downloading...:   0%|          | 0.00/64.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /opt/app-root/src/aicoe-osc-demo/models/distilbert_mnli_pruned80/training were not used when initializing DistilBertForSequenceClassification: ['distilbert.transformer.layer.1.attention.k_lin.quant.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.1.attention.out_lin.quant.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.4.attention.q_lin.quant.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.3.ffn.lin2.module.weight_fake_quant.zero_point', 'distilbert.transformer.layer.3.attention.v_lin.quant.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.3.attention.attention_scores_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.3.ffn.lin2.module.weight_fake_quant.fake_quant_enabled', 'distilbert.transformer.layer.2.attention.out_lin.quant.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.0.attention.k_lin.module.weight', 'distilbert.transformer.layer.5.attention.out_lin.module.weight_fake_quant.zero_point', 'distilbert.transformer.layer.0.ffn.lin1.module.weight_fake_quant.activation_post_process.eps', 'distilbert.transformer.layer.2.attention.context_layer_matmul.input_quant_stubs.1.activation_post_process.zero_point', 'distilbert.transformer.layer.4.attention.context_layer_matmul.input_quant_stubs.1.activation_post_process.zero_point', 'distilbert.transformer.layer.4.ffn.lin2.quant.activation_post_process.scale', 'distilbert.transformer.layer.1.attention.v_lin.quant.activation_post_process.scale', 'distilbert.transformer.layer.4.attention.context_layer_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.5.attention.q_lin.quant.activation_post_process.zero_point', 'distilbert.transformer.layer.1.ffn.lin1.quant.activation_post_process.zero_point', 'distilbert.transformer.layer.3.attention.attention_scores_matmul.input_quant_stubs.1.activation_post_process.observer_enabled', 'distilbert.transformer.layer.2.attention.context_layer_matmul.input_quant_stubs.1.activation_post_process.observer_enabled', 'distilbert.transformer.layer.2.attention.attention_scores_matmul.input_quant_stubs.1.activation_post_process.zero_point', 'distilbert.transformer.layer.2.attention.q_lin.quant.activation_post_process.scale', 'distilbert.transformer.layer.4.attention.k_lin.quant.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.5.ffn.lin1.quant.activation_post_process.zero_point', 'distilbert.transformer.layer.3.ffn.lin2.quant.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.5.ffn.lin2.module.weight_fake_quant.scale', 'distilbert.transformer.layer.3.attention.out_lin.module.weight_fake_quant.activation_post_process.max_val', 'distilbert.transformer.layer.0.ffn.lin2.quant.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.1.attention.attention_scores_matmul.input_quant_stubs.1.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.4.ffn.lin2.module.weight_fake_quant.fake_quant_enabled', 'distilbert.transformer.layer.5.attention.out_lin.module.weight_fake_quant.fake_quant_enabled', 'distilbert.transformer.layer.5.attention.attention_scores_matmul.input_quant_stubs.0.activation_post_process.observer_enabled', 'distilbert.transformer.layer.5.ffn.lin2.quant.activation_post_process.zero_point', 'distilbert.transformer.layer.4.attention.attention_scores_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.1.ffn.lin2.quant.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.0.ffn.lin2.quant.activation_post_process.zero_point', 'distilbert.transformer.layer.1.attention.q_lin.quant.activation_post_process.scale', 'distilbert.transformer.layer.2.ffn.lin2.module.weight_fake_quant.activation_post_process.min_val', 'distilbert.transformer.layer.4.ffn.lin2.quant.activation_post_process.fake_quant_enabled', 'distilbert.embeddings.position_embeddings.weight_fake_quant.scale', 'distilbert.transformer.layer.2.attention.v_lin.module.weight_fake_quant.scale', 'distilbert.transformer.layer.3.attention.v_lin.module.bias', 'distilbert.transformer.layer.2.attention.v_lin.quant.activation_post_process.observer_enabled', 'distilbert.transformer.layer.1.ffn.lin2.quant.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.5.attention.q_lin.quant.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.5.attention.v_lin.module.weight_fake_quant.observer_enabled', 'classifier.module.weight', 'distilbert.transformer.layer.2.attention.v_lin.quant.activation_post_process.scale', 'distilbert.transformer.layer.4.attention.out_lin.module.weight_fake_quant.activation_post_process.eps', 'distilbert.transformer.layer.4.ffn.lin1.module.weight', 'distilbert.transformer.layer.0.attention.attention_scores_matmul.input_quant_stubs.0.activation_post_process.zero_point', 'distilbert.transformer.layer.1.attention.out_lin.quant.activation_post_process.observer_enabled', 'distilbert.transformer.layer.2.ffn.lin1.module.weight_fake_quant.zero_point', 'distilbert.transformer.layer.4.attention.q_lin.module.weight_fake_quant.fake_quant_enabled', 'pre_classifier.module.weight_fake_quant.zero_point', 'distilbert.transformer.layer.4.attention.q_lin.module.bias', 'distilbert.transformer.layer.3.attention.out_lin.module.weight_fake_quant.activation_post_process.eps', 'distilbert.transformer.layer.1.attention.k_lin.module.weight_fake_quant.scale', 'distilbert.transformer.layer.2.attention.q_lin.module.bias', 'distilbert.transformer.layer.2.attention.v_lin.module.bias', 'distilbert.transformer.layer.2.ffn.lin1.quant.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.2.attention.k_lin.module.weight', 'distilbert.transformer.layer.2.ffn.lin2.module.weight_fake_quant.activation_post_process.max_val', 'pre_classifier.module.weight_fake_quant.activation_post_process.min_val', 'distilbert.transformer.layer.1.ffn.lin2.quant.activation_post_process.zero_point', 'distilbert.transformer.layer.5.attention.k_lin.quant.activation_post_process.scale', 'pre_classifier.module.weight_fake_quant.activation_post_process.max_val', 'distilbert.transformer.layer.3.ffn.lin1.module.weight_fake_quant.observer_enabled', 'distilbert.transformer.layer.5.attention.q_lin.quant.activation_post_process.observer_enabled', 'distilbert.transformer.layer.5.ffn.lin2.quant.activation_post_process.observer_enabled', 'distilbert.embeddings.word_embeddings.weight_fake_quant.observer_enabled', 'distilbert.transformer.layer.0.attention.attention_scores_matmul.input_quant_stubs.1.activation_post_process.observer_enabled', 'distilbert.transformer.layer.1.ffn.lin2.quant.activation_post_process.scale', 'distilbert.transformer.layer.4.ffn.lin1.module.weight_fake_quant.activation_post_process.eps', 'distilbert.transformer.layer.4.attention.v_lin.module.weight_fake_quant.fake_quant_enabled', 'distilbert.transformer.layer.0.attention.v_lin.quant.activation_post_process.observer_enabled', 'distilbert.transformer.layer.2.attention.out_lin.quant.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.1.ffn.lin2.quant.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.4.attention.context_layer_matmul.input_quant_stubs.0.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.1.attention.v_lin.quant.activation_post_process.zero_point', 'distilbert.transformer.layer.1.attention.k_lin.quant.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.2.ffn.lin1.quant.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.4.attention.attention_scores_matmul.input_quant_stubs.0.activation_post_process.zero_point', 'distilbert.transformer.layer.4.ffn.lin1.quant.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.5.attention.attention_scores_matmul.input_quant_stubs.1.activation_post_process.scale', 'distilbert.transformer.layer.3.attention.q_lin.quant.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.5.attention.attention_scores_matmul.input_quant_stubs.1.activation_post_process.zero_point', 'distilbert.transformer.layer.2.ffn.lin2.quant.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.0.ffn.lin2.quant.activation_post_process.scale', 'distilbert.transformer.layer.1.attention.out_lin.module.weight_fake_quant.activation_post_process.min_val', 'distilbert.transformer.layer.2.attention.context_layer_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'pre_classifier.module.bias', 'distilbert.transformer.layer.0.ffn.lin1.quant.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.3.ffn.lin2.quant.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.3.attention.k_lin.module.weight_fake_quant.zero_point', 'distilbert.embeddings.word_embeddings.weight_fake_quant.zero_point', 'distilbert.transformer.layer.0.attention.k_lin.quant.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.2.attention.out_lin.quant.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.0.attention.q_lin.quant.activation_post_process.scale', 'distilbert.transformer.layer.0.ffn.lin1.quant.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.5.attention.attention_scores_matmul.input_quant_stubs.0.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.3.ffn.lin1.module.weight_fake_quant.scale', 'distilbert.transformer.layer.5.ffn.lin2.module.weight_fake_quant.activation_post_process.eps', 'distilbert.transformer.layer.3.attention.k_lin.quant.activation_post_process.scale', 'distilbert.transformer.layer.4.attention.attention_scores_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.2.attention.q_lin.quant.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.4.attention.q_lin.module.weight_fake_quant.scale', 'distilbert.transformer.layer.3.attention.out_lin.quant.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.0.attention.out_lin.module.weight_fake_quant.observer_enabled', 'distilbert.transformer.layer.1.attention.k_lin.module.weight_fake_quant.zero_point', 'distilbert.transformer.layer.0.attention.k_lin.module.weight_fake_quant.observer_enabled', 'distilbert.transformer.layer.4.ffn.lin1.quant.activation_post_process.zero_point', 'distilbert.transformer.layer.2.attention.k_lin.quant.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.2.attention.q_lin.module.weight_fake_quant.observer_enabled', 'distilbert.transformer.layer.3.ffn.lin2.module.weight_fake_quant.observer_enabled', 'distilbert.transformer.layer.4.attention.k_lin.module.weight', 'distilbert.transformer.layer.2.attention.k_lin.module.weight_fake_quant.activation_post_process.eps', 'distilbert.transformer.layer.4.attention.attention_scores_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.1.attention.q_lin.module.weight', 'distilbert.transformer.layer.1.attention.attention_scores_matmul.output_quant_stubs.0.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.1.attention.v_lin.module.weight', 'distilbert.transformer.layer.2.attention.attention_scores_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.5.ffn.lin2.module.weight', 'distilbert.transformer.layer.0.attention.attention_scores_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.5.attention.attention_scores_matmul.input_quant_stubs.1.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.2.attention.context_layer_matmul.input_quant_stubs.0.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.5.ffn.lin1.module.weight_fake_quant.zero_point', 'distilbert.transformer.layer.3.attention.context_layer_matmul.input_quant_stubs.0.activation_post_process.observer_enabled', 'distilbert.transformer.layer.5.ffn.lin2.module.weight_fake_quant.observer_enabled', 'distilbert.transformer.layer.0.ffn.lin2.module.weight_fake_quant.scale', 'distilbert.transformer.layer.1.attention.out_lin.quant.activation_post_process.zero_point', 'distilbert.transformer.layer.3.ffn.lin2.quant.activation_post_process.scale', 'distilbert.transformer.layer.5.attention.q_lin.module.weight_fake_quant.activation_post_process.min_val', 'distilbert.transformer.layer.1.ffn.lin1.quant.activation_post_process.scale', 'distilbert.transformer.layer.3.ffn.lin2.module.weight_fake_quant.scale', 'distilbert.transformer.layer.3.attention.v_lin.module.weight_fake_quant.activation_post_process.max_val', 'distilbert.transformer.layer.3.attention.attention_scores_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.1.ffn.lin2.module.weight_fake_quant.zero_point', 'distilbert.transformer.layer.3.ffn.lin1.module.weight_fake_quant.fake_quant_enabled', 'distilbert.transformer.layer.5.attention.out_lin.quant.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.3.attention.out_lin.module.weight_fake_quant.fake_quant_enabled', 'pre_classifier.module.weight_fake_quant.activation_post_process.eps', 'distilbert.transformer.layer.5.attention.q_lin.module.weight_fake_quant.scale', 'distilbert.transformer.layer.4.attention.k_lin.quant.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.2.attention.v_lin.module.weight_fake_quant.activation_post_process.min_val', 'distilbert.transformer.layer.3.attention.out_lin.module.bias', 'distilbert.transformer.layer.2.attention.out_lin.module.weight_fake_quant.activation_post_process.eps', 'distilbert.transformer.layer.2.attention.q_lin.quant.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.1.attention.v_lin.quant.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.1.attention.attention_scores_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.4.attention.k_lin.module.weight_fake_quant.zero_point', 'distilbert.transformer.layer.1.attention.attention_scores_matmul.input_quant_stubs.1.activation_post_process.scale', 'distilbert.transformer.layer.0.attention.out_lin.quant.activation_post_process.observer_enabled', 'distilbert.transformer.layer.0.attention.attention_scores_matmul.input_quant_stubs.0.activation_post_process.observer_enabled', 'distilbert.transformer.layer.0.attention.attention_scores_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.0.attention.q_lin.module.weight_fake_quant.activation_post_process.eps', 'pre_classifier.quant.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.0.attention.context_layer_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.2.ffn.lin1.module.weight_fake_quant.observer_enabled', 'distilbert.transformer.layer.4.attention.k_lin.quant.activation_post_process.zero_point', 'distilbert.transformer.layer.4.attention.q_lin.quant.activation_post_process.zero_point', 'distilbert.transformer.layer.2.attention.q_lin.module.weight_fake_quant.zero_point', 'distilbert.transformer.layer.1.ffn.lin2.module.weight_fake_quant.fake_quant_enabled', 'distilbert.transformer.layer.2.attention.q_lin.module.weight_fake_quant.activation_post_process.min_val', 'distilbert.transformer.layer.4.attention.q_lin.module.weight', 'distilbert.transformer.layer.1.attention.v_lin.module.weight_fake_quant.activation_post_process.min_val', 'classifier.quant.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.0.attention.context_layer_matmul.input_quant_stubs.0.activation_post_process.scale', 'distilbert.transformer.layer.4.attention.out_lin.module.weight_fake_quant.zero_point', 'distilbert.transformer.layer.1.attention.context_layer_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.1.attention.attention_scores_matmul.input_quant_stubs.0.activation_post_process.zero_point', 'distilbert.transformer.layer.1.attention.context_layer_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.0.ffn.lin1.module.weight_fake_quant.activation_post_process.max_val', 'distilbert.transformer.layer.3.attention.out_lin.quant.activation_post_process.zero_point', 'distilbert.transformer.layer.3.attention.k_lin.module.bias', 'distilbert.transformer.layer.4.attention.q_lin.quant.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.2.attention.attention_scores_matmul.output_quant_stubs.0.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.0.attention.context_layer_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.3.attention.context_layer_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.0.ffn.lin1.module.weight', 'distilbert.transformer.layer.3.attention.q_lin.quant.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.4.attention.v_lin.module.weight', 'distilbert.transformer.layer.2.attention.out_lin.module.weight_fake_quant.zero_point', 'distilbert.transformer.layer.4.attention.v_lin.module.weight_fake_quant.zero_point', 'distilbert.transformer.layer.0.attention.q_lin.quant.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.1.attention.q_lin.module.weight_fake_quant.observer_enabled', 'distilbert.transformer.layer.1.ffn.lin1.module.weight_fake_quant.scale', 'classifier.quant.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.3.ffn.lin1.module.weight_fake_quant.activation_post_process.eps', 'distilbert.transformer.layer.2.attention.k_lin.quant.activation_post_process.scale', 'distilbert.transformer.layer.0.attention.v_lin.quant.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.1.attention.attention_scores_matmul.output_quant_stubs.0.activation_post_process.zero_point', 'distilbert.transformer.layer.4.ffn.lin2.module.weight_fake_quant.activation_post_process.max_val', 'classifier.module.weight_fake_quant.activation_post_process.max_val', 'distilbert.transformer.layer.5.attention.v_lin.quant.activation_post_process.observer_enabled', 'distilbert.transformer.layer.5.attention.out_lin.module.bias', 'distilbert.transformer.layer.0.attention.v_lin.module.weight_fake_quant.fake_quant_enabled', 'distilbert.transformer.layer.0.attention.k_lin.module.weight_fake_quant.activation_post_process.eps', 'distilbert.transformer.layer.1.attention.out_lin.quant.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.0.ffn.lin1.module.weight_fake_quant.scale', 'distilbert.transformer.layer.5.attention.q_lin.quant.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.1.attention.attention_scores_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.1.attention.k_lin.quant.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.5.attention.v_lin.module.weight', 'distilbert.transformer.layer.3.attention.k_lin.quant.activation_post_process.observer_enabled', 'distilbert.transformer.layer.3.attention.attention_scores_matmul.output_quant_stubs.0.activation_post_process.scale', 'distilbert.transformer.layer.4.attention.context_layer_matmul.input_quant_stubs.1.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.3.attention.out_lin.quant.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.0.attention.attention_scores_matmul.output_quant_stubs.0.activation_post_process.zero_point', 'distilbert.transformer.layer.5.attention.q_lin.module.bias', 'distilbert.transformer.layer.1.attention.v_lin.module.weight_fake_quant.fake_quant_enabled', 'distilbert.transformer.layer.3.ffn.lin1.quant.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.4.attention.out_lin.module.bias', 'distilbert.transformer.layer.2.ffn.lin1.quant.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.3.attention.out_lin.module.weight_fake_quant.observer_enabled', 'distilbert.transformer.layer.5.attention.context_layer_matmul.input_quant_stubs.0.activation_post_process.zero_point', 'distilbert.transformer.layer.5.attention.out_lin.quant.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.3.attention.v_lin.module.weight_fake_quant.scale', 'distilbert.transformer.layer.2.attention.attention_scores_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.2.attention.attention_scores_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.3.attention.k_lin.quant.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.5.attention.q_lin.quant.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.0.attention.k_lin.quant.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.1.ffn.lin1.module.bias', 'distilbert.embeddings.word_embeddings.weight_fake_quant.activation_post_process.max_val', 'distilbert.transformer.layer.0.attention.attention_scores_matmul.output_quant_stubs.0.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.1.attention.q_lin.module.bias', 'distilbert.transformer.layer.2.attention.attention_scores_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.4.ffn.lin1.quant.activation_post_process.observer_enabled', 'distilbert.transformer.layer.2.attention.k_lin.module.bias', 'distilbert.transformer.layer.4.attention.out_lin.quant.activation_post_process.zero_point', 'distilbert.transformer.layer.0.attention.q_lin.module.weight_fake_quant.fake_quant_enabled', 'distilbert.transformer.layer.1.attention.q_lin.quant.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.3.attention.context_layer_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.0.attention.out_lin.module.bias', 'distilbert.transformer.layer.2.ffn.lin1.quant.activation_post_process.zero_point', 'distilbert.transformer.layer.4.attention.v_lin.module.weight_fake_quant.observer_enabled', 'distilbert.transformer.layer.4.attention.out_lin.module.weight_fake_quant.activation_post_process.max_val', 'distilbert.embeddings.word_embeddings.weight_fake_quant.fake_quant_enabled', 'distilbert.transformer.layer.2.attention.out_lin.quant.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.4.attention.attention_scores_matmul.input_quant_stubs.0.activation_post_process.scale', 'distilbert.transformer.layer.3.ffn.lin1.quant.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.4.attention.k_lin.module.weight_fake_quant.observer_enabled', 'distilbert.transformer.layer.4.attention.attention_scores_matmul.output_quant_stubs.0.activation_post_process.scale', 'distilbert.transformer.layer.5.attention.attention_scores_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.5.ffn.lin1.module.weight_fake_quant.observer_enabled', 'distilbert.transformer.layer.0.ffn.lin2.quant.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.3.attention.v_lin.module.weight_fake_quant.activation_post_process.min_val', 'distilbert.transformer.layer.5.attention.attention_scores_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.0.attention.q_lin.module.weight_fake_quant.scale', 'distilbert.transformer.layer.4.attention.context_layer_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.5.attention.attention_scores_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.3.ffn.lin1.quant.activation_post_process.observer_enabled', 'distilbert.transformer.layer.3.attention.k_lin.quant.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.1.ffn.lin1.quant.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.2.ffn.lin1.quant.activation_post_process.observer_enabled', 'distilbert.transformer.layer.5.ffn.lin1.quant.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.0.attention.attention_scores_matmul.input_quant_stubs.1.activation_post_process.zero_point', 'distilbert.embeddings.word_embeddings.weight_fake_quant.scale', 'distilbert.transformer.layer.5.attention.out_lin.module.weight', 'distilbert.transformer.layer.0.attention.context_layer_matmul.input_quant_stubs.1.activation_post_process.observer_enabled', 'distilbert.transformer.layer.5.ffn.lin2.module.bias', 'distilbert.transformer.layer.1.attention.attention_scores_matmul.output_quant_stubs.0.activation_post_process.observer_enabled', 'distilbert.transformer.layer.4.attention.context_layer_matmul.input_quant_stubs.0.activation_post_process.zero_point', 'distilbert.transformer.layer.3.attention.v_lin.module.weight', 'distilbert.transformer.layer.3.attention.k_lin.quant.activation_post_process.zero_point', 'pre_classifier.module.weight', 'distilbert.transformer.layer.3.attention.attention_scores_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.0.ffn.lin1.module.bias', 'distilbert.transformer.layer.2.attention.v_lin.module.weight_fake_quant.fake_quant_enabled', 'distilbert.transformer.layer.0.attention.out_lin.quant.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.0.attention.attention_scores_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.4.attention.attention_scores_matmul.output_quant_stubs.0.activation_post_process.observer_enabled', 'distilbert.transformer.layer.0.attention.out_lin.module.weight_fake_quant.fake_quant_enabled', 'distilbert.transformer.layer.5.attention.context_layer_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.5.attention.k_lin.quant.activation_post_process.observer_enabled', 'distilbert.transformer.layer.1.attention.k_lin.quant.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.4.attention.q_lin.quant.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.0.ffn.lin1.quant.activation_post_process.zero_point', 'distilbert.transformer.layer.3.attention.attention_scores_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.0.attention.q_lin.module.weight_fake_quant.activation_post_process.min_val', 'distilbert.transformer.layer.1.ffn.lin2.quant.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.4.ffn.lin1.quant.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.3.attention.attention_scores_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.4.attention.k_lin.module.bias', 'distilbert.transformer.layer.4.attention.q_lin.quant.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.0.attention.q_lin.module.weight_fake_quant.zero_point', 'distilbert.transformer.layer.4.attention.v_lin.module.weight_fake_quant.activation_post_process.max_val', 'distilbert.transformer.layer.1.attention.attention_scores_matmul.input_quant_stubs.0.activation_post_process.observer_enabled', 'distilbert.transformer.layer.4.ffn.lin1.module.weight_fake_quant.zero_point', 'distilbert.transformer.layer.2.ffn.lin2.quant.activation_post_process.scale', 'distilbert.transformer.layer.3.attention.q_lin.quant.activation_post_process.scale', 'distilbert.transformer.layer.0.attention.attention_scores_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.5.attention.k_lin.module.weight_fake_quant.activation_post_process.min_val', 'classifier.module.weight_fake_quant.scale', 'distilbert.transformer.layer.2.attention.k_lin.module.weight_fake_quant.activation_post_process.max_val', 'distilbert.transformer.layer.1.attention.k_lin.module.weight_fake_quant.fake_quant_enabled', 'distilbert.transformer.layer.2.ffn.lin1.module.weight_fake_quant.fake_quant_enabled', 'distilbert.transformer.layer.2.ffn.lin1.module.weight_fake_quant.activation_post_process.eps', 'distilbert.transformer.layer.3.attention.context_layer_matmul.input_quant_stubs.0.activation_post_process.scale', 'distilbert.transformer.layer.5.attention.context_layer_matmul.input_quant_stubs.1.activation_post_process.scale', 'distilbert.transformer.layer.0.ffn.lin2.module.weight_fake_quant.activation_post_process.max_val', 'distilbert.transformer.layer.4.attention.q_lin.quant.activation_post_process.scale', 'distilbert.transformer.layer.4.attention.out_lin.quant.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.0.attention.q_lin.quant.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.2.attention.q_lin.module.weight_fake_quant.activation_post_process.eps', 'distilbert.transformer.layer.3.attention.q_lin.quant.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.5.attention.context_layer_matmul.input_quant_stubs.1.activation_post_process.observer_enabled', 'distilbert.transformer.layer.0.ffn.lin2.module.bias', 'distilbert.transformer.layer.3.attention.context_layer_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.5.attention.context_layer_matmul.input_quant_stubs.0.activation_post_process.scale', 'distilbert.transformer.layer.1.attention.out_lin.quant.activation_post_process.scale', 'distilbert.transformer.layer.1.attention.context_layer_matmul.input_quant_stubs.0.activation_post_process.observer_enabled', 'distilbert.transformer.layer.4.attention.out_lin.module.weight', 'distilbert.transformer.layer.2.ffn.lin2.module.weight_fake_quant.scale', 'distilbert.transformer.layer.0.attention.context_layer_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.3.attention.k_lin.module.weight_fake_quant.fake_quant_enabled', 'distilbert.transformer.layer.5.ffn.lin2.quant.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.3.attention.v_lin.quant.activation_post_process.observer_enabled', 'distilbert.transformer.layer.2.attention.q_lin.module.weight_fake_quant.scale', 'distilbert.embeddings.position_embeddings.weight_fake_quant.activation_post_process.eps', 'distilbert.transformer.layer.5.ffn.lin1.module.weight_fake_quant.scale', 'distilbert.transformer.layer.4.attention.k_lin.module.weight_fake_quant.activation_post_process.max_val', 'distilbert.transformer.layer.3.ffn.lin2.module.weight', 'distilbert.transformer.layer.1.attention.attention_scores_matmul.input_quant_stubs.1.activation_post_process.zero_point', 'pre_classifier.quant.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.5.attention.attention_scores_matmul.output_quant_stubs.0.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.2.ffn.lin2.quant.activation_post_process.zero_point', 'distilbert.transformer.layer.1.attention.out_lin.module.weight', 'distilbert.transformer.layer.2.ffn.lin2.module.bias', 'distilbert.transformer.layer.5.attention.v_lin.module.weight_fake_quant.activation_post_process.max_val', 'distilbert.transformer.layer.5.attention.attention_scores_matmul.input_quant_stubs.0.activation_post_process.zero_point', 'distilbert.transformer.layer.3.attention.k_lin.module.weight_fake_quant.activation_post_process.max_val', 'distilbert.transformer.layer.0.attention.k_lin.module.weight_fake_quant.activation_post_process.min_val', 'distilbert.transformer.layer.5.attention.out_lin.quant.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.0.attention.out_lin.module.weight_fake_quant.activation_post_process.eps', 'distilbert.transformer.layer.5.attention.attention_scores_matmul.input_quant_stubs.0.activation_post_process.scale', 'distilbert.embeddings.position_embeddings.weight_fake_quant.activation_post_process.min_val', 'distilbert.transformer.layer.3.attention.context_layer_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.1.ffn.lin1.quant.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.4.attention.v_lin.quant.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.5.attention.out_lin.quant.activation_post_process.zero_point', 'distilbert.transformer.layer.1.attention.k_lin.module.weight_fake_quant.activation_post_process.min_val', 'classifier.quant.activation_post_process.zero_point', 'distilbert.transformer.layer.1.attention.out_lin.module.weight_fake_quant.zero_point', 'distilbert.transformer.layer.4.attention.v_lin.module.bias', 'distilbert.transformer.layer.2.attention.v_lin.module.weight_fake_quant.observer_enabled', 'distilbert.transformer.layer.5.attention.v_lin.quant.activation_post_process.zero_point', 'distilbert.transformer.layer.3.attention.k_lin.module.weight_fake_quant.scale', 'pre_classifier.module.weight_fake_quant.observer_enabled', 'distilbert.transformer.layer.4.attention.attention_scores_matmul.input_quant_stubs.1.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.5.attention.k_lin.quant.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.4.attention.v_lin.quant.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.4.attention.out_lin.module.weight_fake_quant.scale', 'distilbert.transformer.layer.5.attention.k_lin.module.weight_fake_quant.scale', 'distilbert.transformer.layer.4.attention.attention_scores_matmul.input_quant_stubs.1.activation_post_process.zero_point', 'distilbert.transformer.layer.5.ffn.lin1.module.bias', 'distilbert.transformer.layer.2.attention.v_lin.quant.activation_post_process.zero_point', 'distilbert.transformer.layer.5.attention.k_lin.quant.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.3.attention.attention_scores_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.3.attention.context_layer_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.1.ffn.lin1.module.weight', 'distilbert.transformer.layer.3.attention.out_lin.module.weight_fake_quant.activation_post_process.min_val', 'distilbert.transformer.layer.3.attention.v_lin.module.weight_fake_quant.observer_enabled', 'distilbert.transformer.layer.2.attention.out_lin.module.weight_fake_quant.observer_enabled', 'distilbert.transformer.layer.3.ffn.lin1.quant.activation_post_process.scale', 'distilbert.transformer.layer.0.attention.context_layer_matmul.input_quant_stubs.0.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.4.attention.out_lin.quant.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.2.attention.v_lin.module.weight_fake_quant.activation_post_process.eps', 'distilbert.transformer.layer.0.attention.v_lin.module.bias', 'distilbert.transformer.layer.5.attention.q_lin.module.weight_fake_quant.zero_point', 'distilbert.transformer.layer.1.attention.v_lin.quant.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.0.attention.q_lin.module.weight', 'distilbert.transformer.layer.4.ffn.lin2.quant.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.3.attention.context_layer_matmul.input_quant_stubs.0.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.1.attention.context_layer_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.1.ffn.lin2.module.weight_fake_quant.activation_post_process.min_val', 'distilbert.transformer.layer.2.attention.attention_scores_matmul.input_quant_stubs.0.activation_post_process.scale', 'distilbert.transformer.layer.0.ffn.lin2.quant.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.2.ffn.lin2.module.weight_fake_quant.activation_post_process.eps', 'distilbert.transformer.layer.0.attention.attention_scores_matmul.input_quant_stubs.0.activation_post_process.scale', 'distilbert.transformer.layer.3.attention.v_lin.module.weight_fake_quant.fake_quant_enabled', 'distilbert.transformer.layer.3.attention.v_lin.module.weight_fake_quant.activation_post_process.eps', 'distilbert.transformer.layer.2.ffn.lin1.quant.activation_post_process.scale', 'distilbert.transformer.layer.4.attention.v_lin.quant.activation_post_process.zero_point', 'distilbert.transformer.layer.5.attention.context_layer_matmul.input_quant_stubs.1.activation_post_process.zero_point', 'distilbert.transformer.layer.2.attention.attention_scores_matmul.input_quant_stubs.1.activation_post_process.observer_enabled', 'distilbert.transformer.layer.4.attention.context_layer_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'pre_classifier.quant.activation_post_process.zero_point', 'distilbert.transformer.layer.0.attention.v_lin.module.weight_fake_quant.activation_post_process.max_val', 'distilbert.transformer.layer.0.attention.v_lin.module.weight_fake_quant.activation_post_process.min_val', 'distilbert.transformer.layer.5.attention.out_lin.module.weight_fake_quant.activation_post_process.eps', 'distilbert.transformer.layer.0.attention.context_layer_matmul.input_quant_stubs.1.activation_post_process.zero_point', 'distilbert.transformer.layer.0.attention.v_lin.quant.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.3.attention.attention_scores_matmul.input_quant_stubs.1.activation_post_process.scale', 'distilbert.transformer.layer.0.attention.k_lin.module.weight_fake_quant.fake_quant_enabled', 'distilbert.transformer.layer.0.attention.attention_scores_matmul.input_quant_stubs.1.activation_post_process.scale', 'distilbert.transformer.layer.3.attention.q_lin.module.weight_fake_quant.activation_post_process.min_val', 'distilbert.transformer.layer.2.attention.attention_scores_matmul.input_quant_stubs.1.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.5.attention.q_lin.module.weight_fake_quant.activation_post_process.max_val', 'distilbert.transformer.layer.0.attention.k_lin.quant.activation_post_process.zero_point', 'distilbert.transformer.layer.3.attention.attention_scores_matmul.input_quant_stubs.0.activation_post_process.zero_point', 'distilbert.transformer.layer.2.attention.context_layer_matmul.input_quant_stubs.1.activation_post_process.scale', 'distilbert.transformer.layer.3.attention.q_lin.module.weight_fake_quant.fake_quant_enabled', 'distilbert.transformer.layer.3.attention.v_lin.quant.activation_post_process.zero_point', 'distilbert.transformer.layer.5.attention.v_lin.quant.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.3.attention.v_lin.quant.activation_post_process.scale', 'distilbert.transformer.layer.0.attention.v_lin.quant.activation_post_process.scale', 'distilbert.transformer.layer.2.attention.context_layer_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.4.ffn.lin1.quant.activation_post_process.scale', 'distilbert.transformer.layer.3.attention.attention_scores_matmul.input_quant_stubs.1.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.1.attention.k_lin.quant.activation_post_process.observer_enabled', 'distilbert.transformer.layer.1.attention.out_lin.module.weight_fake_quant.activation_post_process.eps', 'distilbert.transformer.layer.3.attention.q_lin.module.weight_fake_quant.activation_post_process.max_val', 'distilbert.transformer.layer.5.attention.context_layer_matmul.input_quant_stubs.1.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.5.ffn.lin1.module.weight_fake_quant.activation_post_process.min_val', 'distilbert.transformer.layer.3.attention.attention_scores_matmul.output_quant_stubs.0.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.4.ffn.lin2.module.bias', 'distilbert.transformer.layer.2.attention.q_lin.module.weight_fake_quant.fake_quant_enabled', 'distilbert.transformer.layer.0.ffn.lin1.module.weight_fake_quant.activation_post_process.min_val', 'distilbert.transformer.layer.5.attention.v_lin.quant.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.1.attention.v_lin.quant.activation_post_process.observer_enabled', 'distilbert.transformer.layer.4.attention.attention_scores_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.5.attention.context_layer_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'classifier.module.weight_fake_quant.observer_enabled', 'distilbert.transformer.layer.3.attention.out_lin.quant.activation_post_process.scale', 'distilbert.transformer.layer.1.attention.context_layer_matmul.input_quant_stubs.0.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.0.attention.k_lin.module.weight_fake_quant.activation_post_process.max_val', 'distilbert.transformer.layer.1.attention.attention_scores_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.3.ffn.lin1.quant.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.0.ffn.lin1.quant.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.2.attention.q_lin.module.weight', 'distilbert.transformer.layer.1.attention.context_layer_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.2.attention.attention_scores_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.3.attention.q_lin.quant.activation_post_process.observer_enabled', 'distilbert.transformer.layer.4.attention.out_lin.module.weight_fake_quant.fake_quant_enabled', 'pre_classifier.quant.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.2.attention.k_lin.quant.activation_post_process.zero_point', 'distilbert.transformer.layer.1.attention.attention_scores_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.1.attention.k_lin.module.weight_fake_quant.activation_post_process.max_val', 'distilbert.transformer.layer.0.attention.attention_scores_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.2.ffn.lin1.module.weight_fake_quant.activation_post_process.min_val', 'distilbert.transformer.layer.3.attention.k_lin.module.weight_fake_quant.activation_post_process.eps', 'distilbert.transformer.layer.3.attention.out_lin.quant.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.2.attention.q_lin.module.weight_fake_quant.activation_post_process.max_val', 'distilbert.transformer.layer.1.attention.attention_scores_matmul.input_quant_stubs.1.activation_post_process.observer_enabled', 'distilbert.transformer.layer.2.attention.v_lin.quant.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.5.attention.context_layer_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.0.attention.k_lin.module.weight_fake_quant.zero_point', 'distilbert.transformer.layer.5.ffn.lin1.quant.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.1.ffn.lin2.module.weight', 'distilbert.transformer.layer.2.attention.out_lin.quant.activation_post_process.scale', 'distilbert.transformer.layer.4.attention.out_lin.quant.activation_post_process.observer_enabled', 'distilbert.transformer.layer.1.attention.out_lin.module.weight_fake_quant.fake_quant_enabled', 'distilbert.transformer.layer.0.attention.k_lin.module.bias', 'distilbert.transformer.layer.3.attention.q_lin.quant.activation_post_process.zero_point', 'distilbert.transformer.layer.2.attention.context_layer_matmul.input_quant_stubs.0.activation_post_process.scale', 'distilbert.transformer.layer.5.attention.k_lin.module.weight_fake_quant.observer_enabled', 'distilbert.transformer.layer.2.attention.k_lin.module.weight_fake_quant.activation_post_process.min_val', 'distilbert.transformer.layer.1.ffn.lin2.module.weight_fake_quant.observer_enabled', 'distilbert.transformer.layer.0.ffn.lin2.module.weight_fake_quant.observer_enabled', 'distilbert.transformer.layer.5.attention.attention_scores_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.1.attention.q_lin.quant.activation_post_process.zero_point', 'distilbert.transformer.layer.2.attention.context_layer_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.eps', 'classifier.module.weight_fake_quant.activation_post_process.min_val', 'distilbert.transformer.layer.2.attention.k_lin.quant.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.3.ffn.lin1.module.weight', 'distilbert.transformer.layer.3.ffn.lin2.quant.activation_post_process.observer_enabled', 'distilbert.transformer.layer.4.attention.out_lin.module.weight_fake_quant.observer_enabled', 'distilbert.transformer.layer.5.attention.k_lin.module.bias', 'distilbert.transformer.layer.5.attention.attention_scores_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.2.attention.attention_scores_matmul.input_quant_stubs.0.activation_post_process.zero_point', 'distilbert.transformer.layer.1.ffn.lin1.quant.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.4.ffn.lin2.module.weight_fake_quant.zero_point', 'distilbert.transformer.layer.4.attention.v_lin.module.weight_fake_quant.activation_post_process.eps', 'distilbert.transformer.layer.5.ffn.lin2.module.weight_fake_quant.activation_post_process.max_val', 'distilbert.transformer.layer.3.attention.attention_scores_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.1.attention.attention_scores_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.3.attention.q_lin.module.weight_fake_quant.observer_enabled', 'distilbert.transformer.layer.4.attention.q_lin.module.weight_fake_quant.activation_post_process.eps', 'distilbert.transformer.layer.0.ffn.lin2.module.weight_fake_quant.zero_point', 'distilbert.transformer.layer.1.attention.q_lin.quant.activation_post_process.observer_enabled', 'distilbert.transformer.layer.5.attention.attention_scores_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.1.ffn.lin1.module.weight_fake_quant.activation_post_process.max_val', 'distilbert.transformer.layer.1.attention.attention_scores_matmul.input_quant_stubs.0.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.5.attention.v_lin.module.weight_fake_quant.scale', 'distilbert.transformer.layer.0.attention.k_lin.quant.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.1.attention.context_layer_matmul.input_quant_stubs.1.activation_post_process.observer_enabled', 'distilbert.transformer.layer.5.attention.k_lin.quant.activation_post_process.zero_point', 'distilbert.transformer.layer.0.attention.v_lin.module.weight_fake_quant.zero_point', 'distilbert.transformer.layer.1.attention.v_lin.module.weight_fake_quant.observer_enabled', 'distilbert.transformer.layer.5.ffn.lin1.quant.activation_post_process.observer_enabled', 'distilbert.transformer.layer.0.ffn.lin2.module.weight', 'distilbert.transformer.layer.2.ffn.lin1.quant.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.2.attention.context_layer_matmul.input_quant_stubs.1.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.2.attention.out_lin.module.weight_fake_quant.fake_quant_enabled', 'distilbert.transformer.layer.1.attention.q_lin.module.weight_fake_quant.scale', 'distilbert.transformer.layer.1.ffn.lin2.quant.activation_post_process.observer_enabled', 'distilbert.transformer.layer.0.attention.q_lin.module.weight_fake_quant.observer_enabled', 'distilbert.transformer.layer.3.attention.context_layer_matmul.input_quant_stubs.1.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.2.attention.v_lin.quant.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.4.attention.attention_scores_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.0.ffn.lin2.module.weight_fake_quant.activation_post_process.eps', 'distilbert.transformer.layer.1.attention.attention_scores_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.5.ffn.lin1.quant.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.5.ffn.lin2.module.weight_fake_quant.activation_post_process.min_val', 'distilbert.transformer.layer.1.ffn.lin1.quant.activation_post_process.observer_enabled', 'distilbert.transformer.layer.0.ffn.lin1.quant.activation_post_process.scale', 'distilbert.transformer.layer.3.attention.k_lin.quant.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.2.attention.k_lin.quant.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.3.attention.attention_scores_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.2.attention.out_lin.module.weight_fake_quant.scale', 'distilbert.transformer.layer.3.attention.v_lin.quant.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.4.attention.v_lin.quant.activation_post_process.observer_enabled', 'distilbert.transformer.layer.4.attention.attention_scores_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.5.attention.k_lin.module.weight_fake_quant.zero_point', 'distilbert.transformer.layer.0.attention.out_lin.quant.activation_post_process.zero_point', 'distilbert.transformer.layer.3.ffn.lin1.quant.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.2.ffn.lin2.quant.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.1.attention.q_lin.quant.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.4.attention.attention_scores_matmul.output_quant_stubs.0.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.0.attention.context_layer_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.0.attention.v_lin.quant.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.5.ffn.lin2.quant.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.5.attention.out_lin.quant.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.1.attention.attention_scores_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.3.attention.v_lin.quant.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.4.ffn.lin2.quant.activation_post_process.zero_point', 'distilbert.transformer.layer.5.attention.attention_scores_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.0.attention.context_layer_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.4.attention.out_lin.quant.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.4.ffn.lin1.module.weight_fake_quant.scale', 'distilbert.transformer.layer.0.attention.k_lin.quant.activation_post_process.scale', 'distilbert.transformer.layer.3.attention.attention_scores_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.4.attention.out_lin.module.weight_fake_quant.activation_post_process.min_val', 'distilbert.transformer.layer.4.ffn.lin1.module.weight_fake_quant.observer_enabled', 'distilbert.transformer.layer.2.attention.k_lin.module.weight_fake_quant.fake_quant_enabled', 'distilbert.embeddings.position_embeddings.weight_fake_quant.fake_quant_enabled', 'distilbert.transformer.layer.2.ffn.lin2.quant.activation_post_process.observer_enabled', 'distilbert.transformer.layer.3.attention.q_lin.module.weight', 'distilbert.transformer.layer.0.ffn.lin1.module.weight_fake_quant.fake_quant_enabled', 'distilbert.transformer.layer.2.attention.v_lin.quant.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.3.attention.q_lin.module.bias', 'distilbert.transformer.layer.4.attention.k_lin.quant.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.3.attention.attention_scores_matmul.input_quant_stubs.0.activation_post_process.scale', 'distilbert.transformer.layer.2.attention.context_layer_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.0.attention.context_layer_matmul.input_quant_stubs.1.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.4.attention.context_layer_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.1.attention.k_lin.quant.activation_post_process.scale', 'distilbert.embeddings.position_embeddings.weight_fake_quant.activation_post_process.max_val', 'distilbert.transformer.layer.2.ffn.lin2.module.weight_fake_quant.observer_enabled', 'distilbert.transformer.layer.2.attention.q_lin.quant.activation_post_process.zero_point', 'distilbert.transformer.layer.0.attention.out_lin.module.weight_fake_quant.zero_point', 'distilbert.transformer.layer.0.attention.out_lin.quant.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.5.attention.attention_scores_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.0.attention.attention_scores_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.0.attention.context_layer_matmul.input_quant_stubs.0.activation_post_process.observer_enabled', 'distilbert.transformer.layer.0.attention.out_lin.quant.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.5.attention.v_lin.quant.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.2.attention.attention_scores_matmul.output_quant_stubs.0.activation_post_process.zero_point', 'distilbert.transformer.layer.5.attention.k_lin.quant.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.3.ffn.lin2.module.bias', 'distilbert.transformer.layer.2.attention.out_lin.module.weight_fake_quant.activation_post_process.min_val', 'distilbert.transformer.layer.1.ffn.lin1.quant.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.1.attention.q_lin.module.weight_fake_quant.fake_quant_enabled', 'distilbert.transformer.layer.1.attention.k_lin.quant.activation_post_process.zero_point', 'distilbert.transformer.layer.0.attention.k_lin.module.weight_fake_quant.scale', 'distilbert.transformer.layer.1.ffn.lin2.module.weight_fake_quant.activation_post_process.max_val', 'distilbert.transformer.layer.1.attention.k_lin.module.weight_fake_quant.activation_post_process.eps', 'distilbert.transformer.layer.4.ffn.lin1.quant.activation_post_process.activation_post_process.min_val', 'classifier.module.weight_fake_quant.zero_point', 'distilbert.transformer.layer.1.ffn.lin2.module.bias', 'distilbert.transformer.layer.3.attention.attention_scores_matmul.input_quant_stubs.1.activation_post_process.zero_point', 'distilbert.transformer.layer.3.attention.context_layer_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.5.attention.v_lin.quant.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.4.attention.q_lin.module.weight_fake_quant.zero_point', 'distilbert.transformer.layer.0.attention.k_lin.quant.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.5.attention.context_layer_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.5.attention.v_lin.module.weight_fake_quant.zero_point', 'distilbert.transformer.layer.1.attention.k_lin.module.weight_fake_quant.observer_enabled', 'distilbert.transformer.layer.2.attention.out_lin.module.bias', 'distilbert.transformer.layer.4.attention.q_lin.module.weight_fake_quant.observer_enabled', 'distilbert.transformer.layer.0.attention.out_lin.module.weight_fake_quant.activation_post_process.max_val', 'distilbert.transformer.layer.5.attention.v_lin.quant.activation_post_process.scale', 'distilbert.transformer.layer.0.ffn.lin2.module.weight_fake_quant.fake_quant_enabled', 'distilbert.transformer.layer.1.attention.context_layer_matmul.input_quant_stubs.0.activation_post_process.zero_point', 'distilbert.transformer.layer.4.attention.q_lin.module.weight_fake_quant.activation_post_process.min_val', 'distilbert.transformer.layer.4.attention.attention_scores_matmul.input_quant_stubs.0.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.3.ffn.lin2.module.weight_fake_quant.activation_post_process.min_val', 'distilbert.transformer.layer.4.ffn.lin1.module.weight_fake_quant.fake_quant_enabled', 'distilbert.transformer.layer.0.attention.q_lin.module.weight_fake_quant.activation_post_process.max_val', 'distilbert.transformer.layer.4.ffn.lin2.quant.activation_post_process.activation_post_process.min_val', 'distilbert.embeddings.word_embeddings.weight_fake_quant.activation_post_process.min_val', 'distilbert.transformer.layer.0.attention.attention_scores_matmul.input_quant_stubs.0.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.1.attention.out_lin.module.weight_fake_quant.scale', 'distilbert.transformer.layer.2.attention.attention_scores_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.5.attention.attention_scores_matmul.output_quant_stubs.0.activation_post_process.scale', 'distilbert.transformer.layer.0.attention.q_lin.quant.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.1.attention.k_lin.module.bias', 'distilbert.transformer.layer.5.attention.v_lin.module.weight_fake_quant.activation_post_process.min_val', 'classifier.quant.activation_post_process.scale', 'distilbert.transformer.layer.2.ffn.lin2.module.weight_fake_quant.zero_point', 'distilbert.transformer.layer.1.attention.out_lin.module.weight_fake_quant.activation_post_process.max_val', 'distilbert.transformer.layer.2.ffn.lin2.module.weight_fake_quant.fake_quant_enabled', 'distilbert.transformer.layer.0.attention.out_lin.quant.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.3.attention.attention_scores_matmul.input_quant_stubs.0.activation_post_process.observer_enabled', 'distilbert.transformer.layer.0.attention.attention_scores_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.0.attention.attention_scores_matmul.input_quant_stubs.1.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.1.attention.out_lin.quant.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.5.attention.context_layer_matmul.input_quant_stubs.0.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.4.attention.attention_scores_matmul.output_quant_stubs.0.activation_post_process.zero_point', 'distilbert.transformer.layer.0.attention.v_lin.quant.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.4.attention.out_lin.quant.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.4.attention.v_lin.quant.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.3.attention.context_layer_matmul.input_quant_stubs.1.activation_post_process.observer_enabled', 'distilbert.transformer.layer.1.attention.q_lin.module.weight_fake_quant.activation_post_process.min_val', 'distilbert.transformer.layer.0.attention.q_lin.quant.activation_post_process.observer_enabled', 'distilbert.transformer.layer.1.attention.v_lin.module.weight_fake_quant.activation_post_process.eps', 'distilbert.transformer.layer.3.attention.out_lin.module.weight_fake_quant.zero_point', 'distilbert.transformer.layer.0.attention.out_lin.module.weight_fake_quant.activation_post_process.min_val', 'distilbert.transformer.layer.2.attention.attention_scores_matmul.input_quant_stubs.0.activation_post_process.observer_enabled', 'distilbert.transformer.layer.0.attention.out_lin.module.weight_fake_quant.scale', 'classifier.module.weight_fake_quant.activation_post_process.eps', 'distilbert.transformer.layer.1.attention.context_layer_matmul.input_quant_stubs.1.activation_post_process.scale', 'distilbert.transformer.layer.3.attention.context_layer_matmul.input_quant_stubs.1.activation_post_process.scale', 'distilbert.transformer.layer.2.attention.attention_scores_matmul.input_quant_stubs.1.activation_post_process.scale', 'distilbert.transformer.layer.4.attention.q_lin.quant.activation_post_process.observer_enabled', 'distilbert.transformer.layer.3.attention.attention_scores_matmul.input_quant_stubs.0.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.5.attention.attention_scores_matmul.output_quant_stubs.0.activation_post_process.zero_point', 'distilbert.transformer.layer.5.attention.out_lin.module.weight_fake_quant.activation_post_process.max_val', 'distilbert.transformer.layer.4.attention.k_lin.quant.activation_post_process.observer_enabled', 'distilbert.transformer.layer.4.attention.attention_scores_matmul.input_quant_stubs.1.activation_post_process.scale', 'distilbert.transformer.layer.4.ffn.lin1.module.weight_fake_quant.activation_post_process.max_val', 'distilbert.transformer.layer.5.ffn.lin2.quant.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.1.attention.v_lin.module.bias', 'distilbert.transformer.layer.3.attention.v_lin.quant.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.0.attention.attention_scores_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.2.attention.q_lin.quant.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.4.attention.v_lin.quant.activation_post_process.scale', 'distilbert.transformer.layer.0.ffn.lin2.quant.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.4.attention.attention_scores_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.5.attention.out_lin.quant.activation_post_process.observer_enabled', 'distilbert.transformer.layer.4.attention.k_lin.module.weight_fake_quant.activation_post_process.min_val', 'distilbert.transformer.layer.1.attention.v_lin.module.weight_fake_quant.zero_point', 'distilbert.transformer.layer.3.attention.out_lin.module.weight', 'distilbert.transformer.layer.3.ffn.lin2.module.weight_fake_quant.activation_post_process.eps', 'pre_classifier.quant.activation_post_process.scale', 'distilbert.transformer.layer.0.attention.q_lin.module.bias', 'distilbert.transformer.layer.2.attention.k_lin.module.weight_fake_quant.scale', 'distilbert.transformer.layer.2.ffn.lin1.module.weight_fake_quant.scale', 'distilbert.transformer.layer.1.attention.context_layer_matmul.input_quant_stubs.1.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.2.ffn.lin2.module.weight', 'distilbert.transformer.layer.5.attention.k_lin.module.weight', 'distilbert.transformer.layer.4.ffn.lin2.module.weight_fake_quant.observer_enabled', 'distilbert.transformer.layer.0.attention.context_layer_matmul.input_quant_stubs.0.activation_post_process.zero_point', 'distilbert.transformer.layer.5.ffn.lin1.module.weight_fake_quant.activation_post_process.max_val', 'classifier.quant.activation_post_process.fake_quant_enabled', 'classifier.module.weight_fake_quant.fake_quant_enabled', 'distilbert.transformer.layer.4.attention.v_lin.module.weight_fake_quant.scale', 'distilbert.transformer.layer.1.attention.attention_scores_matmul.input_quant_stubs.0.activation_post_process.scale', 'distilbert.transformer.layer.1.attention.v_lin.quant.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.1.attention.attention_scores_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.0.ffn.lin1.module.weight_fake_quant.observer_enabled', 'distilbert.transformer.layer.1.ffn.lin2.module.weight_fake_quant.scale', 'distilbert.transformer.layer.2.attention.out_lin.quant.activation_post_process.zero_point', 'distilbert.transformer.layer.3.attention.q_lin.module.weight_fake_quant.scale', 'distilbert.transformer.layer.0.attention.context_layer_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.3.attention.out_lin.module.weight_fake_quant.scale', 'distilbert.transformer.layer.4.ffn.lin2.module.weight', 'distilbert.transformer.layer.0.attention.out_lin.module.weight', 'distilbert.transformer.layer.5.ffn.lin1.quant.activation_post_process.scale', 'pre_classifier.quant.activation_post_process.observer_enabled', 'distilbert.transformer.layer.2.attention.v_lin.module.weight_fake_quant.zero_point', 'distilbert.transformer.layer.0.ffn.lin2.module.weight_fake_quant.activation_post_process.min_val', 'distilbert.transformer.layer.1.attention.q_lin.quant.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.4.attention.k_lin.quant.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.5.attention.attention_scores_matmul.input_quant_stubs.1.activation_post_process.observer_enabled', 'distilbert.transformer.layer.3.ffn.lin2.quant.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.5.ffn.lin1.module.weight', 'distilbert.transformer.layer.3.ffn.lin2.quant.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.3.attention.out_lin.quant.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.5.attention.v_lin.module.weight_fake_quant.fake_quant_enabled', 'classifier.quant.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.3.ffn.lin1.module.weight_fake_quant.zero_point', 'distilbert.transformer.layer.3.attention.v_lin.module.weight_fake_quant.zero_point', 'distilbert.transformer.layer.3.ffn.lin2.module.weight_fake_quant.activation_post_process.max_val', 'distilbert.transformer.layer.2.attention.out_lin.quant.activation_post_process.observer_enabled', 'distilbert.transformer.layer.0.attention.v_lin.module.weight_fake_quant.observer_enabled', 'distilbert.transformer.layer.2.attention.out_lin.module.weight', 'distilbert.transformer.layer.3.ffn.lin1.module.bias', 'distilbert.embeddings.position_embeddings.weight_fake_quant.zero_point', 'distilbert.transformer.layer.3.attention.k_lin.quant.activation_post_process.fake_quant_enabled', 'classifier.module.bias', 'distilbert.transformer.layer.1.attention.q_lin.module.weight_fake_quant.zero_point', 'distilbert.transformer.layer.5.attention.out_lin.module.weight_fake_quant.observer_enabled', 'distilbert.transformer.layer.4.ffn.lin2.module.weight_fake_quant.activation_post_process.min_val', 'distilbert.transformer.layer.0.attention.v_lin.quant.activation_post_process.zero_point', 'distilbert.transformer.layer.1.ffn.lin1.module.weight_fake_quant.zero_point', 'distilbert.transformer.layer.0.ffn.lin1.quant.activation_post_process.observer_enabled', 'distilbert.transformer.layer.4.attention.attention_scores_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.eps', 'pre_classifier.module.weight_fake_quant.fake_quant_enabled', 'distilbert.transformer.layer.1.attention.context_layer_matmul.input_quant_stubs.1.activation_post_process.zero_point', 'distilbert.transformer.layer.1.ffn.lin2.module.weight_fake_quant.activation_post_process.eps', 'distilbert.transformer.layer.2.attention.q_lin.quant.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.2.attention.q_lin.quant.activation_post_process.observer_enabled', 'distilbert.transformer.layer.0.attention.context_layer_matmul.input_quant_stubs.1.activation_post_process.scale', 'distilbert.transformer.layer.5.attention.attention_scores_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.3.ffn.lin1.module.weight_fake_quant.activation_post_process.min_val', 'distilbert.transformer.layer.5.attention.k_lin.quant.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.1.attention.v_lin.module.weight_fake_quant.scale', 'distilbert.transformer.layer.0.ffn.lin2.quant.activation_post_process.observer_enabled', 'distilbert.transformer.layer.4.attention.context_layer_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.4.ffn.lin1.module.bias', 'distilbert.transformer.layer.1.ffn.lin1.module.weight_fake_quant.activation_post_process.min_val', 'distilbert.transformer.layer.0.attention.attention_scores_matmul.output_quant_stubs.0.activation_post_process.scale', 'distilbert.transformer.layer.3.ffn.lin1.quant.activation_post_process.zero_point', 'distilbert.transformer.layer.5.attention.k_lin.module.weight_fake_quant.fake_quant_enabled', 'distilbert.transformer.layer.2.attention.context_layer_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.5.attention.v_lin.module.bias', 'pre_classifier.module.weight_fake_quant.scale', 'distilbert.embeddings.position_embeddings.weight_fake_quant.observer_enabled', 'distilbert.transformer.layer.1.ffn.lin1.module.weight_fake_quant.fake_quant_enabled', 'distilbert.transformer.layer.0.attention.v_lin.module.weight_fake_quant.activation_post_process.eps', 'distilbert.transformer.layer.0.attention.attention_scores_matmul.output_quant_stubs.0.activation_post_process.observer_enabled', 'distilbert.transformer.layer.0.attention.k_lin.quant.activation_post_process.observer_enabled', 'distilbert.transformer.layer.3.attention.q_lin.quant.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.4.ffn.lin2.module.weight_fake_quant.activation_post_process.eps', 'distilbert.transformer.layer.1.attention.k_lin.module.weight', 'distilbert.transformer.layer.0.attention.attention_scores_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.3.attention.attention_scores_matmul.output_quant_stubs.0.activation_post_process.observer_enabled', 'distilbert.transformer.layer.2.attention.attention_scores_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.1.attention.q_lin.module.weight_fake_quant.activation_post_process.max_val', 'distilbert.transformer.layer.5.ffn.lin2.module.weight_fake_quant.zero_point', 'distilbert.transformer.layer.4.attention.attention_scores_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.5.attention.q_lin.module.weight_fake_quant.observer_enabled', 'distilbert.transformer.layer.2.attention.k_lin.module.weight_fake_quant.observer_enabled', 'distilbert.transformer.layer.1.ffn.lin1.module.weight_fake_quant.activation_post_process.eps', 'distilbert.transformer.layer.5.attention.k_lin.module.weight_fake_quant.activation_post_process.eps', 'distilbert.transformer.layer.2.ffn.lin2.quant.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.2.attention.v_lin.module.weight', 'distilbert.transformer.layer.4.attention.k_lin.module.weight_fake_quant.activation_post_process.eps', 'distilbert.transformer.layer.3.attention.context_layer_matmul.input_quant_stubs.1.activation_post_process.zero_point', 'distilbert.transformer.layer.5.attention.v_lin.module.weight_fake_quant.activation_post_process.eps', 'distilbert.transformer.layer.1.attention.out_lin.module.weight_fake_quant.observer_enabled', 'distilbert.embeddings.word_embeddings.weight_fake_quant.activation_post_process.eps', 'distilbert.transformer.layer.1.attention.context_layer_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.4.attention.v_lin.quant.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.4.ffn.lin2.module.weight_fake_quant.scale', 'distilbert.transformer.layer.3.attention.q_lin.module.weight_fake_quant.activation_post_process.eps', 'distilbert.transformer.layer.4.attention.k_lin.module.weight_fake_quant.scale', 'distilbert.transformer.layer.5.attention.q_lin.module.weight_fake_quant.activation_post_process.eps', 'distilbert.transformer.layer.2.ffn.lin1.module.bias', 'distilbert.transformer.layer.5.attention.attention_scores_matmul.output_quant_stubs.0.activation_post_process.observer_enabled', 'distilbert.transformer.layer.1.attention.q_lin.quant.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.5.attention.out_lin.module.weight_fake_quant.activation_post_process.min_val', 'distilbert.transformer.layer.4.attention.out_lin.quant.activation_post_process.scale', 'distilbert.transformer.layer.4.attention.k_lin.module.weight_fake_quant.fake_quant_enabled', 'distilbert.transformer.layer.5.ffn.lin2.module.weight_fake_quant.fake_quant_enabled', 'distilbert.transformer.layer.4.attention.context_layer_matmul.input_quant_stubs.1.activation_post_process.scale', 'distilbert.transformer.layer.2.ffn.lin2.quant.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.3.attention.q_lin.module.weight_fake_quant.zero_point', 'distilbert.transformer.layer.2.attention.v_lin.quant.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.2.attention.k_lin.quant.activation_post_process.observer_enabled', 'distilbert.transformer.layer.0.ffn.lin1.quant.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.3.attention.context_layer_matmul.input_quant_stubs.0.activation_post_process.zero_point', 'distilbert.transformer.layer.0.attention.v_lin.module.weight_fake_quant.scale', 'distilbert.transformer.layer.4.attention.q_lin.module.weight_fake_quant.activation_post_process.max_val', 'distilbert.transformer.layer.1.attention.attention_scores_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.1.ffn.lin1.module.weight_fake_quant.observer_enabled', 'distilbert.transformer.layer.3.attention.k_lin.module.weight', 'distilbert.transformer.layer.4.ffn.lin2.quant.activation_post_process.observer_enabled', 'classifier.quant.activation_post_process.observer_enabled', 'distilbert.transformer.layer.2.attention.attention_scores_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.5.ffn.lin1.module.weight_fake_quant.fake_quant_enabled', 'distilbert.transformer.layer.2.attention.attention_scores_matmul.input_quant_stubs.0.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.3.ffn.lin2.quant.activation_post_process.zero_point', 'distilbert.transformer.layer.3.attention.attention_scores_matmul.output_quant_stubs.0.activation_post_process.zero_point', 'distilbert.transformer.layer.5.attention.q_lin.quant.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.1.attention.attention_scores_matmul.output_quant_stubs.0.activation_post_process.scale', 'distilbert.transformer.layer.5.attention.context_layer_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.1.attention.context_layer_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.2.attention.attention_scores_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.0.attention.q_lin.quant.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.2.ffn.lin1.module.weight', 'distilbert.transformer.layer.5.attention.q_lin.module.weight_fake_quant.fake_quant_enabled', 'distilbert.transformer.layer.5.ffn.lin1.module.weight_fake_quant.activation_post_process.eps', 'distilbert.transformer.layer.2.attention.context_layer_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.4.attention.attention_scores_matmul.input_quant_stubs.0.activation_post_process.observer_enabled', 'distilbert.transformer.layer.1.attention.v_lin.quant.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.4.ffn.lin1.quant.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.2.attention.k_lin.module.weight_fake_quant.zero_point', 'distilbert.transformer.layer.1.attention.v_lin.module.weight_fake_quant.activation_post_process.max_val', 'distilbert.transformer.layer.4.attention.v_lin.module.weight_fake_quant.activation_post_process.min_val', 'distilbert.transformer.layer.5.attention.q_lin.quant.activation_post_process.scale', 'distilbert.transformer.layer.2.attention.attention_scores_matmul.output_quant_stubs.0.activation_post_process.observer_enabled', 'distilbert.transformer.layer.5.ffn.lin1.quant.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.4.attention.attention_scores_matmul.input_quant_stubs.1.activation_post_process.observer_enabled', 'distilbert.transformer.layer.4.attention.context_layer_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.4.ffn.lin2.quant.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.3.ffn.lin1.module.weight_fake_quant.activation_post_process.max_val', 'distilbert.transformer.layer.5.attention.context_layer_matmul.input_quant_stubs.0.activation_post_process.observer_enabled', 'distilbert.transformer.layer.1.attention.q_lin.module.weight_fake_quant.activation_post_process.eps', 'distilbert.transformer.layer.4.attention.k_lin.quant.activation_post_process.scale', 'distilbert.transformer.layer.2.attention.out_lin.module.weight_fake_quant.activation_post_process.max_val', 'distilbert.transformer.layer.0.attention.q_lin.quant.activation_post_process.zero_point', 'distilbert.transformer.layer.1.attention.context_layer_matmul.input_quant_stubs.0.activation_post_process.scale', 'distilbert.transformer.layer.3.attention.k_lin.module.weight_fake_quant.activation_post_process.min_val', 'distilbert.transformer.layer.2.attention.attention_scores_matmul.output_quant_stubs.0.activation_post_process.scale', 'distilbert.transformer.layer.5.ffn.lin2.quant.activation_post_process.scale', 'distilbert.transformer.layer.5.attention.context_layer_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.5.attention.out_lin.module.weight_fake_quant.scale', 'distilbert.transformer.layer.5.ffn.lin2.quant.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.5.attention.out_lin.quant.activation_post_process.scale', 'pre_classifier.quant.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.2.attention.v_lin.module.weight_fake_quant.activation_post_process.max_val', 'distilbert.transformer.layer.4.attention.context_layer_matmul.input_quant_stubs.0.activation_post_process.observer_enabled', 'distilbert.transformer.layer.1.attention.out_lin.module.bias', 'distilbert.transformer.layer.2.ffn.lin1.module.weight_fake_quant.activation_post_process.max_val', 'distilbert.transformer.layer.4.attention.context_layer_matmul.input_quant_stubs.1.activation_post_process.observer_enabled', 'distilbert.transformer.layer.2.attention.k_lin.quant.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.0.attention.out_lin.quant.activation_post_process.scale', 'distilbert.transformer.layer.4.attention.context_layer_matmul.input_quant_stubs.0.activation_post_process.scale', 'distilbert.transformer.layer.4.ffn.lin1.module.weight_fake_quant.activation_post_process.min_val', 'distilbert.transformer.layer.5.attention.q_lin.module.weight', 'distilbert.transformer.layer.5.attention.k_lin.module.weight_fake_quant.activation_post_process.max_val', 'distilbert.transformer.layer.0.ffn.lin1.module.weight_fake_quant.zero_point', 'distilbert.transformer.layer.0.attention.v_lin.module.weight', 'distilbert.transformer.layer.3.attention.out_lin.quant.activation_post_process.observer_enabled', 'distilbert.transformer.layer.1.attention.out_lin.quant.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.2.attention.context_layer_matmul.input_quant_stubs.0.activation_post_process.zero_point', 'distilbert.transformer.layer.3.attention.k_lin.module.weight_fake_quant.observer_enabled', 'distilbert.transformer.layer.2.attention.context_layer_matmul.input_quant_stubs.0.activation_post_process.observer_enabled']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at /opt/app-root/src/aicoe-osc-demo/models/distilbert_mnli_pruned80/training and are newly initialized: ['distilbert.transformer.layer.2.attention.k_lin.bias', 'distilbert.transformer.layer.0.attention.q_lin.bias', 'distilbert.transformer.layer.0.ffn.lin2.bias', 'distilbert.transformer.layer.0.attention.v_lin.bias', 'distilbert.transformer.layer.1.attention.q_lin.bias', 'distilbert.transformer.layer.2.attention.v_lin.bias', 'distilbert.transformer.layer.2.attention.q_lin.bias', 'distilbert.transformer.layer.1.ffn.lin1.weight', 'distilbert.transformer.layer.4.attention.out_lin.bias', 'distilbert.transformer.layer.1.ffn.lin2.weight', 'distilbert.transformer.layer.1.ffn.lin1.bias', 'distilbert.transformer.layer.4.ffn.lin1.bias', 'distilbert.transformer.layer.2.attention.out_lin.weight', 'distilbert.transformer.layer.3.attention.q_lin.weight', 'distilbert.transformer.layer.0.attention.out_lin.bias', 'distilbert.transformer.layer.3.attention.out_lin.bias', 'pre_classifier.bias', 'distilbert.transformer.layer.4.ffn.lin1.weight', 'distilbert.transformer.layer.0.attention.q_lin.weight', 'distilbert.transformer.layer.5.ffn.lin1.weight', 'distilbert.transformer.layer.0.attention.k_lin.bias', 'distilbert.transformer.layer.3.ffn.lin1.bias', 'distilbert.transformer.layer.4.ffn.lin2.bias', 'distilbert.transformer.layer.3.ffn.lin1.weight', 'pre_classifier.weight', 'distilbert.transformer.layer.2.ffn.lin2.bias', 'distilbert.transformer.layer.5.attention.out_lin.weight', 'distilbert.transformer.layer.2.attention.out_lin.bias', 'distilbert.transformer.layer.5.attention.v_lin.weight', 'distilbert.transformer.layer.1.attention.k_lin.weight', 'distilbert.transformer.layer.5.attention.k_lin.weight', 'distilbert.transformer.layer.2.attention.q_lin.weight', 'distilbert.transformer.layer.1.attention.k_lin.bias', 'distilbert.transformer.layer.3.attention.k_lin.bias', 'distilbert.transformer.layer.3.attention.v_lin.bias', 'distilbert.transformer.layer.1.attention.v_lin.bias', 'distilbert.transformer.layer.3.attention.q_lin.bias', 'distilbert.transformer.layer.1.attention.out_lin.bias', 'distilbert.transformer.layer.4.attention.v_lin.bias', 'distilbert.transformer.layer.4.attention.q_lin.weight', 'distilbert.transformer.layer.4.attention.k_lin.weight', 'distilbert.transformer.layer.4.attention.v_lin.weight', 'distilbert.transformer.layer.5.ffn.lin1.bias', 'distilbert.transformer.layer.4.attention.q_lin.bias', 'distilbert.transformer.layer.5.attention.v_lin.bias', 'distilbert.transformer.layer.3.attention.k_lin.weight', 'distilbert.transformer.layer.2.attention.v_lin.weight', 'distilbert.transformer.layer.3.ffn.lin2.weight', 'distilbert.transformer.layer.3.attention.v_lin.weight', 'distilbert.transformer.layer.5.attention.q_lin.bias', 'classifier.weight', 'distilbert.transformer.layer.5.attention.q_lin.weight', 'distilbert.transformer.layer.4.attention.out_lin.weight', 'distilbert.transformer.layer.3.attention.out_lin.weight', 'distilbert.transformer.layer.3.ffn.lin2.bias', 'distilbert.transformer.layer.5.attention.k_lin.bias', 'distilbert.transformer.layer.0.ffn.lin2.weight', 'distilbert.transformer.layer.2.ffn.lin2.weight', 'distilbert.transformer.layer.5.attention.out_lin.bias', 'distilbert.transformer.layer.1.attention.v_lin.weight', 'classifier.bias', 'distilbert.transformer.layer.1.attention.q_lin.weight', 'distilbert.transformer.layer.0.attention.v_lin.weight', 'distilbert.transformer.layer.1.attention.out_lin.weight', 'distilbert.transformer.layer.0.attention.k_lin.weight', 'distilbert.transformer.layer.0.attention.out_lin.weight', 'distilbert.transformer.layer.2.ffn.lin1.bias', 'distilbert.transformer.layer.0.ffn.lin1.bias', 'distilbert.transformer.layer.1.ffn.lin2.bias', 'distilbert.transformer.layer.0.ffn.lin1.weight', 'distilbert.transformer.layer.2.attention.k_lin.weight', 'distilbert.transformer.layer.5.ffn.lin2.weight', 'distilbert.transformer.layer.2.ffn.lin1.weight', 'distilbert.transformer.layer.5.ffn.lin2.bias', 'distilbert.transformer.layer.4.ffn.lin2.weight', 'distilbert.transformer.layer.4.attention.k_lin.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c71223c26da45a38a7f4a9924d90025",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a793a928e30446bb642bb18d33c08bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: question, token_type_ids, sentence, __index_level_0__. If question, token_type_ids, sentence, __index_level_0__ are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/opt/app-root/lib64/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2033\n",
      "  Num Epochs = 16\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2048\n",
      "  Number of trainable parameters = 66955010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2048' max='2048' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2048/2048 03:57, Epoch 16/16]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.398000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.186000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.093000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.069800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: question, token_type_ids, sentence, __index_level_0__. If question, token_type_ids, sentence, __index_level_0__ are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 509\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: question, token_type_ids, sentence, __index_level_0__. If question, token_type_ids, sentence, __index_level_0__ are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 509\n",
      "  Batch size = 16\n",
      "/opt/app-root/lib64/python3.8/site-packages/sklearn/metrics/_classification.py:1580: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/opt/app-root/lib64/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/app-root/lib64/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to /opt/app-root/src/aicoe-osc-demo/models/transformers/distilbert_mnli_pruned80\n",
      "Configuration saved in /opt/app-root/src/aicoe-osc-demo/models/transformers/distilbert_mnli_pruned80/config.json\n",
      "Model weights saved in /opt/app-root/src/aicoe-osc-demo/models/transformers/distilbert_mnli_pruned80/pytorch_model.bin\n",
      "tokenizer config file saved in /opt/app-root/src/aicoe-osc-demo/models/transformers/distilbert_mnli_pruned80/tokenizer_config.json\n",
      "Special tokens file saved in /opt/app-root/src/aicoe-osc-demo/models/transformers/distilbert_mnli_pruned80/special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbbdb3339c1a4090b3f7df2a6256416d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "downloading...:   0%|          | 0.00/695k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf6b03e6ad1c431980830b8030e71a15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "downloading...:   0%|          | 0.00/314 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92e301c5d18b4f1e828159a5e6f6f0a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "downloading...:   0%|          | 0.00/826 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c627319280914579b3d620f080a4da67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "downloading...:   0%|          | 0.00/200 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdb5f500962e4936a234d38de227b753",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "downloading...:   0%|          | 0.00/346 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec859b02f85a41cbac42fa8b77ed8e23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "downloading...:   0%|          | 0.00/256M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6511adcbd66045698a84f4ceedf83e95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "downloading...:   0%|          | 0.00/493 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc78b17ff7f74eb7af81c23ced7cb8de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "downloading...:   0%|          | 0.00/3.68k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8280f025617a4b198d54a2da1cc18067",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "downloading...:   0%|          | 0.00/13.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "414a9600a7604a43bc7b59b0a28768c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "downloading...:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4eb35abcfe14e72969e8f3944de9cf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "downloading...:   0%|          | 0.00/3.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7b8637ec27c45f3b98f8ec1271c0ce6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "downloading...:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d6b0d1f37024d1085f0fdbc99056276",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "downloading...:   0%|          | 0.00/64.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39891f596919480faa994c7aae7a7751",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "downloading...:   0%|          | 0.00/695k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56e474f1811b4fb592f9980e39186ece",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "downloading...:   0%|          | 0.00/826 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22753f5a1d5e42559f62bc1a0fd749b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "downloading...:   0%|          | 0.00/346 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "071031c2197f4123a5505242e6b72d70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "downloading...:   0%|          | 0.00/3.12k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a330187c547490fbc26ac72874768d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "downloading...:   0%|          | 0.00/1.25k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0046fc90ec2435790fc3c62377150e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "downloading...:   0%|          | 0.00/1.05k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91022cf7c92a4e609f1c0808485ae2f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "downloading...:   0%|          | 0.00/4.61k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edece88853ab41ea97fb199697cf6a40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "downloading...:   0%|          | 0.00/64.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading configuration file /opt/app-root/src/aicoe-osc-demo/models/distilbert_qqp_pruned80/training/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"/opt/app-root/src/aicoe-osc-demo/models/distilbert_qqp_pruned80/training\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"finetuning_task\": \"qqp\",\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"not_duplicate\",\n",
      "    \"1\": \"duplicate\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"duplicate\": 1,\n",
      "    \"not_duplicate\": 0\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file /opt/app-root/src/aicoe-osc-demo/models/distilbert_qqp_pruned80/training/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /opt/app-root/src/aicoe-osc-demo/models/distilbert_qqp_pruned80/training were not used when initializing DistilBertForSequenceClassification: ['distilbert.transformer.layer.1.attention.k_lin.quant.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.1.attention.out_lin.quant.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.4.attention.q_lin.quant.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.3.ffn.lin2.module.weight_fake_quant.zero_point', 'distilbert.transformer.layer.3.attention.v_lin.quant.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.3.attention.attention_scores_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.3.ffn.lin2.module.weight_fake_quant.fake_quant_enabled', 'distilbert.transformer.layer.2.attention.out_lin.quant.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.0.attention.k_lin.module.weight', 'distilbert.transformer.layer.5.attention.out_lin.module.weight_fake_quant.zero_point', 'distilbert.transformer.layer.0.ffn.lin1.module.weight_fake_quant.activation_post_process.eps', 'distilbert.transformer.layer.2.attention.context_layer_matmul.input_quant_stubs.1.activation_post_process.zero_point', 'distilbert.transformer.layer.4.attention.context_layer_matmul.input_quant_stubs.1.activation_post_process.zero_point', 'distilbert.transformer.layer.4.ffn.lin2.quant.activation_post_process.scale', 'distilbert.transformer.layer.1.attention.v_lin.quant.activation_post_process.scale', 'distilbert.transformer.layer.4.attention.context_layer_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.5.attention.q_lin.quant.activation_post_process.zero_point', 'distilbert.transformer.layer.1.ffn.lin1.quant.activation_post_process.zero_point', 'distilbert.transformer.layer.3.attention.attention_scores_matmul.input_quant_stubs.1.activation_post_process.observer_enabled', 'distilbert.transformer.layer.2.attention.context_layer_matmul.input_quant_stubs.1.activation_post_process.observer_enabled', 'distilbert.transformer.layer.2.attention.attention_scores_matmul.input_quant_stubs.1.activation_post_process.zero_point', 'distilbert.transformer.layer.2.attention.q_lin.quant.activation_post_process.scale', 'distilbert.transformer.layer.4.attention.k_lin.quant.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.5.ffn.lin1.quant.activation_post_process.zero_point', 'distilbert.transformer.layer.3.ffn.lin2.quant.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.5.ffn.lin2.module.weight_fake_quant.scale', 'distilbert.transformer.layer.3.attention.out_lin.module.weight_fake_quant.activation_post_process.max_val', 'distilbert.transformer.layer.0.ffn.lin2.quant.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.1.attention.attention_scores_matmul.input_quant_stubs.1.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.4.ffn.lin2.module.weight_fake_quant.fake_quant_enabled', 'distilbert.transformer.layer.5.attention.out_lin.module.weight_fake_quant.fake_quant_enabled', 'distilbert.transformer.layer.5.attention.attention_scores_matmul.input_quant_stubs.0.activation_post_process.observer_enabled', 'distilbert.transformer.layer.5.ffn.lin2.quant.activation_post_process.zero_point', 'distilbert.transformer.layer.4.attention.attention_scores_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.1.ffn.lin2.quant.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.0.ffn.lin2.quant.activation_post_process.zero_point', 'distilbert.transformer.layer.1.attention.q_lin.quant.activation_post_process.scale', 'distilbert.transformer.layer.2.ffn.lin2.module.weight_fake_quant.activation_post_process.min_val', 'distilbert.transformer.layer.4.ffn.lin2.quant.activation_post_process.fake_quant_enabled', 'distilbert.embeddings.position_embeddings.weight_fake_quant.scale', 'distilbert.transformer.layer.2.attention.v_lin.module.weight_fake_quant.scale', 'distilbert.transformer.layer.3.attention.v_lin.module.bias', 'distilbert.transformer.layer.2.attention.v_lin.quant.activation_post_process.observer_enabled', 'distilbert.transformer.layer.1.ffn.lin2.quant.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.5.attention.q_lin.quant.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.5.attention.v_lin.module.weight_fake_quant.observer_enabled', 'classifier.module.weight', 'distilbert.transformer.layer.2.attention.v_lin.quant.activation_post_process.scale', 'distilbert.transformer.layer.4.attention.out_lin.module.weight_fake_quant.activation_post_process.eps', 'distilbert.transformer.layer.4.ffn.lin1.module.weight', 'distilbert.transformer.layer.0.attention.attention_scores_matmul.input_quant_stubs.0.activation_post_process.zero_point', 'distilbert.transformer.layer.1.attention.out_lin.quant.activation_post_process.observer_enabled', 'distilbert.transformer.layer.2.ffn.lin1.module.weight_fake_quant.zero_point', 'distilbert.transformer.layer.4.attention.q_lin.module.weight_fake_quant.fake_quant_enabled', 'pre_classifier.module.weight_fake_quant.zero_point', 'distilbert.transformer.layer.4.attention.q_lin.module.bias', 'distilbert.transformer.layer.3.attention.out_lin.module.weight_fake_quant.activation_post_process.eps', 'distilbert.transformer.layer.1.attention.k_lin.module.weight_fake_quant.scale', 'distilbert.transformer.layer.2.attention.q_lin.module.bias', 'distilbert.transformer.layer.2.attention.v_lin.module.bias', 'distilbert.transformer.layer.2.ffn.lin1.quant.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.2.attention.k_lin.module.weight', 'distilbert.transformer.layer.2.ffn.lin2.module.weight_fake_quant.activation_post_process.max_val', 'pre_classifier.module.weight_fake_quant.activation_post_process.min_val', 'distilbert.transformer.layer.1.ffn.lin2.quant.activation_post_process.zero_point', 'distilbert.transformer.layer.5.attention.k_lin.quant.activation_post_process.scale', 'pre_classifier.module.weight_fake_quant.activation_post_process.max_val', 'distilbert.transformer.layer.3.ffn.lin1.module.weight_fake_quant.observer_enabled', 'distilbert.transformer.layer.5.attention.q_lin.quant.activation_post_process.observer_enabled', 'distilbert.transformer.layer.5.ffn.lin2.quant.activation_post_process.observer_enabled', 'distilbert.embeddings.word_embeddings.weight_fake_quant.observer_enabled', 'distilbert.transformer.layer.0.attention.attention_scores_matmul.input_quant_stubs.1.activation_post_process.observer_enabled', 'distilbert.transformer.layer.1.ffn.lin2.quant.activation_post_process.scale', 'distilbert.transformer.layer.4.ffn.lin1.module.weight_fake_quant.activation_post_process.eps', 'distilbert.transformer.layer.4.attention.v_lin.module.weight_fake_quant.fake_quant_enabled', 'distilbert.transformer.layer.0.attention.v_lin.quant.activation_post_process.observer_enabled', 'distilbert.transformer.layer.2.attention.out_lin.quant.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.1.ffn.lin2.quant.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.4.attention.context_layer_matmul.input_quant_stubs.0.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.1.attention.v_lin.quant.activation_post_process.zero_point', 'distilbert.transformer.layer.1.attention.k_lin.quant.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.2.ffn.lin1.quant.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.4.attention.attention_scores_matmul.input_quant_stubs.0.activation_post_process.zero_point', 'distilbert.transformer.layer.4.ffn.lin1.quant.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.5.attention.attention_scores_matmul.input_quant_stubs.1.activation_post_process.scale', 'distilbert.transformer.layer.3.attention.q_lin.quant.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.5.attention.attention_scores_matmul.input_quant_stubs.1.activation_post_process.zero_point', 'distilbert.transformer.layer.2.ffn.lin2.quant.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.0.ffn.lin2.quant.activation_post_process.scale', 'distilbert.transformer.layer.1.attention.out_lin.module.weight_fake_quant.activation_post_process.min_val', 'distilbert.transformer.layer.2.attention.context_layer_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'pre_classifier.module.bias', 'distilbert.transformer.layer.0.ffn.lin1.quant.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.3.ffn.lin2.quant.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.3.attention.k_lin.module.weight_fake_quant.zero_point', 'distilbert.embeddings.word_embeddings.weight_fake_quant.zero_point', 'distilbert.transformer.layer.0.attention.k_lin.quant.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.2.attention.out_lin.quant.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.0.attention.q_lin.quant.activation_post_process.scale', 'distilbert.transformer.layer.0.ffn.lin1.quant.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.5.attention.attention_scores_matmul.input_quant_stubs.0.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.3.ffn.lin1.module.weight_fake_quant.scale', 'distilbert.transformer.layer.5.ffn.lin2.module.weight_fake_quant.activation_post_process.eps', 'distilbert.transformer.layer.3.attention.k_lin.quant.activation_post_process.scale', 'distilbert.transformer.layer.4.attention.attention_scores_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.2.attention.q_lin.quant.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.4.attention.q_lin.module.weight_fake_quant.scale', 'distilbert.transformer.layer.3.attention.out_lin.quant.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.0.attention.out_lin.module.weight_fake_quant.observer_enabled', 'distilbert.transformer.layer.1.attention.k_lin.module.weight_fake_quant.zero_point', 'distilbert.transformer.layer.0.attention.k_lin.module.weight_fake_quant.observer_enabled', 'distilbert.transformer.layer.4.ffn.lin1.quant.activation_post_process.zero_point', 'distilbert.transformer.layer.2.attention.k_lin.quant.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.2.attention.q_lin.module.weight_fake_quant.observer_enabled', 'distilbert.transformer.layer.3.ffn.lin2.module.weight_fake_quant.observer_enabled', 'distilbert.transformer.layer.4.attention.k_lin.module.weight', 'distilbert.transformer.layer.2.attention.k_lin.module.weight_fake_quant.activation_post_process.eps', 'distilbert.transformer.layer.4.attention.attention_scores_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.1.attention.q_lin.module.weight', 'distilbert.transformer.layer.1.attention.attention_scores_matmul.output_quant_stubs.0.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.1.attention.v_lin.module.weight', 'distilbert.transformer.layer.2.attention.attention_scores_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.5.ffn.lin2.module.weight', 'distilbert.transformer.layer.0.attention.attention_scores_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.5.attention.attention_scores_matmul.input_quant_stubs.1.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.2.attention.context_layer_matmul.input_quant_stubs.0.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.5.ffn.lin1.module.weight_fake_quant.zero_point', 'distilbert.transformer.layer.3.attention.context_layer_matmul.input_quant_stubs.0.activation_post_process.observer_enabled', 'distilbert.transformer.layer.5.ffn.lin2.module.weight_fake_quant.observer_enabled', 'distilbert.transformer.layer.0.ffn.lin2.module.weight_fake_quant.scale', 'distilbert.transformer.layer.1.attention.out_lin.quant.activation_post_process.zero_point', 'distilbert.transformer.layer.3.ffn.lin2.quant.activation_post_process.scale', 'distilbert.transformer.layer.5.attention.q_lin.module.weight_fake_quant.activation_post_process.min_val', 'distilbert.transformer.layer.1.ffn.lin1.quant.activation_post_process.scale', 'distilbert.transformer.layer.3.ffn.lin2.module.weight_fake_quant.scale', 'distilbert.transformer.layer.3.attention.v_lin.module.weight_fake_quant.activation_post_process.max_val', 'distilbert.transformer.layer.3.attention.attention_scores_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.1.ffn.lin2.module.weight_fake_quant.zero_point', 'distilbert.transformer.layer.3.ffn.lin1.module.weight_fake_quant.fake_quant_enabled', 'distilbert.transformer.layer.5.attention.out_lin.quant.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.3.attention.out_lin.module.weight_fake_quant.fake_quant_enabled', 'pre_classifier.module.weight_fake_quant.activation_post_process.eps', 'distilbert.transformer.layer.5.attention.q_lin.module.weight_fake_quant.scale', 'distilbert.transformer.layer.4.attention.k_lin.quant.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.2.attention.v_lin.module.weight_fake_quant.activation_post_process.min_val', 'distilbert.transformer.layer.3.attention.out_lin.module.bias', 'distilbert.transformer.layer.2.attention.out_lin.module.weight_fake_quant.activation_post_process.eps', 'distilbert.transformer.layer.2.attention.q_lin.quant.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.1.attention.v_lin.quant.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.1.attention.attention_scores_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.4.attention.k_lin.module.weight_fake_quant.zero_point', 'distilbert.transformer.layer.1.attention.attention_scores_matmul.input_quant_stubs.1.activation_post_process.scale', 'distilbert.transformer.layer.0.attention.out_lin.quant.activation_post_process.observer_enabled', 'distilbert.transformer.layer.0.attention.attention_scores_matmul.input_quant_stubs.0.activation_post_process.observer_enabled', 'distilbert.transformer.layer.0.attention.attention_scores_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.0.attention.q_lin.module.weight_fake_quant.activation_post_process.eps', 'pre_classifier.quant.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.0.attention.context_layer_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.2.ffn.lin1.module.weight_fake_quant.observer_enabled', 'distilbert.transformer.layer.4.attention.k_lin.quant.activation_post_process.zero_point', 'distilbert.transformer.layer.4.attention.q_lin.quant.activation_post_process.zero_point', 'distilbert.transformer.layer.2.attention.q_lin.module.weight_fake_quant.zero_point', 'distilbert.transformer.layer.1.ffn.lin2.module.weight_fake_quant.fake_quant_enabled', 'distilbert.transformer.layer.2.attention.q_lin.module.weight_fake_quant.activation_post_process.min_val', 'distilbert.transformer.layer.4.attention.q_lin.module.weight', 'distilbert.transformer.layer.1.attention.v_lin.module.weight_fake_quant.activation_post_process.min_val', 'classifier.quant.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.0.attention.context_layer_matmul.input_quant_stubs.0.activation_post_process.scale', 'distilbert.transformer.layer.4.attention.out_lin.module.weight_fake_quant.zero_point', 'distilbert.transformer.layer.1.attention.context_layer_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.1.attention.attention_scores_matmul.input_quant_stubs.0.activation_post_process.zero_point', 'distilbert.transformer.layer.1.attention.context_layer_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.0.ffn.lin1.module.weight_fake_quant.activation_post_process.max_val', 'distilbert.transformer.layer.3.attention.out_lin.quant.activation_post_process.zero_point', 'distilbert.transformer.layer.3.attention.k_lin.module.bias', 'distilbert.transformer.layer.4.attention.q_lin.quant.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.2.attention.attention_scores_matmul.output_quant_stubs.0.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.0.attention.context_layer_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.3.attention.context_layer_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.0.ffn.lin1.module.weight', 'distilbert.transformer.layer.3.attention.q_lin.quant.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.4.attention.v_lin.module.weight', 'distilbert.transformer.layer.2.attention.out_lin.module.weight_fake_quant.zero_point', 'distilbert.transformer.layer.4.attention.v_lin.module.weight_fake_quant.zero_point', 'distilbert.transformer.layer.0.attention.q_lin.quant.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.1.attention.q_lin.module.weight_fake_quant.observer_enabled', 'distilbert.transformer.layer.1.ffn.lin1.module.weight_fake_quant.scale', 'classifier.quant.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.3.ffn.lin1.module.weight_fake_quant.activation_post_process.eps', 'distilbert.transformer.layer.2.attention.k_lin.quant.activation_post_process.scale', 'distilbert.transformer.layer.0.attention.v_lin.quant.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.1.attention.attention_scores_matmul.output_quant_stubs.0.activation_post_process.zero_point', 'distilbert.transformer.layer.4.ffn.lin2.module.weight_fake_quant.activation_post_process.max_val', 'classifier.module.weight_fake_quant.activation_post_process.max_val', 'distilbert.transformer.layer.5.attention.v_lin.quant.activation_post_process.observer_enabled', 'distilbert.transformer.layer.5.attention.out_lin.module.bias', 'distilbert.transformer.layer.0.attention.v_lin.module.weight_fake_quant.fake_quant_enabled', 'distilbert.transformer.layer.0.attention.k_lin.module.weight_fake_quant.activation_post_process.eps', 'distilbert.transformer.layer.1.attention.out_lin.quant.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.0.ffn.lin1.module.weight_fake_quant.scale', 'distilbert.transformer.layer.5.attention.q_lin.quant.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.1.attention.attention_scores_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.1.attention.k_lin.quant.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.5.attention.v_lin.module.weight', 'distilbert.transformer.layer.3.attention.k_lin.quant.activation_post_process.observer_enabled', 'distilbert.transformer.layer.3.attention.attention_scores_matmul.output_quant_stubs.0.activation_post_process.scale', 'distilbert.transformer.layer.4.attention.context_layer_matmul.input_quant_stubs.1.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.3.attention.out_lin.quant.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.0.attention.attention_scores_matmul.output_quant_stubs.0.activation_post_process.zero_point', 'distilbert.transformer.layer.5.attention.q_lin.module.bias', 'distilbert.transformer.layer.1.attention.v_lin.module.weight_fake_quant.fake_quant_enabled', 'distilbert.transformer.layer.3.ffn.lin1.quant.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.4.attention.out_lin.module.bias', 'distilbert.transformer.layer.2.ffn.lin1.quant.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.3.attention.out_lin.module.weight_fake_quant.observer_enabled', 'distilbert.transformer.layer.5.attention.context_layer_matmul.input_quant_stubs.0.activation_post_process.zero_point', 'distilbert.transformer.layer.5.attention.out_lin.quant.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.3.attention.v_lin.module.weight_fake_quant.scale', 'distilbert.transformer.layer.2.attention.attention_scores_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.2.attention.attention_scores_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.3.attention.k_lin.quant.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.5.attention.q_lin.quant.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.0.attention.k_lin.quant.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.1.ffn.lin1.module.bias', 'distilbert.embeddings.word_embeddings.weight_fake_quant.activation_post_process.max_val', 'distilbert.transformer.layer.0.attention.attention_scores_matmul.output_quant_stubs.0.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.1.attention.q_lin.module.bias', 'distilbert.transformer.layer.2.attention.attention_scores_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.4.ffn.lin1.quant.activation_post_process.observer_enabled', 'distilbert.transformer.layer.2.attention.k_lin.module.bias', 'distilbert.transformer.layer.4.attention.out_lin.quant.activation_post_process.zero_point', 'distilbert.transformer.layer.0.attention.q_lin.module.weight_fake_quant.fake_quant_enabled', 'distilbert.transformer.layer.1.attention.q_lin.quant.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.3.attention.context_layer_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.0.attention.out_lin.module.bias', 'distilbert.transformer.layer.2.ffn.lin1.quant.activation_post_process.zero_point', 'distilbert.transformer.layer.4.attention.v_lin.module.weight_fake_quant.observer_enabled', 'distilbert.transformer.layer.4.attention.out_lin.module.weight_fake_quant.activation_post_process.max_val', 'distilbert.embeddings.word_embeddings.weight_fake_quant.fake_quant_enabled', 'distilbert.transformer.layer.2.attention.out_lin.quant.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.4.attention.attention_scores_matmul.input_quant_stubs.0.activation_post_process.scale', 'distilbert.transformer.layer.3.ffn.lin1.quant.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.4.attention.k_lin.module.weight_fake_quant.observer_enabled', 'distilbert.transformer.layer.4.attention.attention_scores_matmul.output_quant_stubs.0.activation_post_process.scale', 'distilbert.transformer.layer.5.attention.attention_scores_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.5.ffn.lin1.module.weight_fake_quant.observer_enabled', 'distilbert.transformer.layer.0.ffn.lin2.quant.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.3.attention.v_lin.module.weight_fake_quant.activation_post_process.min_val', 'distilbert.transformer.layer.5.attention.attention_scores_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.0.attention.q_lin.module.weight_fake_quant.scale', 'distilbert.transformer.layer.4.attention.context_layer_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.5.attention.attention_scores_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.3.ffn.lin1.quant.activation_post_process.observer_enabled', 'distilbert.transformer.layer.3.attention.k_lin.quant.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.1.ffn.lin1.quant.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.2.ffn.lin1.quant.activation_post_process.observer_enabled', 'distilbert.transformer.layer.5.ffn.lin1.quant.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.0.attention.attention_scores_matmul.input_quant_stubs.1.activation_post_process.zero_point', 'distilbert.embeddings.word_embeddings.weight_fake_quant.scale', 'distilbert.transformer.layer.5.attention.out_lin.module.weight', 'distilbert.transformer.layer.0.attention.context_layer_matmul.input_quant_stubs.1.activation_post_process.observer_enabled', 'distilbert.transformer.layer.5.ffn.lin2.module.bias', 'distilbert.transformer.layer.1.attention.attention_scores_matmul.output_quant_stubs.0.activation_post_process.observer_enabled', 'distilbert.transformer.layer.4.attention.context_layer_matmul.input_quant_stubs.0.activation_post_process.zero_point', 'distilbert.transformer.layer.3.attention.v_lin.module.weight', 'distilbert.transformer.layer.3.attention.k_lin.quant.activation_post_process.zero_point', 'pre_classifier.module.weight', 'distilbert.transformer.layer.3.attention.attention_scores_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.0.ffn.lin1.module.bias', 'distilbert.transformer.layer.2.attention.v_lin.module.weight_fake_quant.fake_quant_enabled', 'distilbert.transformer.layer.0.attention.out_lin.quant.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.0.attention.attention_scores_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.4.attention.attention_scores_matmul.output_quant_stubs.0.activation_post_process.observer_enabled', 'distilbert.transformer.layer.0.attention.out_lin.module.weight_fake_quant.fake_quant_enabled', 'distilbert.transformer.layer.5.attention.context_layer_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.5.attention.k_lin.quant.activation_post_process.observer_enabled', 'distilbert.transformer.layer.1.attention.k_lin.quant.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.4.attention.q_lin.quant.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.0.ffn.lin1.quant.activation_post_process.zero_point', 'distilbert.transformer.layer.3.attention.attention_scores_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.0.attention.q_lin.module.weight_fake_quant.activation_post_process.min_val', 'distilbert.transformer.layer.1.ffn.lin2.quant.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.4.ffn.lin1.quant.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.3.attention.attention_scores_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.4.attention.k_lin.module.bias', 'distilbert.transformer.layer.4.attention.q_lin.quant.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.0.attention.q_lin.module.weight_fake_quant.zero_point', 'distilbert.transformer.layer.4.attention.v_lin.module.weight_fake_quant.activation_post_process.max_val', 'distilbert.transformer.layer.1.attention.attention_scores_matmul.input_quant_stubs.0.activation_post_process.observer_enabled', 'distilbert.transformer.layer.4.ffn.lin1.module.weight_fake_quant.zero_point', 'distilbert.transformer.layer.2.ffn.lin2.quant.activation_post_process.scale', 'distilbert.transformer.layer.3.attention.q_lin.quant.activation_post_process.scale', 'distilbert.transformer.layer.0.attention.attention_scores_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.5.attention.k_lin.module.weight_fake_quant.activation_post_process.min_val', 'classifier.module.weight_fake_quant.scale', 'distilbert.transformer.layer.2.attention.k_lin.module.weight_fake_quant.activation_post_process.max_val', 'distilbert.transformer.layer.1.attention.k_lin.module.weight_fake_quant.fake_quant_enabled', 'distilbert.transformer.layer.2.ffn.lin1.module.weight_fake_quant.fake_quant_enabled', 'distilbert.transformer.layer.2.ffn.lin1.module.weight_fake_quant.activation_post_process.eps', 'distilbert.transformer.layer.3.attention.context_layer_matmul.input_quant_stubs.0.activation_post_process.scale', 'distilbert.transformer.layer.5.attention.context_layer_matmul.input_quant_stubs.1.activation_post_process.scale', 'distilbert.transformer.layer.0.ffn.lin2.module.weight_fake_quant.activation_post_process.max_val', 'distilbert.transformer.layer.4.attention.q_lin.quant.activation_post_process.scale', 'distilbert.transformer.layer.4.attention.out_lin.quant.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.0.attention.q_lin.quant.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.2.attention.q_lin.module.weight_fake_quant.activation_post_process.eps', 'distilbert.transformer.layer.3.attention.q_lin.quant.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.5.attention.context_layer_matmul.input_quant_stubs.1.activation_post_process.observer_enabled', 'distilbert.transformer.layer.0.ffn.lin2.module.bias', 'distilbert.transformer.layer.3.attention.context_layer_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.5.attention.context_layer_matmul.input_quant_stubs.0.activation_post_process.scale', 'distilbert.transformer.layer.1.attention.out_lin.quant.activation_post_process.scale', 'distilbert.transformer.layer.1.attention.context_layer_matmul.input_quant_stubs.0.activation_post_process.observer_enabled', 'distilbert.transformer.layer.4.attention.out_lin.module.weight', 'distilbert.transformer.layer.2.ffn.lin2.module.weight_fake_quant.scale', 'distilbert.transformer.layer.0.attention.context_layer_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.3.attention.k_lin.module.weight_fake_quant.fake_quant_enabled', 'distilbert.transformer.layer.5.ffn.lin2.quant.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.3.attention.v_lin.quant.activation_post_process.observer_enabled', 'distilbert.transformer.layer.2.attention.q_lin.module.weight_fake_quant.scale', 'distilbert.embeddings.position_embeddings.weight_fake_quant.activation_post_process.eps', 'distilbert.transformer.layer.5.ffn.lin1.module.weight_fake_quant.scale', 'distilbert.transformer.layer.4.attention.k_lin.module.weight_fake_quant.activation_post_process.max_val', 'distilbert.transformer.layer.3.ffn.lin2.module.weight', 'distilbert.transformer.layer.1.attention.attention_scores_matmul.input_quant_stubs.1.activation_post_process.zero_point', 'pre_classifier.quant.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.5.attention.attention_scores_matmul.output_quant_stubs.0.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.2.ffn.lin2.quant.activation_post_process.zero_point', 'distilbert.transformer.layer.1.attention.out_lin.module.weight', 'distilbert.transformer.layer.2.ffn.lin2.module.bias', 'distilbert.transformer.layer.5.attention.v_lin.module.weight_fake_quant.activation_post_process.max_val', 'distilbert.transformer.layer.5.attention.attention_scores_matmul.input_quant_stubs.0.activation_post_process.zero_point', 'distilbert.transformer.layer.3.attention.k_lin.module.weight_fake_quant.activation_post_process.max_val', 'distilbert.transformer.layer.0.attention.k_lin.module.weight_fake_quant.activation_post_process.min_val', 'distilbert.transformer.layer.5.attention.out_lin.quant.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.0.attention.out_lin.module.weight_fake_quant.activation_post_process.eps', 'distilbert.transformer.layer.5.attention.attention_scores_matmul.input_quant_stubs.0.activation_post_process.scale', 'distilbert.embeddings.position_embeddings.weight_fake_quant.activation_post_process.min_val', 'distilbert.transformer.layer.3.attention.context_layer_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.1.ffn.lin1.quant.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.4.attention.v_lin.quant.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.5.attention.out_lin.quant.activation_post_process.zero_point', 'distilbert.transformer.layer.1.attention.k_lin.module.weight_fake_quant.activation_post_process.min_val', 'classifier.quant.activation_post_process.zero_point', 'distilbert.transformer.layer.1.attention.out_lin.module.weight_fake_quant.zero_point', 'distilbert.transformer.layer.4.attention.v_lin.module.bias', 'distilbert.transformer.layer.2.attention.v_lin.module.weight_fake_quant.observer_enabled', 'distilbert.transformer.layer.5.attention.v_lin.quant.activation_post_process.zero_point', 'distilbert.transformer.layer.3.attention.k_lin.module.weight_fake_quant.scale', 'pre_classifier.module.weight_fake_quant.observer_enabled', 'distilbert.transformer.layer.4.attention.attention_scores_matmul.input_quant_stubs.1.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.5.attention.k_lin.quant.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.4.attention.v_lin.quant.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.4.attention.out_lin.module.weight_fake_quant.scale', 'distilbert.transformer.layer.5.attention.k_lin.module.weight_fake_quant.scale', 'distilbert.transformer.layer.4.attention.attention_scores_matmul.input_quant_stubs.1.activation_post_process.zero_point', 'distilbert.transformer.layer.5.ffn.lin1.module.bias', 'distilbert.transformer.layer.2.attention.v_lin.quant.activation_post_process.zero_point', 'distilbert.transformer.layer.5.attention.k_lin.quant.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.3.attention.attention_scores_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.3.attention.context_layer_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.1.ffn.lin1.module.weight', 'distilbert.transformer.layer.3.attention.out_lin.module.weight_fake_quant.activation_post_process.min_val', 'distilbert.transformer.layer.3.attention.v_lin.module.weight_fake_quant.observer_enabled', 'distilbert.transformer.layer.2.attention.out_lin.module.weight_fake_quant.observer_enabled', 'distilbert.transformer.layer.3.ffn.lin1.quant.activation_post_process.scale', 'distilbert.transformer.layer.0.attention.context_layer_matmul.input_quant_stubs.0.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.4.attention.out_lin.quant.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.2.attention.v_lin.module.weight_fake_quant.activation_post_process.eps', 'distilbert.transformer.layer.0.attention.v_lin.module.bias', 'distilbert.transformer.layer.5.attention.q_lin.module.weight_fake_quant.zero_point', 'distilbert.transformer.layer.1.attention.v_lin.quant.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.0.attention.q_lin.module.weight', 'distilbert.transformer.layer.4.ffn.lin2.quant.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.3.attention.context_layer_matmul.input_quant_stubs.0.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.1.attention.context_layer_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.1.ffn.lin2.module.weight_fake_quant.activation_post_process.min_val', 'distilbert.transformer.layer.2.attention.attention_scores_matmul.input_quant_stubs.0.activation_post_process.scale', 'distilbert.transformer.layer.0.ffn.lin2.quant.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.2.ffn.lin2.module.weight_fake_quant.activation_post_process.eps', 'distilbert.transformer.layer.0.attention.attention_scores_matmul.input_quant_stubs.0.activation_post_process.scale', 'distilbert.transformer.layer.3.attention.v_lin.module.weight_fake_quant.fake_quant_enabled', 'distilbert.transformer.layer.3.attention.v_lin.module.weight_fake_quant.activation_post_process.eps', 'distilbert.transformer.layer.2.ffn.lin1.quant.activation_post_process.scale', 'distilbert.transformer.layer.4.attention.v_lin.quant.activation_post_process.zero_point', 'distilbert.transformer.layer.5.attention.context_layer_matmul.input_quant_stubs.1.activation_post_process.zero_point', 'distilbert.transformer.layer.2.attention.attention_scores_matmul.input_quant_stubs.1.activation_post_process.observer_enabled', 'distilbert.transformer.layer.4.attention.context_layer_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'pre_classifier.quant.activation_post_process.zero_point', 'distilbert.transformer.layer.0.attention.v_lin.module.weight_fake_quant.activation_post_process.max_val', 'distilbert.transformer.layer.0.attention.v_lin.module.weight_fake_quant.activation_post_process.min_val', 'distilbert.transformer.layer.5.attention.out_lin.module.weight_fake_quant.activation_post_process.eps', 'distilbert.transformer.layer.0.attention.context_layer_matmul.input_quant_stubs.1.activation_post_process.zero_point', 'distilbert.transformer.layer.0.attention.v_lin.quant.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.3.attention.attention_scores_matmul.input_quant_stubs.1.activation_post_process.scale', 'distilbert.transformer.layer.0.attention.k_lin.module.weight_fake_quant.fake_quant_enabled', 'distilbert.transformer.layer.0.attention.attention_scores_matmul.input_quant_stubs.1.activation_post_process.scale', 'distilbert.transformer.layer.3.attention.q_lin.module.weight_fake_quant.activation_post_process.min_val', 'distilbert.transformer.layer.2.attention.attention_scores_matmul.input_quant_stubs.1.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.5.attention.q_lin.module.weight_fake_quant.activation_post_process.max_val', 'distilbert.transformer.layer.0.attention.k_lin.quant.activation_post_process.zero_point', 'distilbert.transformer.layer.3.attention.attention_scores_matmul.input_quant_stubs.0.activation_post_process.zero_point', 'distilbert.transformer.layer.2.attention.context_layer_matmul.input_quant_stubs.1.activation_post_process.scale', 'distilbert.transformer.layer.3.attention.q_lin.module.weight_fake_quant.fake_quant_enabled', 'distilbert.transformer.layer.3.attention.v_lin.quant.activation_post_process.zero_point', 'distilbert.transformer.layer.5.attention.v_lin.quant.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.3.attention.v_lin.quant.activation_post_process.scale', 'distilbert.transformer.layer.0.attention.v_lin.quant.activation_post_process.scale', 'distilbert.transformer.layer.2.attention.context_layer_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.4.ffn.lin1.quant.activation_post_process.scale', 'distilbert.transformer.layer.3.attention.attention_scores_matmul.input_quant_stubs.1.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.1.attention.k_lin.quant.activation_post_process.observer_enabled', 'distilbert.transformer.layer.1.attention.out_lin.module.weight_fake_quant.activation_post_process.eps', 'distilbert.transformer.layer.3.attention.q_lin.module.weight_fake_quant.activation_post_process.max_val', 'distilbert.transformer.layer.5.attention.context_layer_matmul.input_quant_stubs.1.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.5.ffn.lin1.module.weight_fake_quant.activation_post_process.min_val', 'distilbert.transformer.layer.3.attention.attention_scores_matmul.output_quant_stubs.0.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.4.ffn.lin2.module.bias', 'distilbert.transformer.layer.2.attention.q_lin.module.weight_fake_quant.fake_quant_enabled', 'distilbert.transformer.layer.0.ffn.lin1.module.weight_fake_quant.activation_post_process.min_val', 'distilbert.transformer.layer.5.attention.v_lin.quant.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.1.attention.v_lin.quant.activation_post_process.observer_enabled', 'distilbert.transformer.layer.4.attention.attention_scores_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.5.attention.context_layer_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'classifier.module.weight_fake_quant.observer_enabled', 'distilbert.transformer.layer.3.attention.out_lin.quant.activation_post_process.scale', 'distilbert.transformer.layer.1.attention.context_layer_matmul.input_quant_stubs.0.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.0.attention.k_lin.module.weight_fake_quant.activation_post_process.max_val', 'distilbert.transformer.layer.1.attention.attention_scores_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.3.ffn.lin1.quant.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.0.ffn.lin1.quant.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.2.attention.q_lin.module.weight', 'distilbert.transformer.layer.1.attention.context_layer_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.2.attention.attention_scores_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.3.attention.q_lin.quant.activation_post_process.observer_enabled', 'distilbert.transformer.layer.4.attention.out_lin.module.weight_fake_quant.fake_quant_enabled', 'pre_classifier.quant.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.2.attention.k_lin.quant.activation_post_process.zero_point', 'distilbert.transformer.layer.1.attention.attention_scores_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.1.attention.k_lin.module.weight_fake_quant.activation_post_process.max_val', 'distilbert.transformer.layer.0.attention.attention_scores_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.2.ffn.lin1.module.weight_fake_quant.activation_post_process.min_val', 'distilbert.transformer.layer.3.attention.k_lin.module.weight_fake_quant.activation_post_process.eps', 'distilbert.transformer.layer.3.attention.out_lin.quant.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.2.attention.q_lin.module.weight_fake_quant.activation_post_process.max_val', 'distilbert.transformer.layer.1.attention.attention_scores_matmul.input_quant_stubs.1.activation_post_process.observer_enabled', 'distilbert.transformer.layer.2.attention.v_lin.quant.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.5.attention.context_layer_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.0.attention.k_lin.module.weight_fake_quant.zero_point', 'distilbert.transformer.layer.5.ffn.lin1.quant.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.1.ffn.lin2.module.weight', 'distilbert.transformer.layer.2.attention.out_lin.quant.activation_post_process.scale', 'distilbert.transformer.layer.4.attention.out_lin.quant.activation_post_process.observer_enabled', 'distilbert.transformer.layer.1.attention.out_lin.module.weight_fake_quant.fake_quant_enabled', 'distilbert.transformer.layer.0.attention.k_lin.module.bias', 'distilbert.transformer.layer.3.attention.q_lin.quant.activation_post_process.zero_point', 'distilbert.transformer.layer.2.attention.context_layer_matmul.input_quant_stubs.0.activation_post_process.scale', 'distilbert.transformer.layer.5.attention.k_lin.module.weight_fake_quant.observer_enabled', 'distilbert.transformer.layer.2.attention.k_lin.module.weight_fake_quant.activation_post_process.min_val', 'distilbert.transformer.layer.1.ffn.lin2.module.weight_fake_quant.observer_enabled', 'distilbert.transformer.layer.0.ffn.lin2.module.weight_fake_quant.observer_enabled', 'distilbert.transformer.layer.5.attention.attention_scores_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.1.attention.q_lin.quant.activation_post_process.zero_point', 'distilbert.transformer.layer.2.attention.context_layer_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.eps', 'classifier.module.weight_fake_quant.activation_post_process.min_val', 'distilbert.transformer.layer.2.attention.k_lin.quant.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.3.ffn.lin1.module.weight', 'distilbert.transformer.layer.3.ffn.lin2.quant.activation_post_process.observer_enabled', 'distilbert.transformer.layer.4.attention.out_lin.module.weight_fake_quant.observer_enabled', 'distilbert.transformer.layer.5.attention.k_lin.module.bias', 'distilbert.transformer.layer.5.attention.attention_scores_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.2.attention.attention_scores_matmul.input_quant_stubs.0.activation_post_process.zero_point', 'distilbert.transformer.layer.1.ffn.lin1.quant.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.4.ffn.lin2.module.weight_fake_quant.zero_point', 'distilbert.transformer.layer.4.attention.v_lin.module.weight_fake_quant.activation_post_process.eps', 'distilbert.transformer.layer.5.ffn.lin2.module.weight_fake_quant.activation_post_process.max_val', 'distilbert.transformer.layer.3.attention.attention_scores_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.1.attention.attention_scores_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.3.attention.q_lin.module.weight_fake_quant.observer_enabled', 'distilbert.transformer.layer.4.attention.q_lin.module.weight_fake_quant.activation_post_process.eps', 'distilbert.transformer.layer.0.ffn.lin2.module.weight_fake_quant.zero_point', 'distilbert.transformer.layer.1.attention.q_lin.quant.activation_post_process.observer_enabled', 'distilbert.transformer.layer.5.attention.attention_scores_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.1.ffn.lin1.module.weight_fake_quant.activation_post_process.max_val', 'distilbert.transformer.layer.1.attention.attention_scores_matmul.input_quant_stubs.0.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.5.attention.v_lin.module.weight_fake_quant.scale', 'distilbert.transformer.layer.0.attention.k_lin.quant.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.1.attention.context_layer_matmul.input_quant_stubs.1.activation_post_process.observer_enabled', 'distilbert.transformer.layer.5.attention.k_lin.quant.activation_post_process.zero_point', 'distilbert.transformer.layer.0.attention.v_lin.module.weight_fake_quant.zero_point', 'distilbert.transformer.layer.1.attention.v_lin.module.weight_fake_quant.observer_enabled', 'distilbert.transformer.layer.5.ffn.lin1.quant.activation_post_process.observer_enabled', 'distilbert.transformer.layer.0.ffn.lin2.module.weight', 'distilbert.transformer.layer.2.ffn.lin1.quant.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.2.attention.context_layer_matmul.input_quant_stubs.1.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.2.attention.out_lin.module.weight_fake_quant.fake_quant_enabled', 'distilbert.transformer.layer.1.attention.q_lin.module.weight_fake_quant.scale', 'distilbert.transformer.layer.1.ffn.lin2.quant.activation_post_process.observer_enabled', 'distilbert.transformer.layer.0.attention.q_lin.module.weight_fake_quant.observer_enabled', 'distilbert.transformer.layer.3.attention.context_layer_matmul.input_quant_stubs.1.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.2.attention.v_lin.quant.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.4.attention.attention_scores_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.0.ffn.lin2.module.weight_fake_quant.activation_post_process.eps', 'distilbert.transformer.layer.1.attention.attention_scores_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.5.ffn.lin1.quant.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.5.ffn.lin2.module.weight_fake_quant.activation_post_process.min_val', 'distilbert.transformer.layer.1.ffn.lin1.quant.activation_post_process.observer_enabled', 'distilbert.transformer.layer.0.ffn.lin1.quant.activation_post_process.scale', 'distilbert.transformer.layer.3.attention.k_lin.quant.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.2.attention.k_lin.quant.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.3.attention.attention_scores_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.2.attention.out_lin.module.weight_fake_quant.scale', 'distilbert.transformer.layer.3.attention.v_lin.quant.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.4.attention.v_lin.quant.activation_post_process.observer_enabled', 'distilbert.transformer.layer.4.attention.attention_scores_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.5.attention.k_lin.module.weight_fake_quant.zero_point', 'distilbert.transformer.layer.0.attention.out_lin.quant.activation_post_process.zero_point', 'distilbert.transformer.layer.3.ffn.lin1.quant.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.2.ffn.lin2.quant.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.1.attention.q_lin.quant.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.4.attention.attention_scores_matmul.output_quant_stubs.0.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.0.attention.context_layer_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.0.attention.v_lin.quant.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.5.ffn.lin2.quant.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.5.attention.out_lin.quant.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.1.attention.attention_scores_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.3.attention.v_lin.quant.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.4.ffn.lin2.quant.activation_post_process.zero_point', 'distilbert.transformer.layer.5.attention.attention_scores_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.0.attention.context_layer_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.4.attention.out_lin.quant.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.4.ffn.lin1.module.weight_fake_quant.scale', 'distilbert.transformer.layer.0.attention.k_lin.quant.activation_post_process.scale', 'distilbert.transformer.layer.3.attention.attention_scores_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.4.attention.out_lin.module.weight_fake_quant.activation_post_process.min_val', 'distilbert.transformer.layer.4.ffn.lin1.module.weight_fake_quant.observer_enabled', 'distilbert.transformer.layer.2.attention.k_lin.module.weight_fake_quant.fake_quant_enabled', 'distilbert.embeddings.position_embeddings.weight_fake_quant.fake_quant_enabled', 'distilbert.transformer.layer.2.ffn.lin2.quant.activation_post_process.observer_enabled', 'distilbert.transformer.layer.3.attention.q_lin.module.weight', 'distilbert.transformer.layer.0.ffn.lin1.module.weight_fake_quant.fake_quant_enabled', 'distilbert.transformer.layer.2.attention.v_lin.quant.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.3.attention.q_lin.module.bias', 'distilbert.transformer.layer.4.attention.k_lin.quant.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.3.attention.attention_scores_matmul.input_quant_stubs.0.activation_post_process.scale', 'distilbert.transformer.layer.2.attention.context_layer_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.0.attention.context_layer_matmul.input_quant_stubs.1.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.4.attention.context_layer_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.1.attention.k_lin.quant.activation_post_process.scale', 'distilbert.embeddings.position_embeddings.weight_fake_quant.activation_post_process.max_val', 'distilbert.transformer.layer.2.ffn.lin2.module.weight_fake_quant.observer_enabled', 'distilbert.transformer.layer.2.attention.q_lin.quant.activation_post_process.zero_point', 'distilbert.transformer.layer.0.attention.out_lin.module.weight_fake_quant.zero_point', 'distilbert.transformer.layer.0.attention.out_lin.quant.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.5.attention.attention_scores_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.0.attention.attention_scores_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.0.attention.context_layer_matmul.input_quant_stubs.0.activation_post_process.observer_enabled', 'distilbert.transformer.layer.0.attention.out_lin.quant.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.5.attention.v_lin.quant.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.2.attention.attention_scores_matmul.output_quant_stubs.0.activation_post_process.zero_point', 'distilbert.transformer.layer.5.attention.k_lin.quant.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.3.ffn.lin2.module.bias', 'distilbert.transformer.layer.2.attention.out_lin.module.weight_fake_quant.activation_post_process.min_val', 'distilbert.transformer.layer.1.ffn.lin1.quant.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.1.attention.q_lin.module.weight_fake_quant.fake_quant_enabled', 'distilbert.transformer.layer.1.attention.k_lin.quant.activation_post_process.zero_point', 'distilbert.transformer.layer.0.attention.k_lin.module.weight_fake_quant.scale', 'distilbert.transformer.layer.1.ffn.lin2.module.weight_fake_quant.activation_post_process.max_val', 'distilbert.transformer.layer.1.attention.k_lin.module.weight_fake_quant.activation_post_process.eps', 'distilbert.transformer.layer.4.ffn.lin1.quant.activation_post_process.activation_post_process.min_val', 'classifier.module.weight_fake_quant.zero_point', 'distilbert.transformer.layer.1.ffn.lin2.module.bias', 'distilbert.transformer.layer.3.attention.attention_scores_matmul.input_quant_stubs.1.activation_post_process.zero_point', 'distilbert.transformer.layer.3.attention.context_layer_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.5.attention.v_lin.quant.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.4.attention.q_lin.module.weight_fake_quant.zero_point', 'distilbert.transformer.layer.0.attention.k_lin.quant.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.5.attention.context_layer_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.5.attention.v_lin.module.weight_fake_quant.zero_point', 'distilbert.transformer.layer.1.attention.k_lin.module.weight_fake_quant.observer_enabled', 'distilbert.transformer.layer.2.attention.out_lin.module.bias', 'distilbert.transformer.layer.4.attention.q_lin.module.weight_fake_quant.observer_enabled', 'distilbert.transformer.layer.0.attention.out_lin.module.weight_fake_quant.activation_post_process.max_val', 'distilbert.transformer.layer.5.attention.v_lin.quant.activation_post_process.scale', 'distilbert.transformer.layer.0.ffn.lin2.module.weight_fake_quant.fake_quant_enabled', 'distilbert.transformer.layer.1.attention.context_layer_matmul.input_quant_stubs.0.activation_post_process.zero_point', 'distilbert.transformer.layer.4.attention.q_lin.module.weight_fake_quant.activation_post_process.min_val', 'distilbert.transformer.layer.4.attention.attention_scores_matmul.input_quant_stubs.0.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.3.ffn.lin2.module.weight_fake_quant.activation_post_process.min_val', 'distilbert.transformer.layer.4.ffn.lin1.module.weight_fake_quant.fake_quant_enabled', 'distilbert.transformer.layer.0.attention.q_lin.module.weight_fake_quant.activation_post_process.max_val', 'distilbert.transformer.layer.4.ffn.lin2.quant.activation_post_process.activation_post_process.min_val', 'distilbert.embeddings.word_embeddings.weight_fake_quant.activation_post_process.min_val', 'distilbert.transformer.layer.0.attention.attention_scores_matmul.input_quant_stubs.0.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.1.attention.out_lin.module.weight_fake_quant.scale', 'distilbert.transformer.layer.2.attention.attention_scores_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.5.attention.attention_scores_matmul.output_quant_stubs.0.activation_post_process.scale', 'distilbert.transformer.layer.0.attention.q_lin.quant.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.1.attention.k_lin.module.bias', 'distilbert.transformer.layer.5.attention.v_lin.module.weight_fake_quant.activation_post_process.min_val', 'classifier.quant.activation_post_process.scale', 'distilbert.transformer.layer.2.ffn.lin2.module.weight_fake_quant.zero_point', 'distilbert.transformer.layer.1.attention.out_lin.module.weight_fake_quant.activation_post_process.max_val', 'distilbert.transformer.layer.2.ffn.lin2.module.weight_fake_quant.fake_quant_enabled', 'distilbert.transformer.layer.0.attention.out_lin.quant.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.3.attention.attention_scores_matmul.input_quant_stubs.0.activation_post_process.observer_enabled', 'distilbert.transformer.layer.0.attention.attention_scores_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.0.attention.attention_scores_matmul.input_quant_stubs.1.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.1.attention.out_lin.quant.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.5.attention.context_layer_matmul.input_quant_stubs.0.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.4.attention.attention_scores_matmul.output_quant_stubs.0.activation_post_process.zero_point', 'distilbert.transformer.layer.0.attention.v_lin.quant.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.4.attention.out_lin.quant.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.4.attention.v_lin.quant.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.3.attention.context_layer_matmul.input_quant_stubs.1.activation_post_process.observer_enabled', 'distilbert.transformer.layer.1.attention.q_lin.module.weight_fake_quant.activation_post_process.min_val', 'distilbert.transformer.layer.0.attention.q_lin.quant.activation_post_process.observer_enabled', 'distilbert.transformer.layer.1.attention.v_lin.module.weight_fake_quant.activation_post_process.eps', 'distilbert.transformer.layer.3.attention.out_lin.module.weight_fake_quant.zero_point', 'distilbert.transformer.layer.0.attention.out_lin.module.weight_fake_quant.activation_post_process.min_val', 'distilbert.transformer.layer.2.attention.attention_scores_matmul.input_quant_stubs.0.activation_post_process.observer_enabled', 'distilbert.transformer.layer.0.attention.out_lin.module.weight_fake_quant.scale', 'classifier.module.weight_fake_quant.activation_post_process.eps', 'distilbert.transformer.layer.1.attention.context_layer_matmul.input_quant_stubs.1.activation_post_process.scale', 'distilbert.transformer.layer.3.attention.context_layer_matmul.input_quant_stubs.1.activation_post_process.scale', 'distilbert.transformer.layer.2.attention.attention_scores_matmul.input_quant_stubs.1.activation_post_process.scale', 'distilbert.transformer.layer.4.attention.q_lin.quant.activation_post_process.observer_enabled', 'distilbert.transformer.layer.3.attention.attention_scores_matmul.input_quant_stubs.0.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.5.attention.attention_scores_matmul.output_quant_stubs.0.activation_post_process.zero_point', 'distilbert.transformer.layer.5.attention.out_lin.module.weight_fake_quant.activation_post_process.max_val', 'distilbert.transformer.layer.4.attention.k_lin.quant.activation_post_process.observer_enabled', 'distilbert.transformer.layer.4.attention.attention_scores_matmul.input_quant_stubs.1.activation_post_process.scale', 'distilbert.transformer.layer.4.ffn.lin1.module.weight_fake_quant.activation_post_process.max_val', 'distilbert.transformer.layer.5.ffn.lin2.quant.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.1.attention.v_lin.module.bias', 'distilbert.transformer.layer.3.attention.v_lin.quant.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.0.attention.attention_scores_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.2.attention.q_lin.quant.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.4.attention.v_lin.quant.activation_post_process.scale', 'distilbert.transformer.layer.0.ffn.lin2.quant.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.4.attention.attention_scores_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.5.attention.out_lin.quant.activation_post_process.observer_enabled', 'distilbert.transformer.layer.4.attention.k_lin.module.weight_fake_quant.activation_post_process.min_val', 'distilbert.transformer.layer.1.attention.v_lin.module.weight_fake_quant.zero_point', 'distilbert.transformer.layer.3.attention.out_lin.module.weight', 'distilbert.transformer.layer.3.ffn.lin2.module.weight_fake_quant.activation_post_process.eps', 'pre_classifier.quant.activation_post_process.scale', 'distilbert.transformer.layer.0.attention.q_lin.module.bias', 'distilbert.transformer.layer.2.attention.k_lin.module.weight_fake_quant.scale', 'distilbert.transformer.layer.2.ffn.lin1.module.weight_fake_quant.scale', 'distilbert.transformer.layer.1.attention.context_layer_matmul.input_quant_stubs.1.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.2.ffn.lin2.module.weight', 'distilbert.transformer.layer.5.attention.k_lin.module.weight', 'distilbert.transformer.layer.4.ffn.lin2.module.weight_fake_quant.observer_enabled', 'distilbert.transformer.layer.0.attention.context_layer_matmul.input_quant_stubs.0.activation_post_process.zero_point', 'distilbert.transformer.layer.5.ffn.lin1.module.weight_fake_quant.activation_post_process.max_val', 'classifier.quant.activation_post_process.fake_quant_enabled', 'classifier.module.weight_fake_quant.fake_quant_enabled', 'distilbert.transformer.layer.4.attention.v_lin.module.weight_fake_quant.scale', 'distilbert.transformer.layer.1.attention.attention_scores_matmul.input_quant_stubs.0.activation_post_process.scale', 'distilbert.transformer.layer.1.attention.v_lin.quant.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.1.attention.attention_scores_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.0.ffn.lin1.module.weight_fake_quant.observer_enabled', 'distilbert.transformer.layer.1.ffn.lin2.module.weight_fake_quant.scale', 'distilbert.transformer.layer.2.attention.out_lin.quant.activation_post_process.zero_point', 'distilbert.transformer.layer.3.attention.q_lin.module.weight_fake_quant.scale', 'distilbert.transformer.layer.0.attention.context_layer_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.3.attention.out_lin.module.weight_fake_quant.scale', 'distilbert.transformer.layer.4.ffn.lin2.module.weight', 'distilbert.transformer.layer.0.attention.out_lin.module.weight', 'distilbert.transformer.layer.5.ffn.lin1.quant.activation_post_process.scale', 'pre_classifier.quant.activation_post_process.observer_enabled', 'distilbert.transformer.layer.2.attention.v_lin.module.weight_fake_quant.zero_point', 'distilbert.transformer.layer.0.ffn.lin2.module.weight_fake_quant.activation_post_process.min_val', 'distilbert.transformer.layer.1.attention.q_lin.quant.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.4.attention.k_lin.quant.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.5.attention.attention_scores_matmul.input_quant_stubs.1.activation_post_process.observer_enabled', 'distilbert.transformer.layer.3.ffn.lin2.quant.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.5.ffn.lin1.module.weight', 'distilbert.transformer.layer.3.ffn.lin2.quant.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.3.attention.out_lin.quant.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.5.attention.v_lin.module.weight_fake_quant.fake_quant_enabled', 'classifier.quant.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.3.ffn.lin1.module.weight_fake_quant.zero_point', 'distilbert.transformer.layer.3.attention.v_lin.module.weight_fake_quant.zero_point', 'distilbert.transformer.layer.3.ffn.lin2.module.weight_fake_quant.activation_post_process.max_val', 'distilbert.transformer.layer.2.attention.out_lin.quant.activation_post_process.observer_enabled', 'distilbert.transformer.layer.0.attention.v_lin.module.weight_fake_quant.observer_enabled', 'distilbert.transformer.layer.2.attention.out_lin.module.weight', 'distilbert.transformer.layer.3.ffn.lin1.module.bias', 'distilbert.embeddings.position_embeddings.weight_fake_quant.zero_point', 'distilbert.transformer.layer.3.attention.k_lin.quant.activation_post_process.fake_quant_enabled', 'classifier.module.bias', 'distilbert.transformer.layer.1.attention.q_lin.module.weight_fake_quant.zero_point', 'distilbert.transformer.layer.5.attention.out_lin.module.weight_fake_quant.observer_enabled', 'distilbert.transformer.layer.4.ffn.lin2.module.weight_fake_quant.activation_post_process.min_val', 'distilbert.transformer.layer.0.attention.v_lin.quant.activation_post_process.zero_point', 'distilbert.transformer.layer.1.ffn.lin1.module.weight_fake_quant.zero_point', 'distilbert.transformer.layer.0.ffn.lin1.quant.activation_post_process.observer_enabled', 'distilbert.transformer.layer.4.attention.attention_scores_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.eps', 'pre_classifier.module.weight_fake_quant.fake_quant_enabled', 'distilbert.transformer.layer.1.attention.context_layer_matmul.input_quant_stubs.1.activation_post_process.zero_point', 'distilbert.transformer.layer.1.ffn.lin2.module.weight_fake_quant.activation_post_process.eps', 'distilbert.transformer.layer.2.attention.q_lin.quant.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.2.attention.q_lin.quant.activation_post_process.observer_enabled', 'distilbert.transformer.layer.0.attention.context_layer_matmul.input_quant_stubs.1.activation_post_process.scale', 'distilbert.transformer.layer.5.attention.attention_scores_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.3.ffn.lin1.module.weight_fake_quant.activation_post_process.min_val', 'distilbert.transformer.layer.5.attention.k_lin.quant.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.1.attention.v_lin.module.weight_fake_quant.scale', 'distilbert.transformer.layer.0.ffn.lin2.quant.activation_post_process.observer_enabled', 'distilbert.transformer.layer.4.attention.context_layer_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.4.ffn.lin1.module.bias', 'distilbert.transformer.layer.1.ffn.lin1.module.weight_fake_quant.activation_post_process.min_val', 'distilbert.transformer.layer.0.attention.attention_scores_matmul.output_quant_stubs.0.activation_post_process.scale', 'distilbert.transformer.layer.3.ffn.lin1.quant.activation_post_process.zero_point', 'distilbert.transformer.layer.5.attention.k_lin.module.weight_fake_quant.fake_quant_enabled', 'distilbert.transformer.layer.2.attention.context_layer_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.5.attention.v_lin.module.bias', 'pre_classifier.module.weight_fake_quant.scale', 'distilbert.embeddings.position_embeddings.weight_fake_quant.observer_enabled', 'distilbert.transformer.layer.1.ffn.lin1.module.weight_fake_quant.fake_quant_enabled', 'distilbert.transformer.layer.0.attention.v_lin.module.weight_fake_quant.activation_post_process.eps', 'distilbert.transformer.layer.0.attention.attention_scores_matmul.output_quant_stubs.0.activation_post_process.observer_enabled', 'distilbert.transformer.layer.0.attention.k_lin.quant.activation_post_process.observer_enabled', 'distilbert.transformer.layer.3.attention.q_lin.quant.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.4.ffn.lin2.module.weight_fake_quant.activation_post_process.eps', 'distilbert.transformer.layer.1.attention.k_lin.module.weight', 'distilbert.transformer.layer.0.attention.attention_scores_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.3.attention.attention_scores_matmul.output_quant_stubs.0.activation_post_process.observer_enabled', 'distilbert.transformer.layer.2.attention.attention_scores_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.1.attention.q_lin.module.weight_fake_quant.activation_post_process.max_val', 'distilbert.transformer.layer.5.ffn.lin2.module.weight_fake_quant.zero_point', 'distilbert.transformer.layer.4.attention.attention_scores_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.5.attention.q_lin.module.weight_fake_quant.observer_enabled', 'distilbert.transformer.layer.2.attention.k_lin.module.weight_fake_quant.observer_enabled', 'distilbert.transformer.layer.1.ffn.lin1.module.weight_fake_quant.activation_post_process.eps', 'distilbert.transformer.layer.5.attention.k_lin.module.weight_fake_quant.activation_post_process.eps', 'distilbert.transformer.layer.2.ffn.lin2.quant.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.2.attention.v_lin.module.weight', 'distilbert.transformer.layer.4.attention.k_lin.module.weight_fake_quant.activation_post_process.eps', 'distilbert.transformer.layer.3.attention.context_layer_matmul.input_quant_stubs.1.activation_post_process.zero_point', 'distilbert.transformer.layer.5.attention.v_lin.module.weight_fake_quant.activation_post_process.eps', 'distilbert.transformer.layer.1.attention.out_lin.module.weight_fake_quant.observer_enabled', 'distilbert.embeddings.word_embeddings.weight_fake_quant.activation_post_process.eps', 'distilbert.transformer.layer.1.attention.context_layer_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.4.attention.v_lin.quant.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.4.ffn.lin2.module.weight_fake_quant.scale', 'distilbert.transformer.layer.3.attention.q_lin.module.weight_fake_quant.activation_post_process.eps', 'distilbert.transformer.layer.4.attention.k_lin.module.weight_fake_quant.scale', 'distilbert.transformer.layer.5.attention.q_lin.module.weight_fake_quant.activation_post_process.eps', 'distilbert.transformer.layer.2.ffn.lin1.module.bias', 'distilbert.transformer.layer.5.attention.attention_scores_matmul.output_quant_stubs.0.activation_post_process.observer_enabled', 'distilbert.transformer.layer.1.attention.q_lin.quant.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.5.attention.out_lin.module.weight_fake_quant.activation_post_process.min_val', 'distilbert.transformer.layer.4.attention.out_lin.quant.activation_post_process.scale', 'distilbert.transformer.layer.4.attention.k_lin.module.weight_fake_quant.fake_quant_enabled', 'distilbert.transformer.layer.5.ffn.lin2.module.weight_fake_quant.fake_quant_enabled', 'distilbert.transformer.layer.4.attention.context_layer_matmul.input_quant_stubs.1.activation_post_process.scale', 'distilbert.transformer.layer.2.ffn.lin2.quant.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.3.attention.q_lin.module.weight_fake_quant.zero_point', 'distilbert.transformer.layer.2.attention.v_lin.quant.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.2.attention.k_lin.quant.activation_post_process.observer_enabled', 'distilbert.transformer.layer.0.ffn.lin1.quant.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.3.attention.context_layer_matmul.input_quant_stubs.0.activation_post_process.zero_point', 'distilbert.transformer.layer.0.attention.v_lin.module.weight_fake_quant.scale', 'distilbert.transformer.layer.4.attention.q_lin.module.weight_fake_quant.activation_post_process.max_val', 'distilbert.transformer.layer.1.attention.attention_scores_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.1.ffn.lin1.module.weight_fake_quant.observer_enabled', 'distilbert.transformer.layer.3.attention.k_lin.module.weight', 'distilbert.transformer.layer.4.ffn.lin2.quant.activation_post_process.observer_enabled', 'classifier.quant.activation_post_process.observer_enabled', 'distilbert.transformer.layer.2.attention.attention_scores_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.5.ffn.lin1.module.weight_fake_quant.fake_quant_enabled', 'distilbert.transformer.layer.2.attention.attention_scores_matmul.input_quant_stubs.0.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.3.ffn.lin2.quant.activation_post_process.zero_point', 'distilbert.transformer.layer.3.attention.attention_scores_matmul.output_quant_stubs.0.activation_post_process.zero_point', 'distilbert.transformer.layer.5.attention.q_lin.quant.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.1.attention.attention_scores_matmul.output_quant_stubs.0.activation_post_process.scale', 'distilbert.transformer.layer.5.attention.context_layer_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.1.attention.context_layer_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.2.attention.attention_scores_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.0.attention.q_lin.quant.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.2.ffn.lin1.module.weight', 'distilbert.transformer.layer.5.attention.q_lin.module.weight_fake_quant.fake_quant_enabled', 'distilbert.transformer.layer.5.ffn.lin1.module.weight_fake_quant.activation_post_process.eps', 'distilbert.transformer.layer.2.attention.context_layer_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.4.attention.attention_scores_matmul.input_quant_stubs.0.activation_post_process.observer_enabled', 'distilbert.transformer.layer.1.attention.v_lin.quant.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.4.ffn.lin1.quant.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.2.attention.k_lin.module.weight_fake_quant.zero_point', 'distilbert.transformer.layer.1.attention.v_lin.module.weight_fake_quant.activation_post_process.max_val', 'distilbert.transformer.layer.4.attention.v_lin.module.weight_fake_quant.activation_post_process.min_val', 'distilbert.transformer.layer.5.attention.q_lin.quant.activation_post_process.scale', 'distilbert.transformer.layer.2.attention.attention_scores_matmul.output_quant_stubs.0.activation_post_process.observer_enabled', 'distilbert.transformer.layer.5.ffn.lin1.quant.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.4.attention.attention_scores_matmul.input_quant_stubs.1.activation_post_process.observer_enabled', 'distilbert.transformer.layer.4.attention.context_layer_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.4.ffn.lin2.quant.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.3.ffn.lin1.module.weight_fake_quant.activation_post_process.max_val', 'distilbert.transformer.layer.5.attention.context_layer_matmul.input_quant_stubs.0.activation_post_process.observer_enabled', 'distilbert.transformer.layer.1.attention.q_lin.module.weight_fake_quant.activation_post_process.eps', 'distilbert.transformer.layer.4.attention.k_lin.quant.activation_post_process.scale', 'distilbert.transformer.layer.2.attention.out_lin.module.weight_fake_quant.activation_post_process.max_val', 'distilbert.transformer.layer.0.attention.q_lin.quant.activation_post_process.zero_point', 'distilbert.transformer.layer.1.attention.context_layer_matmul.input_quant_stubs.0.activation_post_process.scale', 'distilbert.transformer.layer.3.attention.k_lin.module.weight_fake_quant.activation_post_process.min_val', 'distilbert.transformer.layer.2.attention.attention_scores_matmul.output_quant_stubs.0.activation_post_process.scale', 'distilbert.transformer.layer.5.ffn.lin2.quant.activation_post_process.scale', 'distilbert.transformer.layer.5.attention.context_layer_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.eps', 'distilbert.transformer.layer.5.attention.out_lin.module.weight_fake_quant.scale', 'distilbert.transformer.layer.5.ffn.lin2.quant.activation_post_process.activation_post_process.max_val', 'distilbert.transformer.layer.5.attention.out_lin.quant.activation_post_process.scale', 'pre_classifier.quant.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.2.attention.v_lin.module.weight_fake_quant.activation_post_process.max_val', 'distilbert.transformer.layer.4.attention.context_layer_matmul.input_quant_stubs.0.activation_post_process.observer_enabled', 'distilbert.transformer.layer.1.attention.out_lin.module.bias', 'distilbert.transformer.layer.2.ffn.lin1.module.weight_fake_quant.activation_post_process.max_val', 'distilbert.transformer.layer.4.attention.context_layer_matmul.input_quant_stubs.1.activation_post_process.observer_enabled', 'distilbert.transformer.layer.2.attention.k_lin.quant.activation_post_process.fake_quant_enabled', 'distilbert.transformer.layer.0.attention.out_lin.quant.activation_post_process.scale', 'distilbert.transformer.layer.4.attention.context_layer_matmul.input_quant_stubs.0.activation_post_process.scale', 'distilbert.transformer.layer.4.ffn.lin1.module.weight_fake_quant.activation_post_process.min_val', 'distilbert.transformer.layer.5.attention.q_lin.module.weight', 'distilbert.transformer.layer.5.attention.k_lin.module.weight_fake_quant.activation_post_process.max_val', 'distilbert.transformer.layer.0.ffn.lin1.module.weight_fake_quant.zero_point', 'distilbert.transformer.layer.0.attention.v_lin.module.weight', 'distilbert.transformer.layer.3.attention.out_lin.quant.activation_post_process.observer_enabled', 'distilbert.transformer.layer.1.attention.out_lin.quant.activation_post_process.activation_post_process.min_val', 'distilbert.transformer.layer.2.attention.context_layer_matmul.input_quant_stubs.0.activation_post_process.zero_point', 'distilbert.transformer.layer.3.attention.k_lin.module.weight_fake_quant.observer_enabled', 'distilbert.transformer.layer.2.attention.context_layer_matmul.input_quant_stubs.0.activation_post_process.observer_enabled']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at /opt/app-root/src/aicoe-osc-demo/models/distilbert_qqp_pruned80/training and are newly initialized: ['distilbert.transformer.layer.2.attention.k_lin.bias', 'distilbert.transformer.layer.0.attention.q_lin.bias', 'distilbert.transformer.layer.0.ffn.lin2.bias', 'distilbert.transformer.layer.0.attention.v_lin.bias', 'distilbert.transformer.layer.1.attention.q_lin.bias', 'distilbert.transformer.layer.2.attention.v_lin.bias', 'distilbert.transformer.layer.2.attention.q_lin.bias', 'distilbert.transformer.layer.1.ffn.lin1.weight', 'distilbert.transformer.layer.4.attention.out_lin.bias', 'distilbert.transformer.layer.1.ffn.lin2.weight', 'distilbert.transformer.layer.1.ffn.lin1.bias', 'distilbert.transformer.layer.4.ffn.lin1.bias', 'distilbert.transformer.layer.2.attention.out_lin.weight', 'distilbert.transformer.layer.3.attention.q_lin.weight', 'distilbert.transformer.layer.0.attention.out_lin.bias', 'distilbert.transformer.layer.3.attention.out_lin.bias', 'pre_classifier.bias', 'distilbert.transformer.layer.4.ffn.lin1.weight', 'distilbert.transformer.layer.0.attention.q_lin.weight', 'distilbert.transformer.layer.5.ffn.lin1.weight', 'distilbert.transformer.layer.0.attention.k_lin.bias', 'distilbert.transformer.layer.3.ffn.lin1.bias', 'distilbert.transformer.layer.4.ffn.lin2.bias', 'distilbert.transformer.layer.3.ffn.lin1.weight', 'pre_classifier.weight', 'distilbert.transformer.layer.2.ffn.lin2.bias', 'distilbert.transformer.layer.5.attention.out_lin.weight', 'distilbert.transformer.layer.2.attention.out_lin.bias', 'distilbert.transformer.layer.5.attention.v_lin.weight', 'distilbert.transformer.layer.1.attention.k_lin.weight', 'distilbert.transformer.layer.5.attention.k_lin.weight', 'distilbert.transformer.layer.2.attention.q_lin.weight', 'distilbert.transformer.layer.1.attention.k_lin.bias', 'distilbert.transformer.layer.3.attention.k_lin.bias', 'distilbert.transformer.layer.3.attention.v_lin.bias', 'distilbert.transformer.layer.1.attention.v_lin.bias', 'distilbert.transformer.layer.3.attention.q_lin.bias', 'distilbert.transformer.layer.1.attention.out_lin.bias', 'distilbert.transformer.layer.4.attention.v_lin.bias', 'distilbert.transformer.layer.4.attention.q_lin.weight', 'distilbert.transformer.layer.4.attention.k_lin.weight', 'distilbert.transformer.layer.4.attention.v_lin.weight', 'distilbert.transformer.layer.5.ffn.lin1.bias', 'distilbert.transformer.layer.4.attention.q_lin.bias', 'distilbert.transformer.layer.5.attention.v_lin.bias', 'distilbert.transformer.layer.3.attention.k_lin.weight', 'distilbert.transformer.layer.2.attention.v_lin.weight', 'distilbert.transformer.layer.3.ffn.lin2.weight', 'distilbert.transformer.layer.3.attention.v_lin.weight', 'distilbert.transformer.layer.5.attention.q_lin.bias', 'classifier.weight', 'distilbert.transformer.layer.5.attention.q_lin.weight', 'distilbert.transformer.layer.4.attention.out_lin.weight', 'distilbert.transformer.layer.3.attention.out_lin.weight', 'distilbert.transformer.layer.3.ffn.lin2.bias', 'distilbert.transformer.layer.5.attention.k_lin.bias', 'distilbert.transformer.layer.0.ffn.lin2.weight', 'distilbert.transformer.layer.2.ffn.lin2.weight', 'distilbert.transformer.layer.5.attention.out_lin.bias', 'distilbert.transformer.layer.1.attention.v_lin.weight', 'classifier.bias', 'distilbert.transformer.layer.1.attention.q_lin.weight', 'distilbert.transformer.layer.0.attention.v_lin.weight', 'distilbert.transformer.layer.1.attention.out_lin.weight', 'distilbert.transformer.layer.0.attention.k_lin.weight', 'distilbert.transformer.layer.0.attention.out_lin.weight', 'distilbert.transformer.layer.2.ffn.lin1.bias', 'distilbert.transformer.layer.0.ffn.lin1.bias', 'distilbert.transformer.layer.1.ffn.lin2.bias', 'distilbert.transformer.layer.0.ffn.lin1.weight', 'distilbert.transformer.layer.2.attention.k_lin.weight', 'distilbert.transformer.layer.5.ffn.lin2.weight', 'distilbert.transformer.layer.2.ffn.lin1.weight', 'distilbert.transformer.layer.5.ffn.lin2.bias', 'distilbert.transformer.layer.4.ffn.lin2.weight', 'distilbert.transformer.layer.4.attention.k_lin.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "defaf8b3473b43c2879d1671d2ffc778",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cb0a4a9f947456ea91f92c1d92d425e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: question, token_type_ids, sentence, __index_level_0__. If question, token_type_ids, sentence, __index_level_0__ are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/opt/app-root/lib64/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2033\n",
      "  Num Epochs = 16\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2048\n",
      "  Number of trainable parameters = 66955010\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2048' max='2048' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2048/2048 03:57, Epoch 16/16]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.386000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.176100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.104900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.062500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: question, token_type_ids, sentence, __index_level_0__. If question, token_type_ids, sentence, __index_level_0__ are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 509\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: question, token_type_ids, sentence, __index_level_0__. If question, token_type_ids, sentence, __index_level_0__ are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 509\n",
      "  Batch size = 16\n",
      "/opt/app-root/lib64/python3.8/site-packages/sklearn/metrics/_classification.py:1580: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/opt/app-root/lib64/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/app-root/lib64/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to /opt/app-root/src/aicoe-osc-demo/models/transformers/distilbert_qqp_pruned80\n",
      "Configuration saved in /opt/app-root/src/aicoe-osc-demo/models/transformers/distilbert_qqp_pruned80/config.json\n",
      "Model weights saved in /opt/app-root/src/aicoe-osc-demo/models/transformers/distilbert_qqp_pruned80/pytorch_model.bin\n",
      "tokenizer config file saved in /opt/app-root/src/aicoe-osc-demo/models/transformers/distilbert_qqp_pruned80/tokenizer_config.json\n",
      "Special tokens file saved in /opt/app-root/src/aicoe-osc-demo/models/transformers/distilbert_qqp_pruned80/special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0125be3bf563441baf9133f77b44909a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "downloading...:   0%|          | 0.00/656 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7b716bb89ad45f582d73b4d1237629e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "downloading...:   0%|          | 0.00/197 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71fa1ee95e024f7faa905ae3791d6fb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "downloading...:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67a28238f97a466f9059aa2928b0c130",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "downloading...:   0%|          | 0.00/358 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eeae4ecf393f420b881962fc2864884f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "downloading...:   0%|          | 0.00/986 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dd9f9ea8eea453fa602fe1f510015fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "downloading...:   0%|          | 0.00/480 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "106db9c1d01c48cead1b7f6aa736827d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "downloading...:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b10557269e384603b81c22b468247625",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "downloading...:   0%|          | 0.00/13.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ada85810f5a54b178fd5aeb405c6f0c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "downloading...:   0%|          | 0.00/418M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf742dbe123a4f9ea8ff1be4353d3545",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "downloading...:   0%|          | 0.00/3.23k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60936c09e94c4a618f60da884f0b447b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "downloading...:   0%|          | 0.00/695k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dde2a1213434c4ebf6eaac1f1dd946b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "downloading...:   0%|          | 0.00/1.63k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "017c075f80a24384aa54a0511457d3cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "downloading...:   0%|          | 0.00/105M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "759267f709094d7b97e6e4ff5416bf35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "downloading...:   0%|          | 0.00/695k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22cc7419470e4e97a84e44bce3b94ca6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "downloading...:   0%|          | 0.00/986 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56df4e153118408c993bcfd3a6c0d5a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "downloading...:   0%|          | 0.00/358 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf5a22a88ac145b2a7978d74aba5decf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "downloading...:   0%|          | 0.00/3.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22e7376d94f5420da7889946b3094df7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "downloading...:   0%|          | 0.00/1.30k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7b84a0321094518a770baa00b325bcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "downloading...:   0%|          | 0.00/823 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48641aca57fd40d6a22fedcb6f55b84e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "downloading...:   0%|          | 0.00/3.95k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7f15a36cf824b74b49456eadc3e2023",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "downloading...:   0%|          | 0.00/105M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading configuration file /opt/app-root/src/aicoe-osc-demo/models/obert_mnli_pruned90/training/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"/opt/app-root/src/aicoe-osc-demo/models/obert_mnli_pruned90/training\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"finetuning_task\": \"mnli\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file /opt/app-root/src/aicoe-osc-demo/models/obert_mnli_pruned90/training/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /opt/app-root/src/aicoe-osc-demo/models/obert_mnli_pruned90/training were not used when initializing BertForSequenceClassification: ['bert.encoder.layer.9.output.dense.module.bias', 'bert.encoder.layer.10.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.observer_enabled', 'bert.encoder.layer.11.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.3.output.dense.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.8.output.dense.module.weight_fake_quant.zero_point', 'bert.encoder.layer.6.attention.output.dense.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.10.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.7.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.3.attention.self.value.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.1.attention.output.dense.module.weight_fake_quant.activation_post_process.min_val', 'bert.embeddings.position_embeddings.activation_post_process.scale', 'bert.encoder.layer.10.attention.self.query.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.5.intermediate.dense.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.1.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.4.intermediate.dense.quant.activation_post_process.scale', 'bert.encoder.layer.6.attention.self.query.module.weight_fake_quant.scale', 'bert.encoder.layer.5.output.dense.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.8.output.dense.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.1.attention.output.dense.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.11.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.scale', 'bert.encoder.layer.6.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.11.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.7.attention.self.query.module.weight', 'bert.encoder.layer.2.attention.self.key.module.weight_fake_quant.activation_post_process.eps', 'bert.pooler.dense.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.6.attention.self.query.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.0.attention.self.key.quant.activation_post_process.scale', 'bert.encoder.layer.2.attention.self.query.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.10.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.observer_enabled', 'bert.encoder.layer.10.intermediate.dense.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.10.attention.output.dense.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.3.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.4.attention.output.dense.module.weight_fake_quant.observer_enabled', 'bert.embeddings.word_embeddings.weight_fake_quant.observer_enabled', 'bert.encoder.layer.8.attention.self.value.quant.activation_post_process.scale', 'bert.encoder.layer.8.output.dense.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.5.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.6.intermediate.dense.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.5.attention.output.dense.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.5.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.zero_point', 'bert.encoder.layer.7.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.7.intermediate.dense.quant.activation_post_process.zero_point', 'bert.encoder.layer.9.attention.output.dense.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.3.attention.self.value.quant.activation_post_process.scale', 'bert.encoder.layer.6.attention.self.query.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.9.attention.self.query.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.0.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.10.attention.self.key.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.7.attention.self.key.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.2.attention.self.value.module.weight', 'bert.encoder.layer.10.attention.self.key.module.weight_fake_quant.scale', 'bert.encoder.layer.7.attention.self.query.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.11.attention.self.query.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.0.attention.self.value.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.5.attention.self.value.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.8.attention.self.value.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.8.attention.self.value.module.weight_fake_quant.zero_point', 'bert.encoder.layer.6.output.dense.quant.activation_post_process.observer_enabled', 'bert.pooler.dense.module.weight_fake_quant.zero_point', 'bert.encoder.layer.3.attention.output.dense.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.0.attention.self.value.module.weight', 'bert.encoder.layer.0.output.dense.module.bias', 'bert.encoder.layer.2.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.zero_point', 'bert.encoder.layer.4.intermediate.dense.module.weight_fake_quant.activation_post_process.max_val', 'classifier.module.weight', 'bert.encoder.layer.8.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.9.attention.output.dense.module.weight_fake_quant.observer_enabled', 'bert.embeddings.position_embeddings.activation_post_process.observer_enabled', 'bert.encoder.layer.3.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.observer_enabled', 'bert.encoder.layer.11.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.scale', 'bert.encoder.layer.1.intermediate.dense.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.6.output.dense.quant.activation_post_process.zero_point', 'bert.encoder.layer.2.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.max_val', 'bert.embeddings.word_embeddings.weight_fake_quant.scale', 'bert.encoder.layer.0.attention.self.query.quant.activation_post_process.scale', 'bert.encoder.layer.8.output.dense.quant.activation_post_process.activation_post_process.eps', 'bert.embeddings.position_embeddings.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.3.intermediate.dense.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.4.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.4.attention.self.query.quant.activation_post_process.activation_post_process.max_val', 'bert.embeddings.word_embeddings.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.10.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.7.attention.output.dense.quant.activation_post_process.scale', 'bert.encoder.layer.4.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.10.intermediate.dense.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.11.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.2.output.dense.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.4.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.4.attention.self.query.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.3.intermediate.dense.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.5.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.zero_point', 'bert.encoder.layer.1.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.9.attention.self.key.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.1.attention.self.query.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.11.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.5.intermediate.dense.module.weight_fake_quant.scale', 'bert.encoder.layer.8.attention.self.key.module.weight_fake_quant.zero_point', 'bert.encoder.layer.7.attention.output.dense.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.4.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.5.attention.output.dense.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.2.attention.self.query.quant.activation_post_process.zero_point', 'bert.encoder.layer.3.output.dense.module.weight_fake_quant.scale', 'bert.encoder.layer.2.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.zero_point', 'bert.encoder.layer.6.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.observer_enabled', 'bert.encoder.layer.3.attention.self.value.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.6.intermediate.dense.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.9.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.3.attention.self.key.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.5.intermediate.dense.quant.activation_post_process.zero_point', 'bert.encoder.layer.6.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.2.attention.self.value.quant.activation_post_process.zero_point', 'bert.encoder.layer.9.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.8.attention.self.query.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.6.attention.self.value.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.7.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.zero_point', 'bert.encoder.layer.10.intermediate.dense.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.4.attention.output.dense.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.0.attention.output.dense.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.0.attention.self.query.quant.activation_post_process.zero_point', 'bert.encoder.layer.7.attention.self.key.module.weight_fake_quant.zero_point', 'bert.encoder.layer.5.attention.output.dense.module.weight', 'bert.encoder.layer.5.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.10.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.4.attention.self.query.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.3.attention.self.value.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.5.attention.self.key.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.11.attention.self.query.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.2.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.2.output.dense.module.bias', 'bert.encoder.layer.11.output.dense.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.6.attention.output.dense.module.weight', 'bert.encoder.layer.11.attention.self.query.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.6.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.8.attention.self.key.module.bias', 'bert.encoder.layer.10.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.zero_point', 'bert.pooler.dense.quant.activation_post_process.zero_point', 'bert.encoder.layer.9.intermediate.dense.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.3.attention.output.dense.quant.activation_post_process.zero_point', 'bert.encoder.layer.1.intermediate.dense.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.2.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.scale', 'bert.encoder.layer.1.attention.self.query.quant.activation_post_process.zero_point', 'bert.encoder.layer.3.attention.output.dense.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.7.attention.self.value.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.11.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.zero_point', 'bert.encoder.layer.11.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.4.attention.output.dense.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.7.attention.self.value.quant.activation_post_process.scale', 'bert.encoder.layer.0.attention.output.dense.module.weight_fake_quant.zero_point', 'bert.encoder.layer.2.output.dense.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.10.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.zero_point', 'bert.encoder.layer.1.attention.self.value.quant.activation_post_process.zero_point', 'bert.encoder.layer.2.intermediate.dense.quant.activation_post_process.scale', 'bert.encoder.layer.6.output.dense.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.10.attention.self.query.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.5.attention.self.query.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.1.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.7.intermediate.dense.module.weight_fake_quant.scale', 'bert.encoder.layer.7.intermediate.dense.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.3.intermediate.dense.module.weight_fake_quant.zero_point', 'bert.encoder.layer.2.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.4.intermediate.dense.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.7.attention.self.query.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.7.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.observer_enabled', 'bert.encoder.layer.10.attention.self.value.module.weight', 'bert.encoder.layer.5.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.6.attention.self.query.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.8.output.dense.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.3.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.11.intermediate.dense.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.5.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.observer_enabled', 'bert.encoder.layer.8.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.zero_point', 'bert.encoder.layer.9.attention.self.query.quant.activation_post_process.zero_point', 'bert.encoder.layer.11.attention.self.query.module.bias', 'bert.encoder.layer.8.attention.self.key.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.4.attention.self.key.module.bias', 'bert.encoder.layer.1.attention.self.value.module.weight', 'bert.encoder.layer.3.intermediate.dense.module.weight_fake_quant.scale', 'bert.encoder.layer.4.attention.self.key.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.3.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.zero_point', 'bert.encoder.layer.9.intermediate.dense.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.3.attention.self.value.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.3.attention.self.query.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.10.attention.self.value.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.2.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.zero_point', 'bert.encoder.layer.8.intermediate.dense.quant.activation_post_process.scale', 'bert.encoder.layer.5.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.9.attention.self.query.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.11.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.6.attention.self.value.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.6.attention.self.query.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.2.output.dense.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.3.output.dense.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.6.attention.self.value.module.weight_fake_quant.zero_point', 'bert.encoder.layer.7.output.dense.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.11.intermediate.dense.module.bias', 'bert.encoder.layer.2.attention.output.dense.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.6.attention.self.query.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.3.attention.self.query.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.11.output.dense.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.0.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.2.attention.self.value.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.5.attention.self.value.module.weight_fake_quant.scale', 'bert.encoder.layer.10.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.scale', 'bert.encoder.layer.4.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.0.attention.output.dense.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.9.output.dense.module.weight_fake_quant.zero_point', 'bert.encoder.layer.5.attention.self.query.quant.activation_post_process.zero_point', 'bert.encoder.layer.11.intermediate.dense.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.11.intermediate.dense.module.weight_fake_quant.activation_post_process.eps', 'bert.embeddings.position_embeddings.weight_fake_quant.scale', 'classifier.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.4.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.10.output.dense.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.9.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.zero_point', 'bert.encoder.layer.10.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.eps', 'bert.pooler.dense.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.1.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.4.output.dense.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.2.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.0.attention.self.key.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.3.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.observer_enabled', 'bert.pooler.dense.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.6.intermediate.dense.module.bias', 'bert.encoder.layer.10.intermediate.dense.quant.activation_post_process.scale', 'bert.encoder.layer.11.attention.self.key.quant.activation_post_process.scale', 'bert.encoder.layer.11.attention.self.value.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.11.output.dense.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.11.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.1.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.eps', 'bert.pooler.dense.module.bias', 'bert.encoder.layer.7.output.dense.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.6.attention.self.key.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.11.output.dense.module.weight_fake_quant.zero_point', 'bert.encoder.layer.3.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.scale', 'bert.encoder.layer.9.attention.output.dense.quant.activation_post_process.scale', 'bert.encoder.layer.1.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.10.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.1.output.dense.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.2.attention.output.dense.module.weight_fake_quant.zero_point', 'bert.encoder.layer.4.attention.self.query.module.bias', 'bert.encoder.layer.1.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.11.attention.self.query.quant.activation_post_process.scale', 'bert.encoder.layer.10.attention.self.key.module.weight', 'bert.embeddings.token_type_embeddings.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.10.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.0.attention.self.query.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.8.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.10.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.zero_point', 'bert.encoder.layer.7.attention.output.dense.module.weight', 'bert.encoder.layer.2.intermediate.dense.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.1.attention.self.query.module.weight_fake_quant.zero_point', 'bert.encoder.layer.1.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.6.attention.self.value.module.weight', 'bert.encoder.layer.10.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.6.attention.self.query.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.3.attention.self.query.module.weight_fake_quant.zero_point', 'bert.encoder.layer.11.attention.output.dense.module.bias', 'bert.encoder.layer.5.attention.self.query.module.weight_fake_quant.zero_point', 'bert.encoder.layer.9.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.observer_enabled', 'bert.encoder.layer.3.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.scale', 'bert.encoder.layer.3.attention.output.dense.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.8.attention.self.value.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.8.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.5.output.dense.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.10.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.3.attention.self.key.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.3.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.7.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.3.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.zero_point', 'bert.encoder.layer.10.attention.self.query.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.4.attention.self.value.module.weight', 'bert.encoder.layer.9.attention.output.dense.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.3.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.4.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.zero_point', 'bert.encoder.layer.6.attention.output.dense.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.8.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.9.attention.self.key.module.bias', 'bert.encoder.layer.2.attention.self.key.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.2.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.observer_enabled', 'bert.encoder.layer.10.attention.self.value.module.weight_fake_quant.scale', 'bert.encoder.layer.6.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.7.attention.output.dense.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.9.attention.self.value.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.10.attention.output.dense.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.6.attention.self.key.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.8.attention.self.key.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.7.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.observer_enabled', 'bert.encoder.layer.10.attention.output.dense.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.10.output.dense.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.8.intermediate.dense.module.bias', 'bert.encoder.layer.6.attention.self.query.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.6.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.observer_enabled', 'bert.encoder.layer.1.attention.self.value.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.1.attention.output.dense.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.7.intermediate.dense.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.11.intermediate.dense.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.6.attention.self.query.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.0.attention.self.value.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.7.attention.self.key.quant.activation_post_process.activation_post_process.max_val', 'bert.embeddings.position_embeddings.weight_fake_quant.observer_enabled', 'bert.encoder.layer.11.output.dense.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.5.attention.self.value.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.3.attention.output.dense.module.bias', 'bert.encoder.layer.9.attention.self.query.quant.activation_post_process.scale', 'bert.encoder.layer.1.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.9.attention.self.value.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.11.intermediate.dense.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.3.attention.self.query.quant.activation_post_process.zero_point', 'bert.encoder.layer.4.attention.self.value.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.0.intermediate.dense.quant.activation_post_process.scale', 'bert.encoder.layer.2.attention.self.key.module.weight_fake_quant.zero_point', 'bert.encoder.layer.11.attention.self.value.module.weight_fake_quant.zero_point', 'bert.encoder.layer.11.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.3.attention.output.dense.quant.activation_post_process.scale', 'bert.encoder.layer.1.intermediate.dense.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.6.output.dense.module.bias', 'bert.encoder.layer.8.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.scale', 'bert.encoder.layer.6.attention.self.query.module.weight_fake_quant.zero_point', 'bert.encoder.layer.0.attention.self.key.quant.activation_post_process.zero_point', 'bert.encoder.layer.11.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.10.attention.self.key.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.5.attention.output.dense.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.7.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'bert.embeddings.word_embeddings.activation_post_process.fake_quant_enabled', 'bert.embeddings.token_type_embeddings.activation_post_process.scale', 'classifier.module.weight_fake_quant.scale', 'bert.encoder.layer.1.attention.self.query.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.9.attention.self.value.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.2.attention.output.dense.module.bias', 'bert.encoder.layer.3.intermediate.dense.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.8.attention.self.query.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.3.output.dense.module.bias', 'bert.encoder.layer.7.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.scale', 'bert.encoder.layer.9.attention.self.query.module.weight', 'bert.encoder.layer.8.attention.self.query.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.7.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.zero_point', 'bert.encoder.layer.1.attention.self.key.quant.activation_post_process.scale', 'bert.encoder.layer.11.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.scale', 'bert.encoder.layer.4.attention.self.query.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.1.attention.self.key.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.4.attention.output.dense.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.4.output.dense.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.0.attention.output.dense.quant.activation_post_process.zero_point', 'bert.encoder.layer.3.output.dense.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.0.attention.self.value.module.bias', 'bert.encoder.layer.1.attention.output.dense.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.11.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.2.intermediate.dense.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.3.attention.self.key.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.7.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.9.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.scale', 'bert.encoder.layer.6.intermediate.dense.module.weight_fake_quant.scale', 'bert.encoder.layer.5.output.dense.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.9.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.10.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.observer_enabled', 'bert.encoder.layer.10.attention.self.value.quant.activation_post_process.scale', 'bert.encoder.layer.0.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.10.intermediate.dense.module.weight_fake_quant.scale', 'bert.encoder.layer.9.intermediate.dense.quant.activation_post_process.activation_post_process.max_val', 'bert.pooler.dense.module.weight', 'bert.encoder.layer.2.attention.self.value.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.8.intermediate.dense.module.weight_fake_quant.zero_point', 'bert.encoder.layer.2.attention.self.value.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.4.attention.self.value.module.bias', 'bert.encoder.layer.6.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.zero_point', 'bert.encoder.layer.9.output.dense.module.weight', 'bert.encoder.layer.10.output.dense.module.weight', 'bert.encoder.layer.0.attention.self.value.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.0.attention.self.query.module.weight_fake_quant.scale', 'bert.encoder.layer.11.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.4.attention.self.key.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.1.output.dense.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.4.attention.self.key.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.10.attention.self.key.module.bias', 'bert.encoder.layer.3.attention.self.key.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.9.attention.self.query.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.8.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.fake_quant_enabled', 'bert.embeddings.position_embeddings.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.5.intermediate.dense.module.weight_fake_quant.zero_point', 'bert.encoder.layer.3.output.dense.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.3.attention.output.dense.module.weight_fake_quant.zero_point', 'bert.encoder.layer.7.attention.self.query.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.9.attention.self.key.quant.activation_post_process.scale', 'bert.encoder.layer.10.attention.self.value.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.9.attention.self.value.module.bias', 'bert.encoder.layer.3.attention.self.value.module.weight_fake_quant.scale', 'bert.encoder.layer.9.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.0.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.2.output.dense.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.2.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.8.attention.self.key.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.8.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.6.output.dense.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.1.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.9.attention.self.value.module.weight', 'bert.encoder.layer.6.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.scale', 'bert.encoder.layer.10.attention.self.value.module.weight_fake_quant.zero_point', 'bert.encoder.layer.6.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.6.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.1.attention.self.value.module.bias', 'bert.encoder.layer.2.output.dense.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.0.attention.output.dense.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.0.attention.self.key.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.1.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.5.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.scale', 'bert.encoder.layer.10.output.dense.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.8.attention.self.value.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.2.output.dense.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.3.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.2.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.zero_point', 'bert.encoder.layer.0.attention.output.dense.module.weight_fake_quant.scale', 'bert.encoder.layer.7.attention.self.value.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.11.output.dense.quant.activation_post_process.scale', 'bert.encoder.layer.0.attention.self.value.module.weight_fake_quant.scale', 'bert.encoder.layer.4.output.dense.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.7.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.observer_enabled', 'bert.encoder.layer.3.attention.self.query.module.weight_fake_quant.scale', 'bert.encoder.layer.0.attention.self.value.quant.activation_post_process.zero_point', 'bert.encoder.layer.8.attention.self.value.quant.activation_post_process.zero_point', 'bert.encoder.layer.9.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.observer_enabled', 'bert.encoder.layer.1.output.dense.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.5.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.3.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.6.output.dense.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.9.attention.output.dense.quant.activation_post_process.zero_point', 'bert.encoder.layer.4.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.scale', 'bert.encoder.layer.4.attention.self.key.quant.activation_post_process.zero_point', 'bert.encoder.layer.7.attention.self.key.quant.activation_post_process.zero_point', 'bert.encoder.layer.6.intermediate.dense.module.weight', 'bert.encoder.layer.8.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.zero_point', 'bert.encoder.layer.3.attention.self.value.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.6.attention.self.value.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.11.attention.self.key.quant.activation_post_process.zero_point', 'bert.encoder.layer.0.intermediate.dense.module.weight_fake_quant.zero_point', 'bert.encoder.layer.3.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.10.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.scale', 'bert.encoder.layer.9.intermediate.dense.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.8.attention.self.value.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.10.output.dense.quant.activation_post_process.zero_point', 'bert.encoder.layer.11.attention.self.key.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.11.attention.self.key.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.4.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.scale', 'bert.encoder.layer.1.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.scale', 'bert.encoder.layer.0.attention.self.query.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.6.attention.self.key.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.6.attention.self.value.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.11.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.observer_enabled', 'bert.encoder.layer.2.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.observer_enabled', 'bert.encoder.layer.8.attention.self.value.module.weight', 'bert.encoder.layer.2.attention.self.query.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.0.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.zero_point', 'bert.encoder.layer.4.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.zero_point', 'bert.encoder.layer.0.attention.self.value.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.9.attention.self.key.module.weight', 'bert.encoder.layer.9.attention.self.value.quant.activation_post_process.scale', 'bert.encoder.layer.9.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.1.attention.output.dense.quant.activation_post_process.zero_point', 'bert.encoder.layer.11.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.0.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.2.attention.self.value.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.6.intermediate.dense.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.0.attention.self.key.module.bias', 'bert.encoder.layer.4.attention.self.value.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.7.attention.output.dense.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.1.attention.self.value.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.9.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.observer_enabled', 'bert.encoder.layer.6.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.zero_point', 'bert.encoder.layer.6.attention.self.value.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.5.attention.self.query.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.6.attention.self.key.module.weight_fake_quant.zero_point', 'bert.encoder.layer.8.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.observer_enabled', 'bert.encoder.layer.9.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.10.attention.output.dense.module.bias', 'bert.encoder.layer.6.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.scale', 'bert.encoder.layer.10.attention.self.query.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.4.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.observer_enabled', 'bert.encoder.layer.1.attention.output.dense.module.weight_fake_quant.scale', 'bert.encoder.layer.0.attention.self.value.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.1.attention.output.dense.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.0.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.min_val', 'bert.pooler.dense.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.11.intermediate.dense.module.weight', 'bert.encoder.layer.8.attention.self.key.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.3.attention.self.value.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.0.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.scale', 'bert.encoder.layer.11.attention.output.dense.module.weight_fake_quant.fake_quant_enabled', 'bert.embeddings.position_embeddings.weight_fake_quant.zero_point', 'bert.encoder.layer.7.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.7.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.1.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.zero_point', 'classifier.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.4.output.dense.module.weight', 'bert.encoder.layer.8.attention.output.dense.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.5.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.2.attention.self.query.module.weight_fake_quant.scale', 'bert.encoder.layer.7.attention.self.value.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.0.output.dense.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.7.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.zero_point', 'bert.encoder.layer.0.attention.self.query.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.4.attention.self.value.module.weight_fake_quant.scale', 'bert.encoder.layer.4.attention.self.key.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.8.attention.self.key.module.weight', 'bert.encoder.layer.1.intermediate.dense.module.weight_fake_quant.zero_point', 'bert.encoder.layer.6.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.scale', 'bert.encoder.layer.3.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.9.intermediate.dense.quant.activation_post_process.zero_point', 'bert.encoder.layer.0.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.scale', 'bert.encoder.layer.9.attention.output.dense.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.9.intermediate.dense.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.7.intermediate.dense.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.10.intermediate.dense.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.3.attention.self.key.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.10.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.5.attention.self.key.quant.activation_post_process.scale', 'bert.encoder.layer.10.output.dense.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.0.intermediate.dense.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.8.attention.self.key.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.2.intermediate.dense.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.9.attention.self.key.module.weight_fake_quant.scale', 'bert.encoder.layer.10.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.2.attention.self.value.module.weight_fake_quant.zero_point', 'bert.encoder.layer.7.output.dense.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.11.intermediate.dense.module.weight_fake_quant.zero_point', 'bert.encoder.layer.6.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.7.attention.self.key.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.0.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.zero_point', 'bert.encoder.layer.2.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.zero_point', 'bert.encoder.layer.0.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.scale', 'bert.encoder.layer.10.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.observer_enabled', 'bert.encoder.layer.9.intermediate.dense.quant.activation_post_process.activation_post_process.min_val', 'bert.embeddings.token_type_embeddings.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.10.attention.self.value.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.5.attention.self.query.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.0.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.observer_enabled', 'bert.encoder.layer.4.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.4.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.2.intermediate.dense.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.5.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.7.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.scale', 'bert.encoder.layer.9.intermediate.dense.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.0.intermediate.dense.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.5.intermediate.dense.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.11.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.6.attention.self.key.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.3.attention.output.dense.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.7.attention.self.value.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.7.attention.self.key.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.2.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.7.attention.self.value.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.6.output.dense.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.10.attention.self.query.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.0.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.11.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.8.output.dense.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.7.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.zero_point', 'bert.encoder.layer.4.attention.self.value.quant.activation_post_process.zero_point', 'bert.encoder.layer.2.intermediate.dense.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.11.attention.self.value.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.10.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.10.intermediate.dense.module.bias', 'bert.encoder.layer.10.attention.self.key.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.2.attention.self.key.module.bias', 'bert.encoder.layer.11.attention.self.value.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.5.attention.self.value.quant.activation_post_process.scale', 'bert.encoder.layer.6.output.dense.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.0.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.observer_enabled', 'bert.encoder.layer.11.attention.output.dense.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.8.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.6.intermediate.dense.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.10.intermediate.dense.quant.activation_post_process.zero_point', 'bert.encoder.layer.7.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.3.attention.output.dense.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.1.attention.self.value.quant.activation_post_process.scale', 'classifier.module.weight_fake_quant.zero_point', 'bert.encoder.layer.5.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.11.attention.self.key.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.0.output.dense.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.1.attention.self.query.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.2.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.7.output.dense.quant.activation_post_process.zero_point', 'bert.encoder.layer.9.attention.self.query.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.11.attention.self.query.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.7.attention.self.query.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.11.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.scale', 'bert.encoder.layer.4.attention.output.dense.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.3.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.4.attention.self.key.module.weight', 'bert.encoder.layer.8.attention.output.dense.module.bias', 'bert.encoder.layer.2.attention.self.query.module.weight_fake_quant.zero_point', 'bert.encoder.layer.8.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.4.intermediate.dense.module.bias', 'bert.encoder.layer.11.attention.self.query.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.10.attention.self.query.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.4.attention.self.value.module.weight_fake_quant.activation_post_process.max_val', 'bert.pooler.dense.quant.activation_post_process.activation_post_process.min_val', 'bert.pooler.dense.module.weight_fake_quant.scale', 'bert.encoder.layer.7.attention.output.dense.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.11.output.dense.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.0.output.dense.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.9.attention.output.dense.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.9.intermediate.dense.module.weight_fake_quant.scale', 'bert.encoder.layer.8.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.3.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.0.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.2.attention.output.dense.module.weight_fake_quant.activation_post_process.max_val', 'bert.pooler.dense.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.11.attention.self.key.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.7.intermediate.dense.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.7.output.dense.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.5.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.8.attention.self.query.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.5.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.eps', 'classifier.quant.activation_post_process.scale', 'bert.encoder.layer.11.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.observer_enabled', 'bert.encoder.layer.4.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.zero_point', 'bert.encoder.layer.9.attention.output.dense.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.2.attention.output.dense.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.7.attention.output.dense.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.5.attention.self.query.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.3.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.6.attention.output.dense.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.7.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.scale', 'bert.encoder.layer.4.attention.self.query.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.0.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.2.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.scale', 'bert.encoder.layer.1.attention.self.value.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.5.attention.self.query.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.0.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.10.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.zero_point', 'bert.encoder.layer.0.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.observer_enabled', 'bert.encoder.layer.6.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.2.output.dense.quant.activation_post_process.scale', 'bert.encoder.layer.5.output.dense.module.weight_fake_quant.zero_point', 'bert.encoder.layer.11.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.observer_enabled', 'bert.encoder.layer.9.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.zero_point', 'bert.encoder.layer.8.output.dense.module.weight', 'bert.encoder.layer.9.attention.output.dense.module.weight_fake_quant.activation_post_process.eps', 'classifier.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.8.attention.self.query.module.weight', 'bert.encoder.layer.4.attention.output.dense.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.7.attention.self.value.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.6.attention.self.query.module.bias', 'bert.encoder.layer.1.intermediate.dense.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.5.intermediate.dense.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.8.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.observer_enabled', 'bert.encoder.layer.9.intermediate.dense.module.weight', 'bert.encoder.layer.11.attention.self.value.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.7.attention.self.query.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.4.attention.self.key.module.weight_fake_quant.zero_point', 'bert.encoder.layer.2.output.dense.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.3.attention.output.dense.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.6.attention.self.query.quant.activation_post_process.zero_point', 'bert.encoder.layer.10.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.10.attention.self.value.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.1.attention.self.query.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.9.attention.output.dense.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.11.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.1.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.0.attention.output.dense.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.5.attention.self.value.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.5.output.dense.quant.activation_post_process.scale', 'bert.encoder.layer.8.intermediate.dense.quant.activation_post_process.zero_point', 'bert.encoder.layer.1.attention.self.key.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.10.attention.self.value.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.6.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.0.intermediate.dense.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.4.attention.output.dense.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.5.attention.self.key.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.7.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.zero_point', 'bert.encoder.layer.5.attention.output.dense.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.7.intermediate.dense.module.weight_fake_quant.zero_point', 'bert.encoder.layer.3.attention.self.key.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.11.output.dense.module.weight', 'bert.encoder.layer.5.attention.output.dense.module.weight_fake_quant.observer_enabled', 'classifier.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.1.attention.self.value.module.weight_fake_quant.scale', 'bert.encoder.layer.8.attention.self.query.quant.activation_post_process.scale', 'bert.encoder.layer.0.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.1.attention.self.key.module.weight_fake_quant.scale', 'bert.embeddings.word_embeddings.activation_post_process.scale', 'bert.encoder.layer.8.attention.output.dense.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.10.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.4.intermediate.dense.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.7.attention.output.dense.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.1.intermediate.dense.quant.activation_post_process.zero_point', 'bert.encoder.layer.4.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.observer_enabled', 'bert.embeddings.word_embeddings.activation_post_process.zero_point', 'bert.encoder.layer.5.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.observer_enabled', 'bert.encoder.layer.2.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.1.attention.self.key.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.8.attention.self.query.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.10.output.dense.quant.activation_post_process.scale', 'bert.encoder.layer.11.attention.output.dense.module.weight_fake_quant.activation_post_process.eps', 'bert.embeddings.token_type_embeddings.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.9.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.1.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.scale', 'bert.encoder.layer.6.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.8.intermediate.dense.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.0.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.scale', 'bert.encoder.layer.10.attention.self.key.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.0.attention.self.value.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.9.output.dense.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.7.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.eps', 'bert.embeddings.position_embeddings.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.2.attention.output.dense.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.11.attention.output.dense.module.weight_fake_quant.scale', 'bert.encoder.layer.6.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.zero_point', 'bert.encoder.layer.1.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.scale', 'bert.encoder.layer.10.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.scale', 'bert.encoder.layer.11.attention.self.value.quant.activation_post_process.activation_post_process.max_val', 'classifier.module.bias', 'bert.encoder.layer.3.attention.self.key.quant.activation_post_process.zero_point', 'bert.encoder.layer.1.attention.self.query.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.6.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.observer_enabled', 'bert.encoder.layer.6.output.dense.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.4.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.7.attention.self.value.module.weight_fake_quant.scale', 'bert.encoder.layer.1.output.dense.quant.activation_post_process.scale', 'bert.encoder.layer.6.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.observer_enabled', 'bert.encoder.layer.8.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.observer_enabled', 'bert.encoder.layer.6.output.dense.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.5.attention.self.key.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.11.attention.self.key.module.weight_fake_quant.zero_point', 'bert.encoder.layer.11.attention.self.value.quant.activation_post_process.scale', 'bert.encoder.layer.1.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.3.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.scale', 'bert.encoder.layer.8.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.zero_point', 'bert.encoder.layer.4.output.dense.module.weight_fake_quant.scale', 'bert.encoder.layer.9.output.dense.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.6.attention.output.dense.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.0.attention.self.key.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.6.attention.output.dense.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.10.output.dense.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.4.intermediate.dense.module.weight_fake_quant.zero_point', 'bert.encoder.layer.10.attention.self.query.module.bias', 'bert.encoder.layer.6.output.dense.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.10.attention.self.value.module.bias', 'bert.encoder.layer.8.attention.self.query.module.weight_fake_quant.zero_point', 'bert.encoder.layer.1.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.10.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.3.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.3.attention.self.key.module.bias', 'bert.encoder.layer.0.attention.self.value.module.weight_fake_quant.zero_point', 'bert.encoder.layer.5.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.2.output.dense.module.weight', 'bert.encoder.layer.2.attention.self.query.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.11.output.dense.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.1.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.5.output.dense.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.8.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.zero_point', 'bert.encoder.layer.7.intermediate.dense.quant.activation_post_process.scale', 'bert.encoder.layer.6.attention.self.query.quant.activation_post_process.scale', 'bert.encoder.layer.6.attention.self.key.module.weight_fake_quant.scale', 'bert.encoder.layer.9.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.4.attention.self.value.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.5.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.4.intermediate.dense.module.weight', 'bert.encoder.layer.7.attention.self.value.module.weight', 'bert.encoder.layer.7.attention.self.query.module.bias', 'bert.encoder.layer.10.attention.output.dense.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.8.attention.self.query.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.8.attention.self.query.module.weight_fake_quant.scale', 'bert.encoder.layer.7.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.10.attention.output.dense.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.9.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.observer_enabled', 'bert.encoder.layer.5.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.10.intermediate.dense.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.3.intermediate.dense.module.bias', 'bert.encoder.layer.10.attention.self.key.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.2.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.8.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.scale', 'bert.encoder.layer.1.output.dense.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.1.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.scale', 'bert.encoder.layer.8.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.scale', 'bert.encoder.layer.6.attention.self.query.module.weight', 'bert.encoder.layer.8.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.scale', 'bert.encoder.layer.11.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.scale', 'bert.encoder.layer.11.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.zero_point', 'bert.encoder.layer.2.attention.self.query.module.bias', 'bert.encoder.layer.9.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.zero_point', 'bert.encoder.layer.4.attention.output.dense.quant.activation_post_process.scale', 'bert.encoder.layer.11.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.zero_point', 'bert.encoder.layer.1.attention.self.key.module.weight_fake_quant.zero_point', 'bert.encoder.layer.8.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.4.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'classifier.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.11.attention.self.query.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.6.attention.output.dense.quant.activation_post_process.zero_point', 'bert.encoder.layer.9.output.dense.module.weight_fake_quant.scale', 'bert.encoder.layer.11.attention.output.dense.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.11.attention.self.value.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.1.attention.self.key.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.7.attention.self.value.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.10.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.scale', 'bert.encoder.layer.2.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.9.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.zero_point', 'bert.embeddings.token_type_embeddings.weight_fake_quant.zero_point', 'bert.encoder.layer.5.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.scale', 'bert.encoder.layer.8.attention.self.query.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.2.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.4.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.observer_enabled', 'bert.encoder.layer.0.attention.self.key.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.10.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.zero_point', 'bert.encoder.layer.3.output.dense.module.weight_fake_quant.zero_point', 'bert.encoder.layer.9.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.11.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.9.attention.self.key.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.5.attention.self.value.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.4.attention.self.query.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.6.attention.output.dense.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.5.attention.self.value.quant.activation_post_process.zero_point', 'bert.encoder.layer.8.attention.output.dense.quant.activation_post_process.scale', 'bert.encoder.layer.9.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.scale', 'bert.encoder.layer.11.attention.self.query.module.weight', 'bert.embeddings.token_type_embeddings.weight_fake_quant.scale', 'bert.encoder.layer.6.attention.output.dense.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.9.output.dense.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.5.attention.output.dense.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.8.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.11.attention.self.query.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.10.attention.self.key.quant.activation_post_process.zero_point', 'bert.encoder.layer.7.attention.output.dense.quant.activation_post_process.zero_point', 'bert.encoder.layer.7.attention.self.query.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.6.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.6.intermediate.dense.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.5.attention.self.value.module.weight', 'bert.encoder.layer.4.attention.output.dense.module.weight_fake_quant.zero_point', 'bert.encoder.layer.8.attention.output.dense.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.7.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.3.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.3.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.5.intermediate.dense.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.9.intermediate.dense.module.weight_fake_quant.zero_point', 'bert.encoder.layer.2.attention.self.key.module.weight_fake_quant.scale', 'bert.encoder.layer.4.intermediate.dense.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.8.attention.self.value.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.0.attention.self.query.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.10.attention.output.dense.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.4.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.7.attention.self.query.module.weight_fake_quant.scale', 'bert.encoder.layer.10.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.0.intermediate.dense.module.weight', 'bert.encoder.layer.5.attention.self.query.module.weight_fake_quant.scale', 'bert.encoder.layer.8.attention.output.dense.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.8.output.dense.quant.activation_post_process.scale', 'bert.encoder.layer.3.attention.self.key.module.weight_fake_quant.zero_point', 'bert.encoder.layer.9.attention.self.key.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.0.attention.output.dense.quant.activation_post_process.scale', 'bert.encoder.layer.5.attention.self.value.module.weight_fake_quant.zero_point', 'bert.encoder.layer.7.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.8.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.10.attention.self.value.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.10.output.dense.module.weight_fake_quant.scale', 'bert.encoder.layer.2.attention.self.value.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.1.attention.self.value.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.0.intermediate.dense.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.8.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.scale', 'bert.encoder.layer.3.attention.self.query.module.weight', 'bert.encoder.layer.4.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.10.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.9.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.scale', 'bert.encoder.layer.3.attention.self.key.module.weight_fake_quant.scale', 'bert.encoder.layer.1.attention.output.dense.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.2.attention.self.key.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.3.output.dense.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.5.attention.self.value.module.bias', 'bert.encoder.layer.11.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.observer_enabled', 'bert.encoder.layer.4.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.scale', 'bert.encoder.layer.8.attention.output.dense.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.1.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.observer_enabled', 'bert.encoder.layer.0.attention.self.key.module.weight', 'bert.encoder.layer.2.attention.self.key.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.9.output.dense.quant.activation_post_process.zero_point', 'bert.encoder.layer.9.attention.self.query.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.6.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.scale', 'bert.encoder.layer.5.attention.self.key.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.0.intermediate.dense.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.8.attention.self.key.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.10.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.0.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.2.attention.self.key.quant.activation_post_process.scale', 'bert.encoder.layer.5.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.observer_enabled', 'bert.encoder.layer.6.attention.self.key.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.6.intermediate.dense.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.7.attention.self.query.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.9.attention.self.query.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.6.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.8.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.scale', 'bert.encoder.layer.5.intermediate.dense.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.4.attention.self.key.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.5.attention.output.dense.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.1.attention.output.dense.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.6.attention.self.value.module.weight_fake_quant.scale', 'bert.encoder.layer.9.attention.self.value.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.1.intermediate.dense.module.bias', 'bert.encoder.layer.1.intermediate.dense.module.weight', 'bert.encoder.layer.0.attention.self.query.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.0.output.dense.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.9.attention.self.value.module.weight_fake_quant.zero_point', 'bert.encoder.layer.11.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.zero_point', 'bert.encoder.layer.11.attention.self.query.module.weight_fake_quant.zero_point', 'bert.encoder.layer.1.attention.self.query.module.weight', 'bert.encoder.layer.7.attention.self.key.quant.activation_post_process.scale', 'bert.encoder.layer.8.attention.output.dense.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.0.attention.self.key.module.weight_fake_quant.scale', 'bert.encoder.layer.0.output.dense.quant.activation_post_process.scale', 'bert.encoder.layer.9.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.scale', 'bert.encoder.layer.10.attention.output.dense.quant.activation_post_process.zero_point', 'bert.encoder.layer.11.output.dense.module.weight_fake_quant.scale', 'bert.encoder.layer.11.attention.self.key.module.weight_fake_quant.scale', 'bert.encoder.layer.8.attention.output.dense.module.weight', 'bert.pooler.dense.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.2.attention.self.query.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.4.output.dense.quant.activation_post_process.activation_post_process.min_val', 'bert.embeddings.token_type_embeddings.weight_fake_quant.observer_enabled', 'bert.encoder.layer.6.attention.output.dense.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.10.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.observer_enabled', 'bert.encoder.layer.11.attention.self.value.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.4.attention.self.value.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.2.attention.output.dense.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.2.attention.self.key.module.weight', 'bert.encoder.layer.3.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.2.attention.self.key.quant.activation_post_process.zero_point', 'bert.encoder.layer.11.attention.output.dense.module.weight', 'bert.encoder.layer.9.attention.self.key.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.7.attention.output.dense.module.weight_fake_quant.scale', 'bert.encoder.layer.11.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.observer_enabled', 'bert.encoder.layer.3.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.zero_point', 'bert.encoder.layer.3.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.11.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.4.output.dense.quant.activation_post_process.scale', 'bert.encoder.layer.4.attention.output.dense.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.1.intermediate.dense.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.10.output.dense.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.0.output.dense.module.weight_fake_quant.zero_point', 'bert.encoder.layer.9.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.observer_enabled', 'bert.encoder.layer.11.intermediate.dense.module.weight_fake_quant.scale', 'bert.encoder.layer.2.attention.self.key.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.5.attention.self.query.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.6.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.observer_enabled', 'bert.encoder.layer.0.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.observer_enabled', 'bert.encoder.layer.2.intermediate.dense.module.weight_fake_quant.scale', 'bert.encoder.layer.6.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.9.attention.self.key.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.5.output.dense.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.5.attention.output.dense.module.weight_fake_quant.scale', 'bert.encoder.layer.5.attention.self.query.module.weight', 'bert.encoder.layer.2.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.7.attention.output.dense.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.7.intermediate.dense.module.weight', 'bert.encoder.layer.9.attention.self.value.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.6.attention.self.key.module.weight', 'bert.encoder.layer.5.intermediate.dense.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.0.attention.self.query.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.5.output.dense.module.bias', 'bert.encoder.layer.9.intermediate.dense.module.bias', 'bert.encoder.layer.4.attention.output.dense.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.3.attention.self.key.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.10.attention.self.query.module.weight_fake_quant.zero_point', 'bert.encoder.layer.0.output.dense.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.4.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.4.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.0.intermediate.dense.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.11.intermediate.dense.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.5.attention.self.value.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.6.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.zero_point', 'bert.encoder.layer.2.attention.output.dense.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.10.intermediate.dense.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.4.intermediate.dense.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.5.attention.self.key.module.bias', 'bert.encoder.layer.5.attention.output.dense.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.4.output.dense.module.bias', 'bert.encoder.layer.7.output.dense.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.4.attention.self.value.module.weight_fake_quant.zero_point', 'bert.encoder.layer.10.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.7.output.dense.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.7.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.2.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.observer_enabled', 'bert.encoder.layer.1.output.dense.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.7.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.observer_enabled', 'bert.encoder.layer.0.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.observer_enabled', 'bert.encoder.layer.2.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.2.intermediate.dense.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.3.output.dense.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.5.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.observer_enabled', 'bert.encoder.layer.11.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.2.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.scale', 'bert.encoder.layer.10.attention.self.query.module.weight', 'bert.encoder.layer.9.attention.self.value.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.4.attention.self.key.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.4.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.observer_enabled', 'bert.encoder.layer.3.attention.output.dense.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.11.attention.output.dense.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.6.intermediate.dense.quant.activation_post_process.zero_point', 'bert.encoder.layer.0.output.dense.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.1.output.dense.module.weight_fake_quant.zero_point', 'bert.encoder.layer.11.attention.self.query.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.0.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.zero_point', 'bert.encoder.layer.9.attention.self.query.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.7.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.observer_enabled', 'bert.encoder.layer.11.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.9.output.dense.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.9.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.4.intermediate.dense.quant.activation_post_process.zero_point', 'bert.encoder.layer.10.attention.output.dense.module.weight_fake_quant.scale', 'bert.encoder.layer.11.output.dense.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.6.attention.output.dense.module.weight_fake_quant.scale', 'bert.encoder.layer.9.intermediate.dense.quant.activation_post_process.scale', 'bert.encoder.layer.0.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.9.output.dense.quant.activation_post_process.scale', 'bert.encoder.layer.7.attention.self.query.module.weight_fake_quant.zero_point', 'bert.encoder.layer.6.output.dense.module.weight_fake_quant.scale', 'bert.encoder.layer.9.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.5.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.10.attention.self.key.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.0.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.5.attention.self.key.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.5.output.dense.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.10.attention.self.query.quant.activation_post_process.zero_point', 'bert.encoder.layer.10.attention.output.dense.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.1.output.dense.module.weight_fake_quant.scale', 'bert.encoder.layer.11.attention.self.key.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.3.attention.self.key.module.weight', 'bert.encoder.layer.0.output.dense.module.weight_fake_quant.scale', 'bert.encoder.layer.1.output.dense.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.10.attention.self.key.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.2.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.2.attention.output.dense.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.1.intermediate.dense.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.3.output.dense.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.5.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.observer_enabled', 'bert.encoder.layer.8.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.eps', 'bert.embeddings.position_embeddings.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.8.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.zero_point', 'bert.encoder.layer.0.intermediate.dense.quant.activation_post_process.activation_post_process.eps', 'classifier.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.8.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'bert.embeddings.position_embeddings.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.0.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.scale', 'bert.encoder.layer.4.attention.self.query.quant.activation_post_process.scale', 'bert.encoder.layer.4.intermediate.dense.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.7.output.dense.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.10.output.dense.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.10.attention.self.query.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.3.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.zero_point', 'bert.encoder.layer.11.attention.self.value.module.weight', 'bert.encoder.layer.6.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.scale', 'bert.encoder.layer.3.intermediate.dense.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.7.attention.self.query.quant.activation_post_process.scale', 'bert.encoder.layer.5.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.4.attention.output.dense.module.weight_fake_quant.scale', 'bert.encoder.layer.1.output.dense.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.0.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.zero_point', 'bert.encoder.layer.3.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.7.output.dense.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.8.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.3.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.6.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.11.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.zero_point', 'bert.encoder.layer.4.attention.self.key.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.10.intermediate.dense.module.weight', 'bert.encoder.layer.3.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.observer_enabled', 'bert.encoder.layer.4.intermediate.dense.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.5.attention.output.dense.quant.activation_post_process.zero_point', 'bert.encoder.layer.4.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.scale', 'bert.embeddings.word_embeddings.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.0.attention.output.dense.module.weight', 'bert.encoder.layer.11.attention.self.value.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.2.intermediate.dense.module.weight_fake_quant.observer_enabled', 'bert.embeddings.word_embeddings.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.8.attention.self.query.module.bias', 'bert.encoder.layer.10.attention.self.value.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.10.intermediate.dense.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.0.attention.self.value.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.9.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.11.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.5.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.4.intermediate.dense.module.weight_fake_quant.scale', 'bert.encoder.layer.7.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.6.attention.self.value.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.2.attention.output.dense.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.7.attention.output.dense.module.weight_fake_quant.zero_point', 'bert.encoder.layer.8.intermediate.dense.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.3.attention.output.dense.module.weight', 'bert.encoder.layer.8.attention.self.query.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.11.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.3.attention.self.query.module.bias', 'bert.encoder.layer.5.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.8.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.observer_enabled', 'bert.encoder.layer.6.attention.self.value.module.bias', 'bert.encoder.layer.4.output.dense.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.2.attention.self.key.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.0.attention.self.key.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.10.attention.output.dense.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.10.intermediate.dense.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.1.attention.self.value.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.1.attention.self.value.module.weight_fake_quant.zero_point', 'bert.encoder.layer.9.attention.self.key.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.3.attention.self.query.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.2.intermediate.dense.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.2.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.7.attention.output.dense.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.2.attention.self.query.module.weight', 'bert.encoder.layer.4.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.zero_point', 'bert.encoder.layer.8.attention.self.value.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.10.attention.self.query.quant.activation_post_process.scale', 'bert.encoder.layer.0.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.8.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.zero_point', 'bert.encoder.layer.0.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.0.attention.self.key.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.5.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.10.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.8.intermediate.dense.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.2.attention.self.query.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.7.attention.self.key.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.6.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.9.attention.self.query.module.weight_fake_quant.scale', 'bert.encoder.layer.6.attention.self.key.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.2.output.dense.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.0.attention.output.dense.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.6.attention.self.key.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.9.attention.self.key.module.weight_fake_quant.zero_point', 'bert.encoder.layer.2.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.4.output.dense.module.weight_fake_quant.zero_point', 'bert.encoder.layer.5.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.7.attention.output.dense.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.11.attention.self.key.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.11.intermediate.dense.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.6.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.scale', 'bert.encoder.layer.10.attention.self.key.quant.activation_post_process.scale', 'bert.encoder.layer.1.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.1.attention.self.query.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.3.attention.output.dense.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.9.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.8.output.dense.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.11.attention.self.value.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.4.attention.self.value.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.9.attention.self.value.module.weight_fake_quant.scale', 'bert.encoder.layer.7.intermediate.dense.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.8.output.dense.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.6.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.5.attention.self.key.module.weight', 'bert.encoder.layer.7.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.observer_enabled', 'bert.encoder.layer.8.attention.self.key.quant.activation_post_process.zero_point', 'bert.encoder.layer.5.attention.self.key.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.3.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.5.attention.output.dense.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.1.attention.output.dense.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.1.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.2.attention.output.dense.module.weight_fake_quant.scale', 'bert.encoder.layer.7.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.5.output.dense.module.weight_fake_quant.scale', 'bert.encoder.layer.2.attention.output.dense.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.11.attention.self.query.quant.activation_post_process.zero_point', 'bert.encoder.layer.4.output.dense.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.8.attention.output.dense.quant.activation_post_process.zero_point', 'bert.encoder.layer.9.attention.self.query.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.6.attention.self.value.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.3.attention.output.dense.module.weight_fake_quant.activation_post_process.min_val', 'bert.embeddings.position_embeddings.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.5.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.scale', 'bert.encoder.layer.4.attention.output.dense.module.weight', 'bert.encoder.layer.1.attention.self.query.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.10.attention.output.dense.module.weight_fake_quant.zero_point', 'bert.encoder.layer.2.output.dense.module.weight_fake_quant.zero_point', 'bert.encoder.layer.8.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.3.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.1.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.scale', 'bert.encoder.layer.10.intermediate.dense.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.2.attention.self.value.quant.activation_post_process.scale', 'bert.encoder.layer.2.output.dense.module.weight_fake_quant.scale', 'bert.encoder.layer.11.attention.self.key.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.6.attention.self.value.quant.activation_post_process.zero_point', 'bert.encoder.layer.9.attention.output.dense.module.weight', 'bert.encoder.layer.11.attention.self.query.module.weight_fake_quant.scale', 'bert.encoder.layer.3.intermediate.dense.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.9.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.6.attention.self.value.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.2.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.2.attention.self.query.quant.activation_post_process.scale', 'bert.encoder.layer.1.output.dense.quant.activation_post_process.zero_point', 'bert.encoder.layer.6.intermediate.dense.module.weight_fake_quant.zero_point', 'bert.encoder.layer.3.intermediate.dense.quant.activation_post_process.activation_post_process.min_val', 'bert.embeddings.token_type_embeddings.activation_post_process.zero_point', 'bert.encoder.layer.3.attention.self.query.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.8.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.9.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.0.attention.self.query.module.bias', 'bert.encoder.layer.9.attention.output.dense.module.bias', 'bert.encoder.layer.3.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.8.attention.output.dense.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.6.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.zero_point', 'bert.encoder.layer.6.attention.output.dense.quant.activation_post_process.scale', 'bert.encoder.layer.8.intermediate.dense.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.9.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.3.output.dense.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.0.attention.output.dense.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.2.attention.output.dense.quant.activation_post_process.zero_point', 'bert.encoder.layer.4.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.6.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.2.attention.self.query.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.1.attention.output.dense.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.1.intermediate.dense.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.4.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.8.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.8.output.dense.module.weight_fake_quant.scale', 'bert.encoder.layer.9.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.scale', 'bert.encoder.layer.8.attention.self.value.module.weight_fake_quant.scale', 'bert.encoder.layer.8.attention.self.value.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.0.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.2.attention.self.value.module.weight_fake_quant.scale', 'bert.encoder.layer.3.attention.self.key.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.7.attention.self.query.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.2.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.3.attention.self.value.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.6.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.4.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.9.output.dense.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.4.attention.self.key.quant.activation_post_process.scale', 'bert.encoder.layer.9.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.observer_enabled', 'bert.encoder.layer.7.attention.self.query.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.9.output.dense.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.2.attention.self.query.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.0.output.dense.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.5.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.scale', 'bert.encoder.layer.2.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.scale', 'bert.encoder.layer.0.output.dense.quant.activation_post_process.zero_point', 'bert.encoder.layer.0.attention.self.query.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.1.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.observer_enabled', 'bert.encoder.layer.2.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.scale', 'classifier.quant.activation_post_process.zero_point', 'bert.encoder.layer.1.attention.self.query.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.11.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.observer_enabled', 'bert.encoder.layer.5.output.dense.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.11.attention.output.dense.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.5.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.10.attention.self.query.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.2.attention.self.key.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.8.attention.self.key.quant.activation_post_process.scale', 'bert.encoder.layer.6.attention.output.dense.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.3.attention.self.key.quant.activation_post_process.scale', 'bert.encoder.layer.6.attention.output.dense.module.weight_fake_quant.zero_point', 'bert.encoder.layer.6.attention.self.key.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.5.attention.output.dense.quant.activation_post_process.scale', 'bert.encoder.layer.7.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.scale', 'bert.encoder.layer.9.output.dense.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.5.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.5.intermediate.dense.module.weight', 'bert.encoder.layer.10.output.dense.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.8.attention.self.key.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.11.attention.output.dense.quant.activation_post_process.scale', 'bert.encoder.layer.9.attention.output.dense.module.weight_fake_quant.zero_point', 'bert.encoder.layer.10.attention.self.value.quant.activation_post_process.zero_point', 'bert.encoder.layer.11.attention.self.value.module.bias', 'bert.encoder.layer.11.intermediate.dense.quant.activation_post_process.zero_point', 'bert.encoder.layer.1.attention.self.value.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.2.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.observer_enabled', 'bert.encoder.layer.11.intermediate.dense.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.2.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.11.attention.self.query.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.7.intermediate.dense.module.bias', 'bert.encoder.layer.8.attention.output.dense.module.weight_fake_quant.scale', 'bert.encoder.layer.2.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.2.attention.self.value.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.9.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.zero_point', 'bert.encoder.layer.8.output.dense.module.bias', 'bert.encoder.layer.6.output.dense.module.weight', 'bert.encoder.layer.10.intermediate.dense.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.5.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.9.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.5.attention.self.value.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.9.intermediate.dense.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.7.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.10.attention.self.value.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.7.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.7.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.11.attention.output.dense.quant.activation_post_process.zero_point', 'bert.encoder.layer.3.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.scale', 'bert.encoder.layer.11.attention.self.key.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.1.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.observer_enabled', 'bert.encoder.layer.6.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.5.attention.self.query.module.bias', 'bert.encoder.layer.1.attention.self.value.quant.activation_post_process.observer_enabled', 'bert.embeddings.word_embeddings.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.7.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.scale', 'bert.encoder.layer.8.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.10.attention.self.key.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.9.attention.self.key.quant.activation_post_process.zero_point', 'bert.encoder.layer.11.attention.output.dense.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.7.attention.self.value.quant.activation_post_process.zero_point', 'bert.encoder.layer.8.attention.output.dense.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.10.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.observer_enabled', 'bert.embeddings.token_type_embeddings.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.0.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.zero_point', 'bert.encoder.layer.1.attention.output.dense.quant.activation_post_process.scale', 'bert.encoder.layer.1.attention.output.dense.module.weight_fake_quant.zero_point', 'bert.encoder.layer.4.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.scale', 'bert.encoder.layer.8.output.dense.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.0.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.1.attention.self.query.module.weight_fake_quant.scale', 'bert.encoder.layer.1.intermediate.dense.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.9.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.9.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.2.intermediate.dense.module.weight_fake_quant.zero_point', 'bert.encoder.layer.4.attention.self.query.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.3.output.dense.quant.activation_post_process.zero_point', 'bert.encoder.layer.10.output.dense.module.weight_fake_quant.zero_point', 'bert.encoder.layer.9.attention.self.value.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.2.attention.self.value.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.6.attention.self.key.quant.activation_post_process.zero_point', 'bert.encoder.layer.5.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.scale', 'bert.encoder.layer.11.attention.self.query.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.10.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.eps', 'bert.embeddings.token_type_embeddings.activation_post_process.observer_enabled', 'bert.encoder.layer.9.intermediate.dense.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.3.attention.self.query.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.6.attention.self.value.quant.activation_post_process.scale', 'bert.encoder.layer.11.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.3.intermediate.dense.module.weight_fake_quant.fake_quant_enabled', 'bert.embeddings.position_embeddings.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.0.output.dense.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.5.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.zero_point', 'bert.encoder.layer.4.attention.self.query.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.10.attention.output.dense.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.2.intermediate.dense.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.1.attention.self.key.quant.activation_post_process.zero_point', 'bert.encoder.layer.1.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.fake_quant_enabled', 'bert.pooler.dense.quant.activation_post_process.observer_enabled', 'classifier.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.4.attention.self.query.quant.activation_post_process.zero_point', 'bert.encoder.layer.4.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.6.attention.output.dense.module.bias', 'bert.encoder.layer.1.attention.self.query.module.bias', 'bert.encoder.layer.3.intermediate.dense.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.2.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.1.attention.output.dense.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.6.output.dense.quant.activation_post_process.scale', 'bert.encoder.layer.1.output.dense.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.5.intermediate.dense.module.bias', 'bert.encoder.layer.5.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.3.intermediate.dense.module.weight', 'bert.encoder.layer.8.attention.self.query.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.3.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.scale', 'bert.encoder.layer.5.intermediate.dense.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.11.output.dense.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.4.attention.self.key.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.9.attention.self.query.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.5.intermediate.dense.quant.activation_post_process.scale', 'bert.encoder.layer.7.output.dense.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.6.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.5.attention.self.key.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.1.attention.self.value.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.5.attention.output.dense.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.11.output.dense.module.bias', 'bert.encoder.layer.3.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.scale', 'bert.encoder.layer.4.attention.self.key.module.weight_fake_quant.scale', 'bert.encoder.layer.1.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.4.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.8.attention.self.value.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.7.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.5.attention.self.value.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.9.attention.self.value.quant.activation_post_process.zero_point', 'bert.encoder.layer.10.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.2.attention.self.value.quant.activation_post_process.activation_post_process.eps', 'bert.pooler.dense.quant.activation_post_process.scale', 'bert.encoder.layer.9.attention.self.query.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.1.intermediate.dense.module.weight_fake_quant.scale', 'bert.encoder.layer.4.attention.output.dense.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.5.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.observer_enabled', 'bert.encoder.layer.1.intermediate.dense.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.6.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.3.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.fake_quant_enabled', 'bert.embeddings.position_embeddings.activation_post_process.zero_point', 'bert.encoder.layer.7.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.2.intermediate.dense.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.4.attention.self.query.module.weight', 'bert.encoder.layer.7.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.3.output.dense.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.0.attention.self.key.module.weight_fake_quant.zero_point', 'bert.encoder.layer.9.attention.self.key.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.3.attention.self.query.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.0.output.dense.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.5.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.zero_point', 'bert.encoder.layer.4.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.zero_point', 'bert.encoder.layer.6.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.9.intermediate.dense.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.9.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.0.intermediate.dense.quant.activation_post_process.zero_point', 'bert.encoder.layer.5.intermediate.dense.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.0.output.dense.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.5.intermediate.dense.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.10.output.dense.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.0.attention.self.query.module.weight', 'bert.pooler.dense.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.3.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.observer_enabled', 'bert.encoder.layer.3.attention.output.dense.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.9.attention.self.key.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.7.attention.self.key.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.6.attention.self.key.quant.activation_post_process.scale', 'bert.encoder.layer.3.attention.self.query.quant.activation_post_process.scale', 'bert.encoder.layer.4.attention.output.dense.quant.activation_post_process.zero_point', 'bert.encoder.layer.3.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.7.attention.self.key.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.5.attention.self.value.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.9.attention.self.value.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.0.attention.self.value.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.7.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.zero_point', 'bert.encoder.layer.2.intermediate.dense.module.weight', 'bert.encoder.layer.2.attention.self.key.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.7.attention.self.value.module.weight_fake_quant.zero_point', 'bert.encoder.layer.9.attention.self.key.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.9.output.dense.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.2.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.1.attention.self.key.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.8.attention.output.dense.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.10.output.dense.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.4.attention.self.query.module.weight_fake_quant.scale', 'bert.encoder.layer.8.attention.self.value.module.bias', 'bert.encoder.layer.10.attention.self.key.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.9.attention.self.value.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.0.intermediate.dense.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.4.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.4.attention.self.key.module.weight_fake_quant.activation_post_process.eps', 'bert.embeddings.position_embeddings.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.8.attention.self.key.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.2.attention.output.dense.quant.activation_post_process.scale', 'bert.encoder.layer.2.attention.self.value.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.5.attention.self.query.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.8.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.observer_enabled', 'bert.encoder.layer.5.attention.self.key.module.weight_fake_quant.zero_point', 'bert.encoder.layer.7.attention.self.key.module.weight_fake_quant.scale', 'bert.encoder.layer.10.attention.self.query.module.weight_fake_quant.scale', 'bert.encoder.layer.0.intermediate.dense.module.bias', 'bert.encoder.layer.10.intermediate.dense.module.weight_fake_quant.zero_point', 'bert.encoder.layer.1.intermediate.dense.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.6.intermediate.dense.quant.activation_post_process.scale', 'bert.encoder.layer.2.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.scale', 'bert.encoder.layer.8.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.observer_enabled', 'bert.encoder.layer.3.intermediate.dense.quant.activation_post_process.zero_point', 'bert.embeddings.word_embeddings.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.1.intermediate.dense.quant.activation_post_process.scale', 'bert.encoder.layer.9.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.zero_point', 'bert.encoder.layer.0.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.observer_enabled', 'bert.encoder.layer.10.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.2.attention.self.query.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.0.attention.output.dense.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.9.output.dense.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.9.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.5.attention.self.query.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.5.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.0.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.0.attention.self.value.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.5.attention.self.key.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.6.attention.self.key.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.6.output.dense.module.weight_fake_quant.zero_point', 'bert.encoder.layer.11.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.0.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.8.attention.self.value.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.8.output.dense.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.5.attention.self.query.quant.activation_post_process.scale', 'bert.encoder.layer.8.intermediate.dense.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.10.attention.self.key.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.8.attention.self.key.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.8.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.6.attention.self.query.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.2.attention.self.query.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.9.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.3.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.observer_enabled', 'bert.encoder.layer.5.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.zero_point', 'bert.encoder.layer.9.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.7.attention.self.key.module.weight', 'bert.encoder.layer.2.intermediate.dense.quant.activation_post_process.zero_point', 'bert.encoder.layer.11.attention.self.value.quant.activation_post_process.zero_point', 'bert.encoder.layer.8.intermediate.dense.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.1.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.zero_point', 'bert.encoder.layer.11.attention.self.key.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.6.output.dense.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.0.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.11.attention.self.key.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.1.output.dense.module.weight', 'bert.encoder.layer.8.intermediate.dense.module.weight_fake_quant.scale', 'bert.encoder.layer.4.output.dense.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.5.attention.self.value.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.10.attention.output.dense.module.weight', 'bert.encoder.layer.2.attention.self.value.module.bias', 'bert.encoder.layer.4.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.9.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.scale', 'bert.encoder.layer.1.attention.self.key.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.3.intermediate.dense.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.10.attention.output.dense.quant.activation_post_process.scale', 'bert.encoder.layer.8.attention.self.key.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.0.output.dense.module.weight', 'bert.encoder.layer.0.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.10.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'bert.embeddings.token_type_embeddings.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.9.attention.self.query.module.weight_fake_quant.zero_point', 'bert.encoder.layer.4.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.11.attention.self.value.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.8.attention.self.query.quant.activation_post_process.zero_point', 'bert.encoder.layer.1.attention.self.value.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.10.attention.self.query.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.11.output.dense.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.1.attention.self.key.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.9.attention.output.dense.module.weight_fake_quant.scale', 'bert.encoder.layer.0.attention.self.key.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.8.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.eps', 'bert.embeddings.word_embeddings.weight_fake_quant.zero_point', 'bert.encoder.layer.4.intermediate.dense.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.5.intermediate.dense.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.3.attention.self.query.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.7.attention.output.dense.module.bias', 'bert.encoder.layer.9.attention.self.value.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.1.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.zero_point', 'bert.encoder.layer.8.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.7.attention.output.dense.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.3.attention.self.key.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.6.attention.self.query.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.8.attention.self.value.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.5.attention.self.query.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.8.output.dense.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.1.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.zero_point', 'bert.encoder.layer.7.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.scale', 'bert.encoder.layer.7.attention.self.query.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.5.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.scale', 'bert.encoder.layer.2.output.dense.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.5.attention.self.value.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.2.output.dense.quant.activation_post_process.zero_point', 'bert.encoder.layer.11.attention.self.value.module.weight_fake_quant.scale', 'bert.encoder.layer.1.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.11.intermediate.dense.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.1.attention.self.key.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.4.intermediate.dense.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.5.output.dense.module.weight', 'bert.encoder.layer.5.attention.self.query.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.4.attention.self.value.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.3.attention.self.value.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.2.attention.output.dense.module.weight', 'bert.encoder.layer.10.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.0.intermediate.dense.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.1.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.observer_enabled', 'bert.encoder.layer.1.attention.self.query.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.4.intermediate.dense.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.7.attention.self.key.module.bias', 'bert.encoder.layer.10.attention.self.key.module.weight_fake_quant.zero_point', 'bert.encoder.layer.0.attention.output.dense.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.8.attention.self.key.module.weight_fake_quant.scale', 'bert.encoder.layer.1.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.observer_enabled', 'bert.encoder.layer.10.attention.output.dense.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.1.attention.self.query.quant.activation_post_process.scale', 'bert.encoder.layer.1.attention.output.dense.module.weight', 'bert.encoder.layer.3.attention.output.dense.module.weight_fake_quant.scale', 'bert.encoder.layer.8.attention.self.query.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.1.attention.self.value.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.7.attention.self.key.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.6.attention.self.key.quant.activation_post_process.activation_post_process.max_val', 'classifier.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.0.intermediate.dense.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.1.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.scale', 'bert.embeddings.word_embeddings.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.1.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.6.intermediate.dense.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.9.output.dense.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.8.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.9.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.1.attention.self.key.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.6.intermediate.dense.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.6.intermediate.dense.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.4.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.observer_enabled', 'bert.encoder.layer.3.output.dense.module.weight', 'bert.encoder.layer.5.output.dense.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.8.output.dense.quant.activation_post_process.zero_point', 'bert.encoder.layer.7.output.dense.module.weight_fake_quant.zero_point', 'classifier.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.0.attention.self.query.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.11.attention.output.dense.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.0.attention.output.dense.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.3.intermediate.dense.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.11.output.dense.quant.activation_post_process.zero_point', 'bert.encoder.layer.7.output.dense.module.weight', 'bert.embeddings.token_type_embeddings.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.3.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.observer_enabled', 'bert.encoder.layer.4.attention.self.value.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.3.output.dense.quant.activation_post_process.scale', 'bert.encoder.layer.7.intermediate.dense.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.7.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.7.attention.self.key.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.3.output.dense.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.11.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.zero_point', 'bert.encoder.layer.4.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.scale', 'bert.encoder.layer.4.output.dense.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.4.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.zero_point', 'bert.encoder.layer.7.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.0.attention.self.key.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.3.attention.self.query.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.0.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.1.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.0.attention.self.value.quant.activation_post_process.scale', 'bert.encoder.layer.3.attention.self.query.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.9.attention.self.query.module.bias', 'bert.encoder.layer.5.output.dense.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.8.intermediate.dense.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.1.attention.output.dense.module.bias', 'bert.encoder.layer.3.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.zero_point', 'bert.encoder.layer.11.output.dense.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.1.attention.self.query.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.3.attention.self.value.quant.activation_post_process.zero_point', 'bert.encoder.layer.8.attention.output.dense.module.weight_fake_quant.zero_point', 'bert.encoder.layer.11.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.scale', 'bert.encoder.layer.3.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.zero_point', 'bert.encoder.layer.0.attention.self.query.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.6.attention.self.value.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.1.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.4.attention.self.value.quant.activation_post_process.scale', 'bert.encoder.layer.5.output.dense.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.4.attention.self.query.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.6.intermediate.dense.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.7.intermediate.dense.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.4.output.dense.quant.activation_post_process.zero_point', 'bert.encoder.layer.3.attention.self.value.quant.activation_post_process.activation_post_process.max_val', 'bert.embeddings.word_embeddings.activation_post_process.observer_enabled', 'bert.encoder.layer.10.attention.self.query.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.11.attention.output.dense.module.weight_fake_quant.zero_point', 'bert.encoder.layer.7.attention.self.value.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.7.intermediate.dense.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.4.attention.self.query.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.3.attention.self.value.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.5.attention.self.key.module.weight_fake_quant.scale', 'bert.encoder.layer.1.attention.self.context_layer_matmul.output_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.3.attention.self.value.module.weight', 'bert.encoder.layer.0.attention.output.dense.module.weight_fake_quant.fake_quant_enabled', 'bert.encoder.layer.6.attention.self.value.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.7.attention.self.value.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.1.output.dense.module.bias', 'bert.encoder.layer.2.attention.self.query.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.4.attention.self.value.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.6.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.zero_point', 'bert.embeddings.token_type_embeddings.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.9.attention.output.dense.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.8.attention.output.dense.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.8.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.4.output.dense.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.2.attention.self.key.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.0.attention.self.query.module.weight_fake_quant.zero_point', 'bert.encoder.layer.0.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.zero_point', 'bert.encoder.layer.8.intermediate.dense.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.10.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.zero_point', 'bert.encoder.layer.1.attention.self.key.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.3.attention.self.value.module.weight_fake_quant.zero_point', 'bert.encoder.layer.4.attention.self.key.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.7.attention.self.key.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.4.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.2.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.observer_enabled', 'bert.encoder.layer.3.attention.self.query.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.7.attention.self.value.module.bias', 'bert.encoder.layer.7.intermediate.dense.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.1.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.zero_point', 'bert.encoder.layer.4.attention.self.query.module.weight_fake_quant.zero_point', 'bert.encoder.layer.1.attention.self.key.module.bias', 'bert.encoder.layer.0.attention.self.key.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.2.attention.self.key.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.4.attention.self.value.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.10.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.scale', 'bert.embeddings.token_type_embeddings.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.2.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.0.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.1.output.dense.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.4.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.2.attention.self.value.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.10.attention.self.value.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.9.attention.self.key.module.weight_fake_quant.activation_post_process.min_val', 'bert.embeddings.word_embeddings.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.2.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.5.attention.self.key.quant.activation_post_process.zero_point', 'bert.encoder.layer.5.attention.output.dense.module.bias', 'bert.encoder.layer.8.intermediate.dense.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.7.output.dense.quant.activation_post_process.scale', 'bert.encoder.layer.11.attention.self.key.module.bias', 'bert.encoder.layer.7.output.dense.module.weight_fake_quant.scale', 'bert.encoder.layer.2.output.dense.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.5.output.dense.quant.activation_post_process.zero_point', 'bert.encoder.layer.0.attention.output.dense.module.bias', 'bert.encoder.layer.3.attention.self.key.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.1.attention.self.key.module.weight', 'bert.encoder.layer.11.intermediate.dense.quant.activation_post_process.scale', 'bert.encoder.layer.4.output.dense.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.7.intermediate.dense.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.5.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.zero_point', 'bert.encoder.layer.2.attention.output.dense.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.0.attention.self.value.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.1.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.observer_enabled', 'bert.encoder.layer.11.attention.self.key.module.weight', 'bert.encoder.layer.8.intermediate.dense.module.weight', 'classifier.module.weight_fake_quant.activation_post_process.max_val', 'bert.encoder.layer.10.output.dense.module.bias', 'bert.encoder.layer.2.intermediate.dense.module.bias', 'bert.encoder.layer.10.attention.self.value.quant.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.4.attention.output.dense.module.bias', 'bert.encoder.layer.11.attention.output.dense.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.6.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.6.attention.self.key.module.bias', 'bert.encoder.layer.3.attention.self.value.module.bias', 'bert.encoder.layer.9.attention.output.dense.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.0.intermediate.dense.module.weight_fake_quant.scale', 'bert.encoder.layer.7.attention.self.value.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.0.attention.self.key.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.3.intermediate.dense.quant.activation_post_process.scale', 'bert.encoder.layer.7.attention.self.query.quant.activation_post_process.zero_point', 'bert.encoder.layer.2.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.observer_enabled', 'bert.encoder.layer.7.output.dense.module.bias', 'bert.encoder.layer.11.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.10.attention.self.attention_scores_matmul.input_quant_stubs.0.activation_post_process.scale', 'bert.encoder.layer.3.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.fake_quant_enabled', 'bert.encoder.layer.5.attention.self.key.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.6.attention.self.attention_scores_matmul.input_quant_stubs.1.activation_post_process.observer_enabled', 'bert.encoder.layer.7.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.11.intermediate.dense.quant.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.2.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.zero_point', 'bert.encoder.layer.0.attention.self.query.quant.activation_post_process.observer_enabled', 'bert.encoder.layer.6.attention.output.dense.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.1.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.zero_point', 'bert.encoder.layer.0.attention.self.attention_scores_matmul.output_quant_stubs.0.activation_post_process.scale', 'bert.encoder.layer.8.intermediate.dense.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.11.attention.output.dense.quant.activation_post_process.activation_post_process.max_val', 'bert.encoder.layer.6.attention.self.context_layer_matmul.input_quant_stubs.1.activation_post_process.activation_post_process.eps', 'bert.encoder.layer.1.output.dense.quant.activation_post_process.activation_post_process.min_val', 'bert.encoder.layer.5.attention.output.dense.module.weight_fake_quant.zero_point', 'bert.pooler.dense.module.weight_fake_quant.activation_post_process.min_val', 'bert.encoder.layer.5.attention.self.key.module.weight_fake_quant.activation_post_process.eps', 'bert.encoder.layer.7.output.dense.module.weight_fake_quant.observer_enabled', 'bert.encoder.layer.4.attention.self.context_layer_matmul.input_quant_stubs.0.activation_post_process.observer_enabled']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /opt/app-root/src/aicoe-osc-demo/models/obert_mnli_pruned90/training and are newly initialized: ['bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.5.attention.output.dense.bias', 'classifier.weight', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.pooler.dense.bias', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.pooler.dense.weight', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.3.attention.output.dense.bias', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.0.intermediate.dense.weight', 'classifier.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.1.attention.output.dense.bias', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.8.attention.output.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "157003d2187948c2b9edf36d6127227f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ea6787e177640beaa3e42dd763caf34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: question, sentence, __index_level_0__. If question, sentence, __index_level_0__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/opt/app-root/lib64/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2033\n",
      "  Num Epochs = 16\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2048\n",
      "  Number of trainable parameters = 109483778\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2048' max='2048' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2048/2048 07:41, Epoch 16/16]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.363000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.155700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.084600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.054100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: question, sentence, __index_level_0__. If question, sentence, __index_level_0__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 509\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: question, sentence, __index_level_0__. If question, sentence, __index_level_0__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 509\n",
      "  Batch size = 16\n",
      "/opt/app-root/lib64/python3.8/site-packages/sklearn/metrics/_classification.py:1580: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/opt/app-root/lib64/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/app-root/lib64/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to /opt/app-root/src/aicoe-osc-demo/models/transformers/obert_mnli_pruned90\n",
      "Configuration saved in /opt/app-root/src/aicoe-osc-demo/models/transformers/obert_mnli_pruned90/config.json\n",
      "Model weights saved in /opt/app-root/src/aicoe-osc-demo/models/transformers/obert_mnli_pruned90/pytorch_model.bin\n",
      "tokenizer config file saved in /opt/app-root/src/aicoe-osc-demo/models/transformers/obert_mnli_pruned90/tokenizer_config.json\n",
      "Special tokens file saved in /opt/app-root/src/aicoe-osc-demo/models/transformers/obert_mnli_pruned90/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "model_scores = []\n",
    "\n",
    "for (stub,path,model_name_s3) in zip(stubs, paths,model_name_4_s3):\n",
    "    sparse_model = Model(stub, path)\n",
    "    sparse_model.download()\n",
    "    training_path=path + '/training'\n",
    "    model_name = stub\n",
    "    tokenizer = AutoTokenizer.from_pretrained(training_path, use_fast=True)\n",
    "    sparse_model = AutoModelForSequenceClassification.from_pretrained(training_path, num_labels=num_labels)\n",
    "\n",
    "    metric = evaluate.load('f1')\n",
    "    encoded_climate_dataset = climate_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "    args = TrainingArguments(\n",
    "        f\"{model_name}-finetuned\",\n",
    "        evaluation_strategy = \"no\",\n",
    "        save_strategy = \"no\",\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        num_train_epochs=16, #16\n",
    "        weight_decay=0.01,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model='f1',\n",
    "        push_to_hub = False,\n",
    "        save_on_each_node=True\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        sparse_model,\n",
    "        args,\n",
    "        train_dataset=encoded_climate_dataset[\"train\"],\n",
    "        eval_dataset=None,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    train = trainer.train()\n",
    "\n",
    "    trainer.evaluate(eval_dataset=encoded_climate_dataset['test'])\n",
    "    pred = trainer.predict(encoded_climate_dataset['test'])\n",
    "    test_data[\"pred\"] = np.argmax(pred.predictions, axis=1)\n",
    "    groups = test_data.groupby(\"question\")\n",
    "    scores = {}\n",
    "\n",
    "    # kpi wise performance metrics\n",
    "    scores_df = scores_fn(groups)\n",
    "\n",
    "    model_scores.append(\n",
    "        {'Stub_Model': stub,\n",
    "         'F1 Score': scores_df.loc['f1_score'].mean(),\n",
    "         'train_runtime': train.metrics['train_runtime'],\n",
    "         'train_samples_per_second':train.metrics['train_samples_per_second'],\n",
    "         'train_steps_per_second':train.metrics['train_steps_per_second'],\n",
    "         'total_flos':train.metrics['total_flos'],\n",
    "         'train_loss':train.metrics['train_loss'],\n",
    "         'epoch':train.metrics['epoch'],\n",
    "         'test_runtime':pred.metrics['test_runtime'],\n",
    "         'test_samples_per_second':pred.metrics['test_samples_per_second'],\n",
    "         'test_steps_per_second':pred.metrics['test_steps_per_second']})\n",
    "    #### Saving Model Locally and in S3###\n",
    "    local_model_path = f'/opt/app-root/src/aicoe-osc-demo/models/transformers/{model_name_s3}'\n",
    "    save_model(local_model_path, model_name_s3)\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "40ed2cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_score = pd.DataFrame(model_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ee16be5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Stub_Model</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>train_runtime</th>\n",
       "      <th>train_samples_per_second</th>\n",
       "      <th>train_steps_per_second</th>\n",
       "      <th>total_flos</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>epoch</th>\n",
       "      <th>test_runtime</th>\n",
       "      <th>test_samples_per_second</th>\n",
       "      <th>test_steps_per_second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>zoo:nlp/text_classification/distilbert-none/py...</td>\n",
       "      <td>0.907176</td>\n",
       "      <td>239.9323</td>\n",
       "      <td>135.572</td>\n",
       "      <td>8.536</td>\n",
       "      <td>1.815870e+15</td>\n",
       "      <td>0.183508</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.2351</td>\n",
       "      <td>412.122</td>\n",
       "      <td>25.909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>zoo:nlp/text_classification/distilbert-none/py...</td>\n",
       "      <td>0.913952</td>\n",
       "      <td>240.2301</td>\n",
       "      <td>135.404</td>\n",
       "      <td>8.525</td>\n",
       "      <td>1.815870e+15</td>\n",
       "      <td>0.179335</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.2394</td>\n",
       "      <td>410.689</td>\n",
       "      <td>25.819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>zoo:nlp/text_classification/obert-base/pytorch...</td>\n",
       "      <td>0.895963</td>\n",
       "      <td>464.3408</td>\n",
       "      <td>70.052</td>\n",
       "      <td>4.411</td>\n",
       "      <td>3.606740e+15</td>\n",
       "      <td>0.161586</td>\n",
       "      <td>16.0</td>\n",
       "      <td>2.3217</td>\n",
       "      <td>219.234</td>\n",
       "      <td>13.783</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Stub_Model  F1 Score  train_runtime  \\\n",
       "0  zoo:nlp/text_classification/distilbert-none/py...  0.907176       239.9323   \n",
       "1  zoo:nlp/text_classification/distilbert-none/py...  0.913952       240.2301   \n",
       "2  zoo:nlp/text_classification/obert-base/pytorch...  0.895963       464.3408   \n",
       "\n",
       "   train_samples_per_second  train_steps_per_second    total_flos  train_loss  \\\n",
       "0                   135.572                   8.536  1.815870e+15    0.183508   \n",
       "1                   135.404                   8.525  1.815870e+15    0.179335   \n",
       "2                    70.052                   4.411  3.606740e+15    0.161586   \n",
       "\n",
       "   epoch  test_runtime  test_samples_per_second  test_steps_per_second  \n",
       "0   16.0        1.2351                  412.122                 25.909  \n",
       "1   16.0        1.2394                  410.689                 25.819  \n",
       "2   16.0        2.3217                  219.234                 13.783  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5dc367",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8da894d",
   "metadata": {},
   "source": [
    "In this notebook, we compared three different models from the sparsezoo. The corresponding metrics were displayed in the dataframe above."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
