{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f256d46-e4e1-4d3b-a2a1-acc516d0f307",
   "metadata": {
    "papermill": {
     "duration": 0.005744,
     "end_time": "2022-10-07T19:33:15.580568",
     "exception": false,
     "start_time": "2022-10-07T19:33:15.574824",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Performance benchmarks\n",
    "This notebook takes the test dataset for the relevance and kpi models and runs the evaluation. The output of this notebook are the f1 scores for relevance and kpi models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aec683f6-e875-48ab-8667-63710ceef385",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-07T19:33:15.590426Z",
     "iopub.status.busy": "2022-10-07T19:33:15.589875Z",
     "iopub.status.idle": "2022-10-07T19:33:18.883404Z",
     "shell.execute_reply": "2022-10-07T19:33:18.882735Z"
    },
    "papermill": {
     "duration": 3.30074,
     "end_time": "2022-10-07T19:33:18.885511",
     "exception": false,
     "start_time": "2022-10-07T19:33:15.584771",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/07/2022 19:33:17 - INFO - matplotlib.font_manager -   generated new fontManager\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/07/2022 19:33:18 - INFO - farm.modeling.prediction_head -   Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n"
     ]
    }
   ],
   "source": [
    "import config\n",
    "import os\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "from sklearn.metrics import recall_score, precision_score, f1_score, accuracy_score\n",
    "from src.data.s3_communication import S3Communication, S3FileType\n",
    "from farm.infer import Inferencer\n",
    "import zipfile\n",
    "from farm.infer import QAInferencer\n",
    "from farm.data_handler.utils import write_squad_predictions\n",
    "from farm.evaluation import squad_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e07d5a0-f9e5-4eba-ac28-6b5c4734b054",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-07T19:33:18.895577Z",
     "iopub.status.busy": "2022-10-07T19:33:18.895089Z",
     "iopub.status.idle": "2022-10-07T19:33:18.899599Z",
     "shell.execute_reply": "2022-10-07T19:33:18.899002Z"
    },
    "papermill": {
     "duration": 0.011639,
     "end_time": "2022-10-07T19:33:18.901426",
     "exception": false,
     "start_time": "2022-10-07T19:33:18.889787",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load credentials\n",
    "dotenv_dir = os.environ.get(\n",
    "    \"CREDENTIAL_DOTENV_DIR\", os.environ.get(\"PWD\", \"/opt/app-root/src\")\n",
    ")\n",
    "dotenv_path = pathlib.Path(dotenv_dir) / \"credentials.env\"\n",
    "if os.path.exists(dotenv_path):\n",
    "    load_dotenv(dotenv_path=dotenv_path, override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f19abb63-d82c-4159-bd3d-ef1901b79d6f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-07T19:33:18.911871Z",
     "iopub.status.busy": "2022-10-07T19:33:18.911418Z",
     "iopub.status.idle": "2022-10-07T19:33:18.995155Z",
     "shell.execute_reply": "2022-10-07T19:33:18.994529Z"
    },
    "papermill": {
     "duration": 0.090141,
     "end_time": "2022-10-07T19:33:18.997018",
     "exception": false,
     "start_time": "2022-10-07T19:33:18.906877",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# init s3 connector\n",
    "s3c = S3Communication(\n",
    "    s3_endpoint_url=os.getenv(\"S3_ENDPOINT\"),\n",
    "    aws_access_key_id=os.getenv(\"AWS_ACCESS_KEY_ID\"),\n",
    "    aws_secret_access_key=os.getenv(\"AWS_SECRET_ACCESS_KEY\"),\n",
    "    s3_bucket=os.getenv(\"S3_BUCKET\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e86b6e0-87ff-4255-8922-933642429d9a",
   "metadata": {
    "papermill": {
     "duration": 0.003808,
     "end_time": "2022-10-07T19:33:19.004776",
     "exception": false,
     "start_time": "2022-10-07T19:33:19.000968",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Retrieve the test dataset and the trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5dfca1c-10ee-4792-8060-5205a0c4fff9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-07T19:33:19.013665Z",
     "iopub.status.busy": "2022-10-07T19:33:19.013234Z",
     "iopub.status.idle": "2022-10-07T19:33:19.892773Z",
     "shell.execute_reply": "2022-10-07T19:33:19.892102Z"
    },
    "papermill": {
     "duration": 0.886377,
     "end_time": "2022-10-07T19:33:19.895045",
     "exception": false,
     "start_time": "2022-10-07T19:33:19.008668",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3c.download_files_in_prefix_to_dir(\n",
    "    config.BASE_TRAIN_TEST_DATASET_S3_PREFIX,\n",
    "    config.BASE_PROCESSED_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "755b111d-18d1-4bb6-b31f-ff269502ee2c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-07T19:33:19.905366Z",
     "iopub.status.busy": "2022-10-07T19:33:19.904908Z",
     "iopub.status.idle": "2022-10-07T19:33:24.322429Z",
     "shell.execute_reply": "2022-10-07T19:33:24.321681Z"
    },
    "papermill": {
     "duration": 4.424789,
     "end_time": "2022-10-07T19:33:24.324892",
     "exception": false,
     "start_time": "2022-10-07T19:33:19.900103",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_root = pathlib.Path(str(config.DATA_FOLDER)+\"/models\")\n",
    "if not os.path.exists(model_root):\n",
    "    model_root.mkdir(parents=True, exist_ok=True)\n",
    "model_rel_zip = pathlib.Path(model_root, 'RELEVANCE.zip')\n",
    "s3c.download_file_from_s3(model_rel_zip, config.CHECKPOINT_S3_PREFIX, \"RELEVANCE.zip\")\n",
    "\n",
    "with zipfile.ZipFile(pathlib.Path(model_root, 'RELEVANCE.zip'), 'r') as z:\n",
    "    z.extractall(model_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3585d7c-4f19-46e1-ae89-42a7569fbae9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-07T19:33:24.340067Z",
     "iopub.status.busy": "2022-10-07T19:33:24.339581Z",
     "iopub.status.idle": "2022-10-07T19:33:35.824459Z",
     "shell.execute_reply": "2022-10-07T19:33:35.823765Z"
    },
    "papermill": {
     "duration": 11.495384,
     "end_time": "2022-10-07T19:33:35.826720",
     "exception": false,
     "start_time": "2022-10-07T19:33:24.331336",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_root = pathlib.Path(str(config.DATA_FOLDER)+\"/models\")\n",
    "if not os.path.exists(model_root):\n",
    "    model_root.mkdir(parents=True, exist_ok=True)\n",
    "model_rel_zip = pathlib.Path(model_root, 'KPI_EXTRACTION.zip')\n",
    "s3c.download_file_from_s3(model_rel_zip, config.CHECKPOINT_S3_PREFIX, 'KPI_EXTRACTION.zip')\n",
    "\n",
    "with zipfile.ZipFile(pathlib.Path(model_root, 'KPI_EXTRACTION.zip'), 'r') as z:\n",
    "    z.extractall(model_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f160999f-7401-48b9-9b64-90c418fbbf7c",
   "metadata": {
    "papermill": {
     "duration": 0.00403,
     "end_time": "2022-10-07T19:33:35.835129",
     "exception": false,
     "start_time": "2022-10-07T19:33:35.831099",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Relevance performance scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80fa48b5-2779-4f00-aae2-86c951cd4246",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-07T19:33:35.844173Z",
     "iopub.status.busy": "2022-10-07T19:33:35.843620Z",
     "iopub.status.idle": "2022-10-07T19:33:35.862345Z",
     "shell.execute_reply": "2022-10-07T19:33:35.861773Z"
    },
    "papermill": {
     "duration": 0.025431,
     "end_time": "2022-10-07T19:33:35.864307",
     "exception": false,
     "start_time": "2022-10-07T19:33:35.838876",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>text_b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1531</th>\n",
       "      <td>1</td>\n",
       "      <td>What is the total amount of direct greenhouse ...</td>\n",
       "      <td>Without any regulatory mandates, our system's ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>1</td>\n",
       "      <td>In which year was the annual report or the sus...</td>\n",
       "      <td>BP Sustainability Report 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2083</th>\n",
       "      <td>0</td>\n",
       "      <td>What is the total volume of natural gas liquid...</td>\n",
       "      <td>It is true that coal produces a greater volume...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>648</th>\n",
       "      <td>0</td>\n",
       "      <td>What is the climate commitment scenario consid...</td>\n",
       "      <td>ability. Our new Sustainability Report, prepar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>744</th>\n",
       "      <td>0</td>\n",
       "      <td>What is the company name?</td>\n",
       "      <td>Each of our regulatory agreements can include ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                               text  \\\n",
       "1531      1  What is the total amount of direct greenhouse ...   \n",
       "110       1  In which year was the annual report or the sus...   \n",
       "2083      0  What is the total volume of natural gas liquid...   \n",
       "648       0  What is the climate commitment scenario consid...   \n",
       "744       0                          What is the company name?   \n",
       "\n",
       "                                                 text_b  \n",
       "1531  Without any regulatory mandates, our system's ...  \n",
       "110                       BP Sustainability Report 2018  \n",
       "2083  It is true that coal produces a greater volume...  \n",
       "648   ability. Our new Sustainability Report, prepar...  \n",
       "744   Each of our regulatory agreements can include ...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_path = str(config.BASE_PROCESSED_DATA)+'/rel_test_split.csv'\n",
    "test_data = pd.read_csv(test_data_path, index_col=0)\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1ece564-12ee-4e62-8bcd-0062f210d955",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-07T19:33:35.873896Z",
     "iopub.status.busy": "2022-10-07T19:33:35.873364Z",
     "iopub.status.idle": "2022-10-07T19:35:28.262412Z",
     "shell.execute_reply": "2022-10-07T19:35:28.261605Z"
    },
    "papermill": {
     "duration": 112.3959,
     "end_time": "2022-10-07T19:35:28.264379",
     "exception": false,
     "start_time": "2022-10-07T19:33:35.868479",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/07/2022 19:33:35 - INFO - farm.utils -   device: cpu n_gpu: 0, distributed training: False, automatic mixed precision training: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/07/2022 19:33:38 - INFO - farm.modeling.adaptive_model -   Found files for loading 1 prediction heads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/07/2022 19:33:38 - WARNING - farm.modeling.prediction_head -   `layer_dims` will be deprecated in future releases\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/07/2022 19:33:38 - INFO - farm.modeling.prediction_head -   Prediction head initialized with size [768, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/07/2022 19:33:38 - INFO - farm.modeling.prediction_head -   Loading prediction head from /opt/app-root/data/models/RELEVANCE/prediction_head_0.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/07/2022 19:33:39 - INFO - farm.modeling.tokenization -   Loading tokenizer of type 'RobertaTokenizer'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/07/2022 19:33:39 - INFO - farm.data_handler.processor -   Initialized processor without tasks. Supply `metric` and `label_list` to the constructor for using the default task or add a custom task later via processor.add_task()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/07/2022 19:33:39 - INFO - farm.utils -   device: cpu n_gpu: 0, distributed training: False, automatic mixed precision training: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/07/2022 19:33:39 - INFO - farm.infer -   Got ya 7 parallel workers to do inference ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/07/2022 19:33:39 - INFO - farm.infer -    0    0    0    0    0    0    0 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/07/2022 19:33:39 - INFO - farm.infer -   /w\\  /w\\  /w\\  /w\\  /w\\  /w\\  /w\\\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/07/2022 19:33:39 - INFO - farm.infer -   /'\\  / \\  /'\\  /'\\  / \\  / \\  /'\\\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/07/2022 19:33:39 - INFO - farm.infer -               \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/app-root/lib64/python3.8/site-packages/transformers/tokenization_utils.py:458: FutureWarning: `is_pretokenized` is deprecated and will be removed in a future version, use `is_split_into_words` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/07/2022 19:33:39 - INFO - farm.data_handler.processor -   *** Show 2 random examples ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/07/2022 19:33:39 - INFO - farm.data_handler.processor -   \n",
      "\n",
      "      .--.        _____                       _      \n",
      "    .'_\\/_'.     / ____|                     | |     \n",
      "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
      "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
      "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
      "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
      "   (/\\||/                             |_|           \n",
      "______\\||/___________________________________________                     \n",
      "\n",
      "ID: 14-0\n",
      "Clear Text: \n",
      " \ttext: In which year was the annual report or the sustainability report published?\n",
      " \ttext_b: * MOL divested 50% stake in OOO Zapadno-Malobalykskoye (holder of ZMB hydrocarbon license) and 100% stake in OOO MOL Wes- tern Siberia (owning Surgut-7 block) in August and September 2013. MOL has also finalized the deal of selling 49% share of BaiTex LLC (holder of the hydrocarbon licenses for Baituganskoye field) in early 2014.\n",
      " \ttext_classification_label: 0\n",
      "Tokenized: \n",
      " \ttokens: ['In', 'Ġwhich', 'Ġyear', 'Ġwas', 'Ġthe', 'Ġannual', 'Ġreport', 'Ġor', 'Ġthe', 'Ġsustainability', 'Ġreport', 'Ġpublished', '?']\n",
      " \ttokens_b: ['*', 'ĠM', 'OL', 'Ġdiv', 'ested', 'Ġ50', '%', 'Ġstake', 'Ġin', 'ĠO', 'OO', 'ĠZap', 'ad', 'no', '-', 'Mal', 'obal', 'y', 'ks', 'k', 'oy', 'e', 'Ġ(', 'holder', 'Ġof', 'ĠZ', 'MB', 'Ġhydro', 'carbon', 'Ġlicense', ')', 'Ġand', 'Ġ100', '%', 'Ġstake', 'Ġin', 'ĠO', 'OO', 'ĠM', 'OL', 'ĠWes', '-', 'Ġter', 'n', 'ĠSiberia', 'Ġ(', 'own', 'ing', 'ĠS', 'urg', 'ut', '-', '7', 'Ġblock', ')', 'Ġin', 'ĠAugust', 'Ġand', 'ĠSeptember', 'Ġ2013', '.', 'ĠM', 'OL', 'Ġhas', 'Ġalso', 'Ġfinalized', 'Ġthe', 'Ġdeal', 'Ġof', 'Ġselling', 'Ġ49', '%', 'Ġshare', 'Ġof', 'ĠBai', 'Tex', 'ĠLLC', 'Ġ(', 'holder', 'Ġof', 'Ġthe', 'Ġhydro', 'carbon', 'Ġlicenses', 'Ġfor', 'ĠB', 'ait', 'ug', 'ansk', 'oy', 'e', 'Ġfield', ')', 'Ġin', 'Ġearly', 'Ġ2014', '.']\n",
      "Features: \n",
      " \tinput_ids: [0, 1121, 61, 76, 21, 5, 1013, 266, 50, 5, 11128, 266, 1027, 116, 2, 2, 3226, 256, 3384, 14445, 11718, 654, 207, 1968, 11, 384, 9332, 31308, 625, 2362, 12, 18764, 37005, 219, 2258, 330, 2160, 242, 36, 14074, 9, 525, 8651, 13575, 23612, 4385, 43, 8, 727, 207, 1968, 11, 384, 9332, 256, 3384, 15590, 12, 8470, 282, 36554, 36, 3355, 154, 208, 7150, 1182, 12, 406, 1803, 43, 11, 830, 8, 772, 1014, 4, 256, 3384, 34, 67, 16449, 5, 432, 9, 2183, 2766, 207, 458, 9, 33155, 40175, 2291, 36, 14074, 9, 5, 13575, 23612, 11150, 13, 163, 5236, 3252, 30075, 2160, 242, 882, 43, 11, 419, 777, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      " \tpadding_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \tsegment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \ttext_classification_label_ids: [0]\n",
      "_____________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/07/2022 19:33:39 - INFO - farm.data_handler.processor -   \n",
      "\n",
      "      .--.        _____                       _      \n",
      "    .'_\\/_'.     / ____|                     | |     \n",
      "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
      "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
      "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
      "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
      "   (/\\||/                             |_|           \n",
      "______\\||/___________________________________________                     \n",
      "\n",
      "ID: 12-0\n",
      "Clear Text: \n",
      " \ttext: What is the total volume of hydrocarbons production?\n",
      " \ttext_b: Net profit attributable to equity holders of the parent \n",
      " \ttext_classification_label: 0\n",
      "Tokenized: \n",
      " \ttokens: ['What', 'Ġis', 'Ġthe', 'Ġtotal', 'Ġvolume', 'Ġof', 'Ġhydro', 'car', 'bons', 'Ġproduction', '?']\n",
      " \ttokens_b: ['Net', 'Ġprofit', 'Ġattributable', 'Ġto', 'Ġequity', 'Ġholders', 'Ġof', 'Ġthe', 'Ġparent']\n",
      "Features: \n",
      " \tinput_ids: [0, 2264, 16, 5, 746, 3149, 9, 13575, 5901, 16830, 931, 116, 2, 2, 15721, 1963, 18297, 7, 2355, 9758, 9, 5, 4095, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      " \tpadding_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \tsegment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \ttext_classification_label_ids: [0]\n",
      "_____________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/app-root/lib64/python3.8/site-packages/transformers/tokenization_utils.py:458: FutureWarning: `is_pretokenized` is deprecated and will be removed in a future version, use `is_split_into_words` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/app-root/lib64/python3.8/site-packages/transformers/tokenization_utils.py:458: FutureWarning: `is_pretokenized` is deprecated and will be removed in a future version, use `is_split_into_words` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:   0%|          | 0/4 [00:00<?, ? Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/app-root/lib64/python3.8/site-packages/transformers/tokenization_utils.py:458: FutureWarning: `is_pretokenized` is deprecated and will be removed in a future version, use `is_split_into_words` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/app-root/lib64/python3.8/site-packages/transformers/tokenization_utils.py:458: FutureWarning: `is_pretokenized` is deprecated and will be removed in a future version, use `is_split_into_words` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/app-root/lib64/python3.8/site-packages/transformers/tokenization_utils.py:458: FutureWarning: `is_pretokenized` is deprecated and will be removed in a future version, use `is_split_into_words` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/app-root/lib64/python3.8/site-packages/transformers/tokenization_utils.py:458: FutureWarning: `is_pretokenized` is deprecated and will be removed in a future version, use `is_split_into_words` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  25%|██▌       | 1/4 [00:02<00:08,  2.79s/ Batches]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  50%|█████     | 2/4 [00:03<00:03,  1.74s/ Batches]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  75%|███████▌  | 3/4 [00:04<00:01,  1.31s/ Batches]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples: 100%|██████████| 4/4 [00:05<00:00,  1.08s/ Batches]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples: 100%|██████████| 4/4 [00:05<00:00,  1.33s/ Batches]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:   0%|          | 0/4 [00:00<?, ? Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  25%|██▌       | 1/4 [00:00<00:02,  1.24 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  50%|█████     | 2/4 [00:01<00:01,  1.27 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  75%|███████▌  | 3/4 [00:02<00:00,  1.27 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples: 100%|██████████| 4/4 [00:02<00:00,  1.40 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples: 100%|██████████| 4/4 [00:02<00:00,  1.34 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:   0%|          | 0/4 [00:00<?, ? Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  25%|██▌       | 1/4 [00:00<00:02,  1.06 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  50%|█████     | 2/4 [00:01<00:01,  1.15 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  75%|███████▌  | 3/4 [00:02<00:00,  1.18 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples: 100%|██████████| 4/4 [00:03<00:00,  1.31 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples: 100%|██████████| 4/4 [00:03<00:00,  1.24 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:   0%|          | 0/4 [00:00<?, ? Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  25%|██▌       | 1/4 [00:00<00:02,  1.21 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  50%|█████     | 2/4 [00:01<00:01,  1.16 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  75%|███████▌  | 3/4 [00:02<00:00,  1.20 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples: 100%|██████████| 4/4 [00:03<00:00,  1.36 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples: 100%|██████████| 4/4 [00:03<00:00,  1.29 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:   0%|          | 0/4 [00:00<?, ? Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  25%|██▌       | 1/4 [00:00<00:02,  1.00 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  50%|█████     | 2/4 [00:01<00:01,  1.10 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  75%|███████▌  | 3/4 [00:02<00:00,  1.17 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples: 100%|██████████| 4/4 [00:03<00:00,  1.33 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples: 100%|██████████| 4/4 [00:03<00:00,  1.24 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:   0%|          | 0/4 [00:00<?, ? Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  25%|██▌       | 1/4 [00:00<00:02,  1.22 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  50%|█████     | 2/4 [00:01<00:01,  1.26 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  75%|███████▌  | 3/4 [00:02<00:00,  1.19 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples: 100%|██████████| 4/4 [00:03<00:00,  1.34 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples: 100%|██████████| 4/4 [00:03<00:00,  1.29 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:   0%|          | 0/4 [00:00<?, ? Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  25%|██▌       | 1/4 [00:00<00:02,  1.26 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  50%|█████     | 2/4 [00:01<00:01,  1.28 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  75%|███████▌  | 3/4 [00:02<00:00,  1.26 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples: 100%|██████████| 4/4 [00:02<00:00,  1.38 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples: 100%|██████████| 4/4 [00:02<00:00,  1.34 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:   0%|          | 0/4 [00:00<?, ? Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  25%|██▌       | 1/4 [00:00<00:02,  1.20 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  50%|█████     | 2/4 [00:01<00:01,  1.18 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  75%|███████▌  | 3/4 [00:02<00:00,  1.16 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples: 100%|██████████| 4/4 [00:03<00:00,  1.30 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples: 100%|██████████| 4/4 [00:03<00:00,  1.25 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:   0%|          | 0/4 [00:00<?, ? Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  25%|██▌       | 1/4 [00:00<00:02,  1.23 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  50%|█████     | 2/4 [00:01<00:01,  1.08 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  75%|███████▌  | 3/4 [00:02<00:00,  1.07 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples: 100%|██████████| 4/4 [00:03<00:00,  1.22 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples: 100%|██████████| 4/4 [00:03<00:00,  1.18 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:   0%|          | 0/4 [00:00<?, ? Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  25%|██▌       | 1/4 [00:00<00:02,  1.25 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  50%|█████     | 2/4 [00:01<00:01,  1.26 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  75%|███████▌  | 3/4 [00:02<00:00,  1.26 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples: 100%|██████████| 4/4 [00:02<00:00,  1.40 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples: 100%|██████████| 4/4 [00:02<00:00,  1.35 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:   0%|          | 0/4 [00:00<?, ? Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  25%|██▌       | 1/4 [00:00<00:02,  1.18 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  50%|█████     | 2/4 [00:01<00:01,  1.18 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  75%|███████▌  | 3/4 [00:02<00:00,  1.21 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples: 100%|██████████| 4/4 [00:03<00:00,  1.37 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples: 100%|██████████| 4/4 [00:03<00:00,  1.30 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:   0%|          | 0/4 [00:00<?, ? Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  25%|██▌       | 1/4 [00:00<00:02,  1.30 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  50%|█████     | 2/4 [00:01<00:01,  1.24 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  75%|███████▌  | 3/4 [00:02<00:00,  1.13 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples: 100%|██████████| 4/4 [00:03<00:00,  1.27 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples: 100%|██████████| 4/4 [00:03<00:00,  1.24 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:   0%|          | 0/4 [00:00<?, ? Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  25%|██▌       | 1/4 [00:00<00:02,  1.18 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  50%|█████     | 2/4 [00:01<00:01,  1.24 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  75%|███████▌  | 3/4 [00:02<00:00,  1.25 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples: 100%|██████████| 4/4 [00:03<00:00,  1.37 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples: 100%|██████████| 4/4 [00:03<00:00,  1.31 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:   0%|          | 0/4 [00:00<?, ? Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  25%|██▌       | 1/4 [00:00<00:02,  1.25 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  50%|█████     | 2/4 [00:01<00:01,  1.25 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  75%|███████▌  | 3/4 [00:02<00:00,  1.21 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples: 100%|██████████| 4/4 [00:03<00:00,  1.29 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples: 100%|██████████| 4/4 [00:03<00:00,  1.27 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:   0%|          | 0/4 [00:00<?, ? Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  25%|██▌       | 1/4 [00:00<00:02,  1.24 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  50%|█████     | 2/4 [00:01<00:01,  1.22 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  75%|███████▌  | 3/4 [00:02<00:00,  1.25 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples: 100%|██████████| 4/4 [00:03<00:00,  1.38 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples: 100%|██████████| 4/4 [00:03<00:00,  1.32 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:   0%|          | 0/4 [00:00<?, ? Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  25%|██▌       | 1/4 [00:00<00:02,  1.17 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  50%|█████     | 2/4 [00:01<00:01,  1.19 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  75%|███████▌  | 3/4 [00:02<00:00,  1.23 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples: 100%|██████████| 4/4 [00:03<00:00,  1.38 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples: 100%|██████████| 4/4 [00:03<00:00,  1.31 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:   0%|          | 0/4 [00:00<?, ? Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  25%|██▌       | 1/4 [00:00<00:02,  1.24 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  50%|█████     | 2/4 [00:01<00:01,  1.26 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  75%|███████▌  | 3/4 [00:02<00:00,  1.25 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples: 100%|██████████| 4/4 [00:03<00:00,  1.34 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples: 100%|██████████| 4/4 [00:03<00:00,  1.31 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:   0%|          | 0/4 [00:00<?, ? Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  25%|██▌       | 1/4 [00:00<00:02,  1.13 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  50%|█████     | 2/4 [00:01<00:01,  1.19 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  75%|███████▌  | 3/4 [00:02<00:00,  1.21 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples: 100%|██████████| 4/4 [00:03<00:00,  1.26 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples: 100%|██████████| 4/4 [00:03<00:00,  1.23 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:   0%|          | 0/4 [00:00<?, ? Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  25%|██▌       | 1/4 [00:01<00:03,  1.01s/ Batches]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  50%|█████     | 2/4 [00:01<00:01,  1.03 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  75%|███████▌  | 3/4 [00:02<00:00,  1.09 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples: 100%|██████████| 4/4 [00:03<00:00,  1.26 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples: 100%|██████████| 4/4 [00:03<00:00,  1.17 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:   0%|          | 0/4 [00:00<?, ? Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  25%|██▌       | 1/4 [00:00<00:02,  1.17 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  50%|█████     | 2/4 [00:01<00:01,  1.20 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  75%|███████▌  | 3/4 [00:02<00:00,  1.15 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples: 100%|██████████| 4/4 [00:03<00:00,  1.26 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples: 100%|██████████| 4/4 [00:03<00:00,  1.22 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:   0%|          | 0/4 [00:00<?, ? Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  25%|██▌       | 1/4 [00:00<00:02,  1.23 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  50%|█████     | 2/4 [00:01<00:01,  1.23 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  75%|███████▌  | 3/4 [00:02<00:00,  1.27 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples: 100%|██████████| 4/4 [00:02<00:00,  1.39 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples: 100%|██████████| 4/4 [00:02<00:00,  1.33 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:   0%|          | 0/4 [00:00<?, ? Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  25%|██▌       | 1/4 [00:01<00:03,  1.01s/ Batches]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  50%|█████     | 2/4 [00:01<00:01,  1.13 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  75%|███████▌  | 3/4 [00:02<00:00,  1.18 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples: 100%|██████████| 4/4 [00:03<00:00,  1.33 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples: 100%|██████████| 4/4 [00:03<00:00,  1.24 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:   0%|          | 0/4 [00:00<?, ? Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  25%|██▌       | 1/4 [00:00<00:02,  1.20 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  50%|█████     | 2/4 [00:01<00:01,  1.24 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  75%|███████▌  | 3/4 [00:02<00:00,  1.26 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples: 100%|██████████| 4/4 [00:02<00:00,  1.40 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples: 100%|██████████| 4/4 [00:02<00:00,  1.34 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:   0%|          | 0/4 [00:00<?, ? Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  25%|██▌       | 1/4 [00:00<00:02,  1.09 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  50%|█████     | 2/4 [00:01<00:01,  1.14 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  75%|███████▌  | 3/4 [00:02<00:00,  1.20 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples: 100%|██████████| 4/4 [00:03<00:00,  1.35 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples: 100%|██████████| 4/4 [00:03<00:00,  1.27 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:   0%|          | 0/4 [00:00<?, ? Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  25%|██▌       | 1/4 [00:00<00:02,  1.27 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  50%|█████     | 2/4 [00:01<00:01,  1.28 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  75%|███████▌  | 3/4 [00:02<00:00,  1.25 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples: 100%|██████████| 4/4 [00:03<00:00,  1.35 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples: 100%|██████████| 4/4 [00:03<00:00,  1.32 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:   0%|          | 0/4 [00:00<?, ? Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  25%|██▌       | 1/4 [00:00<00:02,  1.25 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  50%|█████     | 2/4 [00:01<00:01,  1.27 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  75%|███████▌  | 3/4 [00:02<00:00,  1.26 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples: 100%|██████████| 4/4 [00:02<00:00,  1.39 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples: 100%|██████████| 4/4 [00:02<00:00,  1.34 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:   0%|          | 0/4 [00:00<?, ? Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  25%|██▌       | 1/4 [00:00<00:02,  1.28 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  50%|█████     | 2/4 [00:01<00:01,  1.18 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  75%|███████▌  | 3/4 [00:02<00:00,  1.19 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples: 100%|██████████| 4/4 [00:03<00:00,  1.30 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples: 100%|██████████| 4/4 [00:03<00:00,  1.26 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:   0%|          | 0/4 [00:00<?, ? Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  25%|██▌       | 1/4 [00:00<00:02,  1.28 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  50%|█████     | 2/4 [00:01<00:01,  1.19 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  75%|███████▌  | 3/4 [00:02<00:00,  1.12 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples: 100%|██████████| 4/4 [00:03<00:00,  1.26 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples: 100%|██████████| 4/4 [00:03<00:00,  1.23 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:   0%|          | 0/4 [00:00<?, ? Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  25%|██▌       | 1/4 [00:00<00:02,  1.19 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  50%|█████     | 2/4 [00:01<00:01,  1.24 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  75%|███████▌  | 3/4 [00:02<00:00,  1.21 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples: 100%|██████████| 4/4 [00:03<00:00,  1.37 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples: 100%|██████████| 4/4 [00:03<00:00,  1.31 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:   0%|          | 0/4 [00:00<?, ? Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  25%|██▌       | 1/4 [00:00<00:02,  1.30 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  50%|█████     | 2/4 [00:01<00:01,  1.23 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  75%|███████▌  | 3/4 [00:02<00:00,  1.21 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples: 100%|██████████| 4/4 [00:03<00:00,  1.31 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples: 100%|██████████| 4/4 [00:03<00:00,  1.28 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:   0%|          | 0/4 [00:00<?, ? Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  25%|██▌       | 1/4 [00:00<00:02,  1.17 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  50%|█████     | 2/4 [00:01<00:01,  1.16 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  75%|███████▌  | 3/4 [00:02<00:00,  1.18 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples: 100%|██████████| 4/4 [00:03<00:00,  1.20 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples: 100%|██████████| 4/4 [00:03<00:00,  1.19 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:   0%|          | 0/4 [00:00<?, ? Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  25%|██▌       | 1/4 [00:00<00:02,  1.12 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  50%|█████     | 2/4 [00:01<00:01,  1.17 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  75%|███████▌  | 3/4 [00:02<00:00,  1.22 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples: 100%|██████████| 4/4 [00:03<00:00,  1.36 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples: 100%|██████████| 4/4 [00:03<00:00,  1.29 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:   0%|          | 0/4 [00:00<?, ? Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  25%|██▌       | 1/4 [00:00<00:02,  1.29 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  50%|█████     | 2/4 [00:01<00:01,  1.26 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  75%|███████▌  | 3/4 [00:02<00:00,  1.22 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples: 100%|██████████| 4/4 [00:03<00:00,  1.33 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples: 100%|██████████| 4/4 [00:03<00:00,  1.29 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:   0%|          | 0/4 [00:00<?, ? Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  25%|██▌       | 1/4 [00:00<00:02,  1.19 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  50%|█████     | 2/4 [00:01<00:01,  1.23 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  75%|███████▌  | 3/4 [00:02<00:00,  1.24 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples: 100%|██████████| 4/4 [00:02<00:00,  1.54 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples: 100%|██████████| 4/4 [00:02<00:00,  1.41 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# get predictions from current model\n",
    "model = Inferencer.load(str(model_root)+'/RELEVANCE')\n",
    "\n",
    "result = model.inference_from_file(test_data_path)\n",
    "results = [d for r in result for d in r[\"predictions\"]]\n",
    "preds = [int(r[\"label\"]) for r in results]\n",
    "\n",
    "test_data[\"pred\"] = preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "354533f5-5269-41df-808e-678b9551b546",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-07T19:35:28.299168Z",
     "iopub.status.busy": "2022-10-07T19:35:28.298825Z",
     "iopub.status.idle": "2022-10-07T19:35:28.381422Z",
     "shell.execute_reply": "2022-10-07T19:35:28.380779Z"
    },
    "papermill": {
     "duration": 0.102067,
     "end_time": "2022-10-07T19:35:28.383256",
     "exception": false,
     "start_time": "2022-10-07T19:35:28.281189",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/app-root/lib64/python3.8/site-packages/sklearn/metrics/_classification.py:1580: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/opt/app-root/lib64/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/app-root/lib64/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#evalute performance\n",
    "groups = test_data.groupby(\"text\")\n",
    "scores = {}\n",
    "for group, data in groups:\n",
    "    pred = data.pred\n",
    "    true = data.label\n",
    "    scores[group] = {}\n",
    "    scores[group][\"accuracy\"] = accuracy_score(true, pred)\n",
    "    scores[group][\"f1_score\"] = f1_score(true, pred)\n",
    "    scores[group][\"recall_score\"] = recall_score(true, pred)\n",
    "    scores[group][\"precision_score\"] = precision_score(true, pred)\n",
    "    scores[group][\"support\"] = len(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da688f37-f9c6-4eee-bedd-9b0db250fe5a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-07T19:35:28.416787Z",
     "iopub.status.busy": "2022-10-07T19:35:28.416203Z",
     "iopub.status.idle": "2022-10-07T19:35:28.446028Z",
     "shell.execute_reply": "2022-10-07T19:35:28.445323Z"
    },
    "papermill": {
     "duration": 0.048318,
     "end_time": "2022-10-07T19:35:28.447999",
     "exception": false,
     "start_time": "2022-10-07T19:35:28.399681",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>In which year was the annual report or the sustainability report published?</th>\n",
       "      <th>What is the annual total production from coal?</th>\n",
       "      <th>What is the base year for carbon reduction commitment?</th>\n",
       "      <th>What is the climate commitment scenario considered?</th>\n",
       "      <th>What is the company name?</th>\n",
       "      <th>What is the target carbon reduction in percentage?</th>\n",
       "      <th>What is the target year for climate commitment?</th>\n",
       "      <th>What is the total amount of direct greenhouse gases emissions referred to as scope 1 emissions?</th>\n",
       "      <th>What is the total amount of energy indirect greenhouse gases emissions referred to as scope 2 emissions?</th>\n",
       "      <th>What is the total amount of scope 1 and 2 greenhouse gases emissions?</th>\n",
       "      <th>...</th>\n",
       "      <th>What is the total amount of upstream energy indirect greenhouse gases emissions referred to as scope 3 emissions?</th>\n",
       "      <th>What is the total installed capacity from coal?</th>\n",
       "      <th>What is the total installed capacity from lignite (brown coal)?</th>\n",
       "      <th>What is the total volume of crude oil liquid production?</th>\n",
       "      <th>What is the total volume of hydrocarbons production?</th>\n",
       "      <th>What is the total volume of natural gas liquid production?</th>\n",
       "      <th>What is the total volume of natural gas production?</th>\n",
       "      <th>What is the total volume of proven and probable hydrocarbons reserves?</th>\n",
       "      <th>What is the volume of estimated probable hydrocarbons reserves?</th>\n",
       "      <th>What is the volume of estimated proven hydrocarbons reserves?</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.891304</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.924528</td>\n",
       "      <td>0.970588</td>\n",
       "      <td>0.978261</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.957143</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1_score</th>\n",
       "      <td>0.871795</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.975610</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.949153</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall_score</th>\n",
       "      <td>0.871795</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision_score</th>\n",
       "      <td>0.871795</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.903226</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>support</th>\n",
       "      <td>92.000000</td>\n",
       "      <td>6.0</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>53.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>18.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>15.0</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>47.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 In which year was the annual report or the sustainability report published?  \\\n",
       "accuracy                                                  0.891304                             \n",
       "f1_score                                                  0.871795                             \n",
       "recall_score                                              0.871795                             \n",
       "precision_score                                           0.871795                             \n",
       "support                                                  92.000000                             \n",
       "\n",
       "                 What is the annual total production from coal?  \\\n",
       "accuracy                                                    1.0   \n",
       "f1_score                                                    1.0   \n",
       "recall_score                                                1.0   \n",
       "precision_score                                             1.0   \n",
       "support                                                     6.0   \n",
       "\n",
       "                 What is the base year for carbon reduction commitment?  \\\n",
       "accuracy                                                  0.960000        \n",
       "f1_score                                                  0.960000        \n",
       "recall_score                                              1.000000        \n",
       "precision_score                                           0.923077        \n",
       "support                                                  25.000000        \n",
       "\n",
       "                 What is the climate commitment scenario considered?  \\\n",
       "accuracy                                                  0.960000     \n",
       "f1_score                                                  0.952381     \n",
       "recall_score                                              0.909091     \n",
       "precision_score                                           1.000000     \n",
       "support                                                  25.000000     \n",
       "\n",
       "                 What is the company name?  \\\n",
       "accuracy                          0.924528   \n",
       "f1_score                          0.900000   \n",
       "recall_score                      0.900000   \n",
       "precision_score                   0.900000   \n",
       "support                          53.000000   \n",
       "\n",
       "                 What is the target carbon reduction in percentage?  \\\n",
       "accuracy                                                  0.970588    \n",
       "f1_score                                                  0.960000    \n",
       "recall_score                                              1.000000    \n",
       "precision_score                                           0.923077    \n",
       "support                                                  34.000000    \n",
       "\n",
       "                 What is the target year for climate commitment?  \\\n",
       "accuracy                                                0.978261   \n",
       "f1_score                                                0.975610   \n",
       "recall_score                                            1.000000   \n",
       "precision_score                                         0.952381   \n",
       "support                                                46.000000   \n",
       "\n",
       "                 What is the total amount of direct greenhouse gases emissions referred to as scope 1 emissions?  \\\n",
       "accuracy                                                       1.0                                                 \n",
       "f1_score                                                       1.0                                                 \n",
       "recall_score                                                   1.0                                                 \n",
       "precision_score                                                1.0                                                 \n",
       "support                                                       18.0                                                 \n",
       "\n",
       "                 What is the total amount of energy indirect greenhouse gases emissions referred to as scope 2 emissions?  \\\n",
       "accuracy                                                       1.0                                                          \n",
       "f1_score                                                       1.0                                                          \n",
       "recall_score                                                   1.0                                                          \n",
       "precision_score                                                1.0                                                          \n",
       "support                                                       13.0                                                          \n",
       "\n",
       "                 What is the total amount of scope 1 and 2 greenhouse gases emissions?  \\\n",
       "accuracy                                                       1.0                       \n",
       "f1_score                                                       1.0                       \n",
       "recall_score                                                   1.0                       \n",
       "precision_score                                                1.0                       \n",
       "support                                                        2.0                       \n",
       "\n",
       "                 ...  \\\n",
       "accuracy         ...   \n",
       "f1_score         ...   \n",
       "recall_score     ...   \n",
       "precision_score  ...   \n",
       "support          ...   \n",
       "\n",
       "                 What is the total amount of upstream energy indirect greenhouse gases emissions referred to as scope 3 emissions?  \\\n",
       "accuracy                                                       1.0                                                                   \n",
       "f1_score                                                       1.0                                                                   \n",
       "recall_score                                                   1.0                                                                   \n",
       "precision_score                                                1.0                                                                   \n",
       "support                                                       15.0                                                                   \n",
       "\n",
       "                 What is the total installed capacity from coal?  \\\n",
       "accuracy                                                0.750000   \n",
       "f1_score                                                0.800000   \n",
       "recall_score                                            0.666667   \n",
       "precision_score                                         1.000000   \n",
       "support                                                 4.000000   \n",
       "\n",
       "                 What is the total installed capacity from lignite (brown coal)?  \\\n",
       "accuracy                                                       1.0                 \n",
       "f1_score                                                       0.0                 \n",
       "recall_score                                                   0.0                 \n",
       "precision_score                                                0.0                 \n",
       "support                                                        1.0                 \n",
       "\n",
       "                 What is the total volume of crude oil liquid production?  \\\n",
       "accuracy                                                       1.0          \n",
       "f1_score                                                       1.0          \n",
       "recall_score                                                   1.0          \n",
       "precision_score                                                1.0          \n",
       "support                                                        3.0          \n",
       "\n",
       "                 What is the total volume of hydrocarbons production?  \\\n",
       "accuracy                                                  0.957143      \n",
       "f1_score                                                  0.949153      \n",
       "recall_score                                              1.000000      \n",
       "precision_score                                           0.903226      \n",
       "support                                                  70.000000      \n",
       "\n",
       "                 What is the total volume of natural gas liquid production?  \\\n",
       "accuracy                                                       1.0            \n",
       "f1_score                                                       1.0            \n",
       "recall_score                                                   1.0            \n",
       "precision_score                                                1.0            \n",
       "support                                                        4.0            \n",
       "\n",
       "                 What is the total volume of natural gas production?  \\\n",
       "accuracy                                                  0.937500     \n",
       "f1_score                                                  0.888889     \n",
       "recall_score                                              0.800000     \n",
       "precision_score                                           1.000000     \n",
       "support                                                  16.000000     \n",
       "\n",
       "                 What is the total volume of proven and probable hydrocarbons reserves?  \\\n",
       "accuracy                                                  0.969697                        \n",
       "f1_score                                                  0.969697                        \n",
       "recall_score                                              0.941176                        \n",
       "precision_score                                           1.000000                        \n",
       "support                                                  33.000000                        \n",
       "\n",
       "                 What is the volume of estimated probable hydrocarbons reserves?  \\\n",
       "accuracy                                                       1.0                 \n",
       "f1_score                                                       1.0                 \n",
       "recall_score                                                   1.0                 \n",
       "precision_score                                                1.0                 \n",
       "support                                                        1.0                 \n",
       "\n",
       "                 What is the volume of estimated proven hydrocarbons reserves?  \n",
       "accuracy                                                       1.0              \n",
       "f1_score                                                       1.0              \n",
       "recall_score                                                   1.0              \n",
       "precision_score                                                1.0              \n",
       "support                                                       47.0              \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# kpi wise performance metrics\n",
    "scores_df = pd.DataFrame(scores)\n",
    "scores_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5a5cfcb-56f2-4769-a17d-adfcdbc6cd83",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-07T19:35:28.482252Z",
     "iopub.status.busy": "2022-10-07T19:35:28.481997Z",
     "iopub.status.idle": "2022-10-07T19:35:28.488309Z",
     "shell.execute_reply": "2022-10-07T19:35:28.487649Z"
    },
    "papermill": {
     "duration": 0.025569,
     "end_time": "2022-10-07T19:35:28.490222",
     "exception": false,
     "start_time": "2022-10-07T19:35:28.464653",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9155963800586726"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_df.loc['f1_score'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa1dbaa-8e2b-46b4-b78a-039bdb17e9d2",
   "metadata": {
    "papermill": {
     "duration": 0.016997,
     "end_time": "2022-10-07T19:35:28.524011",
     "exception": false,
     "start_time": "2022-10-07T19:35:28.507014",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Kpi extraction performance scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b48209ca-747e-4ef7-808b-bc4cee523f59",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-07T19:35:28.558152Z",
     "iopub.status.busy": "2022-10-07T19:35:28.557612Z",
     "iopub.status.idle": "2022-10-07T19:35:28.870962Z",
     "shell.execute_reply": "2022-10-07T19:35:28.870247Z"
    },
    "papermill": {
     "duration": 0.332646,
     "end_time": "2022-10-07T19:35:28.872821",
     "exception": false,
     "start_time": "2022-10-07T19:35:28.540175",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>version</th>\n",
       "      <th>data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>v2.0</td>\n",
       "      <td>{'title': 'VERBUND-Integrated-Annual-Report-20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>v2.0</td>\n",
       "      <td>{'title': 'FUGRO_JV2019_Clickable.pdf', 'parag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>v2.0</td>\n",
       "      <td>{'title': 'Enel SpA Annual Report 2019.pdf', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>v2.0</td>\n",
       "      <td>{'title': 'Sustainability Report 2014_EN.pdf',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>v2.0</td>\n",
       "      <td>{'title': 'RWE-annual-report-2019.pdf', 'parag...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  version                                               data\n",
       "0    v2.0  {'title': 'VERBUND-Integrated-Annual-Report-20...\n",
       "1    v2.0  {'title': 'FUGRO_JV2019_Clickable.pdf', 'parag...\n",
       "2    v2.0  {'title': 'Enel SpA Annual Report 2019.pdf', '...\n",
       "3    v2.0  {'title': 'Sustainability Report 2014_EN.pdf',...\n",
       "4    v2.0  {'title': 'RWE-annual-report-2019.pdf', 'parag..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_path = str(config.BASE_PROCESSED_DATA)+'/kpi_test_split.json'\n",
    "test_data = pd.read_json(test_data_path)\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e4c87559-9f58-47b3-b787-cb8ba9006636",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-07T19:35:28.907874Z",
     "iopub.status.busy": "2022-10-07T19:35:28.907243Z",
     "iopub.status.idle": "2022-10-07T19:35:29.055148Z",
     "shell.execute_reply": "2022-10-07T19:35:29.054447Z"
    },
    "papermill": {
     "duration": 0.167564,
     "end_time": "2022-10-07T19:35:29.057194",
     "exception": false,
     "start_time": "2022-10-07T19:35:28.889630",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>kpi_id</th>\n",
       "      <th>question</th>\n",
       "      <th>sectors</th>\n",
       "      <th>add_year</th>\n",
       "      <th>kpi_category</th>\n",
       "      <th>Unnamed: 5</th>\n",
       "      <th>Unnamed: 6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>What is the company name?</td>\n",
       "      <td>OG, CM, CU</td>\n",
       "      <td>False</td>\n",
       "      <td>TEXT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>In which year was the annual report or the sus...</td>\n",
       "      <td>OG, CM, CU</td>\n",
       "      <td>False</td>\n",
       "      <td>TEXT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>What is the total volume of proven and probabl...</td>\n",
       "      <td>OG</td>\n",
       "      <td>True</td>\n",
       "      <td>TEXT, TABLE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.1</td>\n",
       "      <td>What is the volume of estimated proven hydroca...</td>\n",
       "      <td>OG</td>\n",
       "      <td>True</td>\n",
       "      <td>TEXT, TABLE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.2</td>\n",
       "      <td>What is the volume of estimated probable hydro...</td>\n",
       "      <td>OG</td>\n",
       "      <td>True</td>\n",
       "      <td>TEXT, TABLE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   kpi_id                                           question     sectors  \\\n",
       "0     0.0                          What is the company name?  OG, CM, CU   \n",
       "1     1.0  In which year was the annual report or the sus...  OG, CM, CU   \n",
       "2     2.0  What is the total volume of proven and probabl...          OG   \n",
       "3     2.1  What is the volume of estimated proven hydroca...          OG   \n",
       "4     2.2  What is the volume of estimated probable hydro...          OG   \n",
       "\n",
       "   add_year kpi_category  Unnamed: 5  Unnamed: 6  \n",
       "0     False         TEXT         NaN         NaN  \n",
       "1     False         TEXT         NaN         NaN  \n",
       "2      True  TEXT, TABLE         NaN         NaN  \n",
       "3      True  TEXT, TABLE         NaN         NaN  \n",
       "4      True  TEXT, TABLE         NaN         NaN  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kpi_df = s3c.download_df_from_s3(\n",
    "    \"aicoe-osc-demo/kpi_mapping\",\n",
    "    \"kpi_mapping.csv\",\n",
    "    filetype=S3FileType.CSV,\n",
    "    header=0,\n",
    ")\n",
    "kpi_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eec03e15-c345-49ce-a5c5-ec44fb13805f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-07T19:35:29.092170Z",
     "iopub.status.busy": "2022-10-07T19:35:29.091637Z",
     "iopub.status.idle": "2022-10-07T19:35:41.473952Z",
     "shell.execute_reply": "2022-10-07T19:35:41.473033Z"
    },
    "papermill": {
     "duration": 12.402021,
     "end_time": "2022-10-07T19:35:41.476334",
     "exception": false,
     "start_time": "2022-10-07T19:35:29.074313",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/07/2022 19:35:29 - INFO - farm.utils -   device: cuda n_gpu: 1, distributed training: False, automatic mixed precision training: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/07/2022 19:35:37 - INFO - farm.modeling.adaptive_model -   Found files for loading 1 prediction heads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/07/2022 19:35:37 - WARNING - farm.modeling.prediction_head -   Some unused parameters are passed to the QuestionAnsweringHead. Might not be a problem. Params: {\"training\": false, \"num_labels\": 2, \"ph_output_type\": \"per_token_squad\", \"model_type\": \"span_classification\", \"label_tensor_name\": \"question_answering_label_ids\", \"label_list\": [\"start_token\", \"end_token\"], \"metric\": \"squad\", \"name\": \"QuestionAnsweringHead\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/07/2022 19:35:37 - INFO - farm.modeling.prediction_head -   Prediction head initialized with size [1024, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/07/2022 19:35:37 - INFO - farm.modeling.prediction_head -   Loading prediction head from /opt/app-root/data/models/KPI_EXTRACTION/prediction_head_0.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/07/2022 19:35:40 - INFO - farm.modeling.tokenization -   Loading tokenizer of type 'RobertaTokenizer'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/07/2022 19:35:41 - INFO - farm.data_handler.processor -   Initialized processor without tasks. Supply `metric` and `label_list` to the constructor for using the default task or add a custom task later via processor.add_task()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/07/2022 19:35:41 - INFO - farm.utils -   device: cuda n_gpu: 1, distributed training: False, automatic mixed precision training: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/07/2022 19:35:41 - INFO - farm.infer -   Got ya 7 parallel workers to do inference ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/07/2022 19:35:41 - INFO - farm.infer -    0    0    0    0    0    0    0 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/07/2022 19:35:41 - INFO - farm.infer -   /w\\  /w\\  /w\\  /w\\  /w\\  /w\\  /w\\\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/07/2022 19:35:41 - INFO - farm.infer -   /'\\  / \\  /'\\  /'\\  / \\  / \\  /'\\\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/07/2022 19:35:41 - INFO - farm.infer -               \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/07/2022 19:35:41 - WARNING - farm.infer -   QAInferencer always has task_type='question_answering' even if another value is provided to Inferencer.load() or QAInferencer()\n"
     ]
    }
   ],
   "source": [
    "model = QAInferencer.load(str(model_root)+'/KPI_EXTRACTION', batch_size=40, gpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "126f2fdd-6bc3-48b0-b29b-ca7355c7b0d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-07T19:35:41.514857Z",
     "iopub.status.busy": "2022-10-07T19:35:41.514105Z",
     "iopub.status.idle": "2022-10-07T19:39:47.830807Z",
     "shell.execute_reply": "2022-10-07T19:39:47.830093Z"
    },
    "papermill": {
     "duration": 246.337752,
     "end_time": "2022-10-07T19:39:47.832624",
     "exception": false,
     "start_time": "2022-10-07T19:35:41.494872",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:   0%|          | 0/11 [00:00<?, ? Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:   9%|▉         | 1/11 [00:00<00:07,  1.40 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  18%|█▊        | 2/11 [00:01<00:06,  1.49 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  27%|██▋       | 3/11 [00:01<00:05,  1.60 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  36%|███▋      | 4/11 [00:02<00:04,  1.65 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  45%|████▌     | 5/11 [00:03<00:03,  1.69 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  55%|█████▍    | 6/11 [00:03<00:02,  1.71 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  64%|██████▎   | 7/11 [00:04<00:02,  1.67 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  73%|███████▎  | 8/11 [00:04<00:01,  1.69 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  82%|████████▏ | 9/11 [00:05<00:01,  1.67 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  91%|█████████ | 10/11 [00:06<00:00,  1.62 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples: 100%|██████████| 11/11 [00:06<00:00,  2.06 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples: 100%|██████████| 11/11 [00:06<00:00,  1.74 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:   0%|          | 0/9 [00:00<?, ? Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  11%|█         | 1/9 [00:00<00:05,  1.55 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  22%|██▏       | 2/9 [00:01<00:04,  1.66 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  33%|███▎      | 3/9 [00:01<00:03,  1.70 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  44%|████▍     | 4/9 [00:02<00:02,  1.72 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  56%|█████▌    | 5/9 [00:02<00:02,  1.71 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  67%|██████▋   | 6/9 [00:03<00:01,  1.71 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  78%|███████▊  | 7/9 [00:04<00:01,  1.70 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  89%|████████▉ | 8/9 [00:04<00:00,  1.72 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples: 100%|██████████| 9/9 [00:05<00:00,  1.58 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples: 100%|██████████| 9/9 [00:05<00:00,  1.65 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:   0%|          | 0/8 [00:00<?, ? Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  12%|█▎        | 1/8 [00:00<00:03,  1.76 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  25%|██▌       | 2/8 [00:01<00:03,  1.76 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  38%|███▊      | 3/8 [00:01<00:02,  1.76 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  50%|█████     | 4/8 [00:02<00:02,  1.65 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  62%|██████▎   | 5/8 [00:02<00:01,  1.67 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  75%|███████▌  | 6/8 [00:03<00:01,  1.69 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  88%|████████▊ | 7/8 [00:04<00:00,  1.71 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples: 100%|██████████| 8/8 [00:04<00:00,  2.10 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples: 100%|██████████| 8/8 [00:04<00:00,  1.84 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:   0%|          | 0/14 [00:00<?, ? Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:   7%|▋         | 1/14 [00:00<00:07,  1.67 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  14%|█▍        | 2/14 [00:01<00:09,  1.28 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  21%|██▏       | 3/14 [00:02<00:07,  1.44 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  29%|██▊       | 4/14 [00:02<00:06,  1.57 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  36%|███▌      | 5/14 [00:03<00:05,  1.65 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  43%|████▎     | 6/14 [00:03<00:04,  1.70 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  50%|█████     | 7/14 [00:04<00:04,  1.73 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  57%|█████▋    | 8/14 [00:04<00:03,  1.75 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  64%|██████▍   | 9/14 [00:05<00:02,  1.77 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  71%|███████▏  | 10/14 [00:05<00:02,  1.78 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  79%|███████▊  | 11/14 [00:06<00:01,  1.79 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  86%|████████▌ | 12/14 [00:07<00:01,  1.79 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  93%|█████████▎| 13/14 [00:07<00:00,  1.79 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples: 100%|██████████| 14/14 [00:08<00:00,  1.81 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples: 100%|██████████| 14/14 [00:08<00:00,  1.71 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:   0%|          | 0/15 [00:00<?, ? Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:   7%|▋         | 1/15 [00:00<00:07,  1.81 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  13%|█▎        | 2/15 [00:01<00:07,  1.81 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  20%|██        | 3/15 [00:01<00:06,  1.81 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  27%|██▋       | 4/15 [00:02<00:06,  1.81 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  33%|███▎      | 5/15 [00:02<00:05,  1.80 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  40%|████      | 6/15 [00:03<00:04,  1.81 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  47%|████▋     | 7/15 [00:03<00:04,  1.81 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  53%|█████▎    | 8/15 [00:04<00:03,  1.80 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  60%|██████    | 9/15 [00:04<00:03,  1.80 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  67%|██████▋   | 10/15 [00:05<00:02,  1.80 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  73%|███████▎  | 11/15 [00:06<00:02,  1.80 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  80%|████████  | 12/15 [00:06<00:01,  1.80 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  87%|████████▋ | 13/15 [00:07<00:01,  1.80 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  93%|█████████▎| 14/15 [00:07<00:00,  1.80 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples: 100%|██████████| 15/15 [00:07<00:00,  1.93 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "10/07/2022 19:36:17 - ERROR - farm.modeling.predictions -   Both start and end offsets should be 0: \n",
      "6, 6 with a no_answer. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:   0%|          | 0/15 [00:00<?, ? Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:   7%|▋         | 1/15 [00:00<00:07,  1.81 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  13%|█▎        | 2/15 [00:01<00:07,  1.81 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  20%|██        | 3/15 [00:01<00:06,  1.80 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  27%|██▋       | 4/15 [00:02<00:06,  1.80 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  33%|███▎      | 5/15 [00:02<00:05,  1.79 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  40%|████      | 6/15 [00:03<00:05,  1.79 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  47%|████▋     | 7/15 [00:03<00:04,  1.79 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  53%|█████▎    | 8/15 [00:04<00:03,  1.79 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  60%|██████    | 9/15 [00:05<00:03,  1.80 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  67%|██████▋   | 10/15 [00:05<00:02,  1.80 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  73%|███████▎  | 11/15 [00:06<00:02,  1.79 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  80%|████████  | 12/15 [00:06<00:01,  1.79 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  87%|████████▋ | 13/15 [00:07<00:01,  1.79 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  93%|█████████▎| 14/15 [00:07<00:00,  1.79 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples: 100%|██████████| 15/15 [00:08<00:00,  2.08 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples: 100%|██████████| 15/15 [00:08<00:00,  1.85 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:   0%|          | 0/18 [00:00<?, ? Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:   6%|▌         | 1/18 [00:00<00:09,  1.81 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  11%|█         | 2/18 [00:01<00:08,  1.81 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  17%|█▋        | 3/18 [00:01<00:08,  1.80 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  22%|██▏       | 4/18 [00:02<00:07,  1.80 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  28%|██▊       | 5/18 [00:02<00:07,  1.80 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  33%|███▎      | 6/18 [00:03<00:06,  1.80 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  39%|███▉      | 7/18 [00:03<00:06,  1.80 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  44%|████▍     | 8/18 [00:04<00:05,  1.80 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  50%|█████     | 9/18 [00:05<00:05,  1.79 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  56%|█████▌    | 10/18 [00:05<00:04,  1.79 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  61%|██████    | 11/18 [00:06<00:03,  1.79 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  67%|██████▋   | 12/18 [00:06<00:03,  1.79 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  72%|███████▏  | 13/18 [00:07<00:02,  1.79 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  78%|███████▊  | 14/18 [00:07<00:02,  1.79 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  83%|████████▎ | 15/18 [00:08<00:01,  1.79 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  89%|████████▉ | 16/18 [00:08<00:01,  1.80 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  94%|█████████▍| 17/18 [00:09<00:00,  1.80 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples: 100%|██████████| 18/18 [00:09<00:00,  1.84 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples: 100%|██████████| 18/18 [00:09<00:00,  1.80 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:   0%|          | 0/9 [00:00<?, ? Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  11%|█         | 1/9 [00:00<00:04,  1.81 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  22%|██▏       | 2/9 [00:01<00:03,  1.81 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  33%|███▎      | 3/9 [00:01<00:03,  1.80 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  44%|████▍     | 4/9 [00:02<00:02,  1.80 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  56%|█████▌    | 5/9 [00:02<00:02,  1.80 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  67%|██████▋   | 6/9 [00:03<00:01,  1.81 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  78%|███████▊  | 7/9 [00:03<00:01,  1.81 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  89%|████████▉ | 8/9 [00:04<00:00,  1.80 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples: 100%|██████████| 9/9 [00:04<00:00,  2.07 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples: 100%|██████████| 9/9 [00:04<00:00,  1.89 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:   0%|          | 0/12 [00:00<?, ? Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:   8%|▊         | 1/12 [00:00<00:06,  1.80 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  17%|█▋        | 2/12 [00:01<00:05,  1.80 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  25%|██▌       | 3/12 [00:01<00:05,  1.80 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  33%|███▎      | 4/12 [00:02<00:04,  1.80 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  42%|████▏     | 5/12 [00:02<00:03,  1.80 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  50%|█████     | 6/12 [00:03<00:03,  1.80 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  58%|█████▊    | 7/12 [00:03<00:02,  1.80 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  67%|██████▋   | 8/12 [00:04<00:02,  1.80 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  75%|███████▌  | 9/12 [00:05<00:01,  1.80 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  83%|████████▎ | 10/12 [00:05<00:01,  1.80 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  92%|█████████▏| 11/12 [00:06<00:00,  1.79 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples: 100%|██████████| 12/12 [00:06<00:00,  1.88 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples: 100%|██████████| 12/12 [00:06<00:00,  1.82 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:   0%|          | 0/16 [00:00<?, ? Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:   6%|▋         | 1/16 [00:00<00:08,  1.79 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  12%|█▎        | 2/16 [00:01<00:07,  1.79 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  19%|█▉        | 3/16 [00:01<00:07,  1.79 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  25%|██▌       | 4/16 [00:02<00:06,  1.79 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  31%|███▏      | 5/16 [00:02<00:06,  1.79 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  38%|███▊      | 6/16 [00:03<00:05,  1.79 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  44%|████▍     | 7/16 [00:03<00:05,  1.79 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  50%|█████     | 8/16 [00:04<00:04,  1.79 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  56%|█████▋    | 9/16 [00:05<00:03,  1.79 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  62%|██████▎   | 10/16 [00:05<00:03,  1.78 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  69%|██████▉   | 11/16 [00:06<00:02,  1.78 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  75%|███████▌  | 12/16 [00:06<00:02,  1.78 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  81%|████████▏ | 13/16 [00:07<00:01,  1.78 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  88%|████████▊ | 14/16 [00:07<00:01,  1.78 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  94%|█████████▍| 15/16 [00:08<00:00,  1.78 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples: 100%|██████████| 16/16 [00:08<00:00,  2.15 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples: 100%|██████████| 16/16 [00:08<00:00,  1.85 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:   0%|          | 0/12 [00:00<?, ? Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:   8%|▊         | 1/12 [00:00<00:06,  1.79 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  17%|█▋        | 2/12 [00:01<00:05,  1.79 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  25%|██▌       | 3/12 [00:01<00:05,  1.78 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  33%|███▎      | 4/12 [00:02<00:04,  1.78 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  42%|████▏     | 5/12 [00:02<00:03,  1.78 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  50%|█████     | 6/12 [00:03<00:03,  1.78 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  58%|█████▊    | 7/12 [00:03<00:02,  1.78 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  67%|██████▋   | 8/12 [00:04<00:02,  1.78 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  75%|███████▌  | 9/12 [00:05<00:01,  1.78 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  83%|████████▎ | 10/12 [00:05<00:01,  1.79 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  92%|█████████▏| 11/12 [00:06<00:00,  1.78 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples: 100%|██████████| 12/12 [00:06<00:00,  1.96 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples: 100%|██████████| 12/12 [00:06<00:00,  1.83 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:   0%|          | 0/13 [00:00<?, ? Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:   8%|▊         | 1/13 [00:00<00:06,  1.78 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  15%|█▌        | 2/13 [00:01<00:06,  1.77 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  23%|██▎       | 3/13 [00:01<00:05,  1.78 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  31%|███       | 4/13 [00:02<00:05,  1.77 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  38%|███▊      | 5/13 [00:02<00:04,  1.77 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  46%|████▌     | 6/13 [00:03<00:03,  1.78 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  54%|█████▍    | 7/13 [00:03<00:03,  1.78 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  62%|██████▏   | 8/13 [00:04<00:02,  1.78 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  69%|██████▉   | 9/13 [00:05<00:02,  1.77 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  77%|███████▋  | 10/13 [00:05<00:01,  1.78 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  85%|████████▍ | 11/13 [00:06<00:01,  1.77 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  92%|█████████▏| 12/13 [00:06<00:00,  1.77 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples: 100%|██████████| 13/13 [00:07<00:00,  2.05 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples: 100%|██████████| 13/13 [00:07<00:00,  1.84 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:   0%|          | 0/10 [00:00<?, ? Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  10%|█         | 1/10 [00:00<00:05,  1.78 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  20%|██        | 2/10 [00:01<00:04,  1.78 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  30%|███       | 3/10 [00:01<00:03,  1.78 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  40%|████      | 4/10 [00:02<00:03,  1.78 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  50%|█████     | 5/10 [00:02<00:02,  1.79 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  60%|██████    | 6/10 [00:03<00:02,  1.79 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  70%|███████   | 7/10 [00:03<00:01,  1.78 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  80%|████████  | 8/10 [00:04<00:01,  1.78 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  90%|█████████ | 9/10 [00:05<00:00,  1.78 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples: 100%|██████████| 10/10 [00:05<00:00,  1.95 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:   0%|          | 0/12 [00:00<?, ? Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:   8%|▊         | 1/12 [00:00<00:06,  1.79 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  17%|█▋        | 2/12 [00:01<00:05,  1.78 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  25%|██▌       | 3/12 [00:01<00:05,  1.78 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  33%|███▎      | 4/12 [00:02<00:04,  1.78 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  42%|████▏     | 5/12 [00:02<00:03,  1.78 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  50%|█████     | 6/12 [00:03<00:03,  1.78 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  58%|█████▊    | 7/12 [00:03<00:02,  1.78 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  67%|██████▋   | 8/12 [00:04<00:02,  1.78 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  75%|███████▌  | 9/12 [00:05<00:01,  1.78 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  83%|████████▎ | 10/12 [00:05<00:01,  1.79 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  92%|█████████▏| 11/12 [00:06<00:00,  1.79 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples: 100%|██████████| 12/12 [00:06<00:00,  2.31 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples: 100%|██████████| 12/12 [00:06<00:00,  1.90 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:   0%|          | 0/9 [00:00<?, ? Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  11%|█         | 1/9 [00:00<00:04,  1.80 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  22%|██▏       | 2/9 [00:01<00:03,  1.79 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  33%|███▎      | 3/9 [00:01<00:03,  1.79 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  44%|████▍     | 4/9 [00:02<00:02,  1.79 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  56%|█████▌    | 5/9 [00:02<00:02,  1.79 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  67%|██████▋   | 6/9 [00:03<00:01,  1.79 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  78%|███████▊  | 7/9 [00:03<00:01,  1.79 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  89%|████████▉ | 8/9 [00:04<00:00,  1.79 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples: 100%|██████████| 9/9 [00:04<00:00,  1.87 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples: 100%|██████████| 9/9 [00:04<00:00,  1.82 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:   0%|          | 0/10 [00:00<?, ? Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  10%|█         | 1/10 [00:00<00:05,  1.79 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  20%|██        | 2/10 [00:01<00:04,  1.79 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  30%|███       | 3/10 [00:01<00:03,  1.78 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  40%|████      | 4/10 [00:02<00:03,  1.78 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  50%|█████     | 5/10 [00:02<00:02,  1.78 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  60%|██████    | 6/10 [00:03<00:02,  1.78 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  70%|███████   | 7/10 [00:03<00:01,  1.79 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  80%|████████  | 8/10 [00:04<00:01,  1.79 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  90%|█████████ | 9/10 [00:05<00:00,  1.78 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples: 100%|██████████| 10/10 [00:05<00:00,  1.96 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples: 100%|██████████| 10/10 [00:05<00:00,  1.84 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:   0%|          | 0/13 [00:00<?, ? Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:   8%|▊         | 1/13 [00:00<00:06,  1.77 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  15%|█▌        | 2/13 [00:01<00:06,  1.78 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  23%|██▎       | 3/13 [00:01<00:05,  1.78 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  31%|███       | 4/13 [00:02<00:05,  1.78 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  38%|███▊      | 5/13 [00:02<00:04,  1.78 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  46%|████▌     | 6/13 [00:03<00:03,  1.78 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  54%|█████▍    | 7/13 [00:03<00:03,  1.78 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  62%|██████▏   | 8/13 [00:04<00:02,  1.78 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  69%|██████▉   | 9/13 [00:05<00:02,  1.78 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  77%|███████▋  | 10/13 [00:05<00:01,  1.78 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  85%|████████▍ | 11/13 [00:06<00:01,  1.78 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  92%|█████████▏| 12/13 [00:06<00:00,  1.78 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples: 100%|██████████| 13/13 [00:07<00:00,  1.85 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples: 100%|██████████| 13/13 [00:07<00:00,  1.80 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:   0%|          | 0/15 [00:00<?, ? Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:   7%|▋         | 1/15 [00:00<00:07,  1.79 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  13%|█▎        | 2/15 [00:01<00:07,  1.78 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  20%|██        | 3/15 [00:01<00:06,  1.77 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  27%|██▋       | 4/15 [00:02<00:06,  1.77 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  33%|███▎      | 5/15 [00:02<00:05,  1.77 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  40%|████      | 6/15 [00:03<00:05,  1.77 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  47%|████▋     | 7/15 [00:03<00:04,  1.78 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  53%|█████▎    | 8/15 [00:04<00:03,  1.78 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  60%|██████    | 9/15 [00:05<00:03,  1.78 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  67%|██████▋   | 10/15 [00:05<00:02,  1.78 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  73%|███████▎  | 11/15 [00:06<00:02,  1.78 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  80%|████████  | 12/15 [00:06<00:01,  1.78 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  87%|████████▋ | 13/15 [00:07<00:01,  1.78 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  93%|█████████▎| 14/15 [00:07<00:00,  1.78 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples: 100%|██████████| 15/15 [00:07<00:00,  1.89 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:   0%|          | 0/16 [00:00<?, ? Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:   6%|▋         | 1/16 [00:00<00:08,  1.77 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  12%|█▎        | 2/16 [00:01<00:07,  1.78 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  19%|█▉        | 3/16 [00:01<00:07,  1.78 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  25%|██▌       | 4/16 [00:02<00:06,  1.78 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  31%|███▏      | 5/16 [00:02<00:06,  1.78 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  38%|███▊      | 6/16 [00:03<00:05,  1.78 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  44%|████▍     | 7/16 [00:03<00:05,  1.78 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  50%|█████     | 8/16 [00:04<00:04,  1.78 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  56%|█████▋    | 9/16 [00:05<00:03,  1.78 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  62%|██████▎   | 10/16 [00:05<00:03,  1.78 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  69%|██████▉   | 11/16 [00:06<00:02,  1.77 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  75%|███████▌  | 12/16 [00:06<00:02,  1.77 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  81%|████████▏ | 13/16 [00:07<00:01,  1.78 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  88%|████████▊ | 14/16 [00:07<00:01,  1.78 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  94%|█████████▍| 15/16 [00:08<00:00,  1.78 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples: 100%|██████████| 16/16 [00:08<00:00,  1.80 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples: 100%|██████████| 16/16 [00:08<00:00,  1.78 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:   0%|          | 0/11 [00:00<?, ? Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:   9%|▉         | 1/11 [00:00<00:05,  1.79 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  18%|█▊        | 2/11 [00:01<00:05,  1.79 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  27%|██▋       | 3/11 [00:01<00:04,  1.78 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  36%|███▋      | 4/11 [00:02<00:03,  1.78 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  45%|████▌     | 5/11 [00:02<00:03,  1.78 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  55%|█████▍    | 6/11 [00:03<00:02,  1.78 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  64%|██████▎   | 7/11 [00:03<00:02,  1.78 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  73%|███████▎  | 8/11 [00:04<00:01,  1.78 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  82%|████████▏ | 9/11 [00:05<00:01,  1.78 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  91%|█████████ | 10/11 [00:05<00:00,  1.78 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples: 100%|██████████| 11/11 [00:05<00:00,  1.94 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:   0%|          | 0/10 [00:00<?, ? Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  10%|█         | 1/10 [00:00<00:05,  1.78 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  20%|██        | 2/10 [00:01<00:04,  1.77 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  30%|███       | 3/10 [00:01<00:03,  1.77 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  40%|████      | 4/10 [00:02<00:03,  1.78 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  50%|█████     | 5/10 [00:02<00:02,  1.78 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  60%|██████    | 6/10 [00:03<00:02,  1.77 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  70%|███████   | 7/10 [00:03<00:01,  1.78 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  80%|████████  | 8/10 [00:04<00:01,  1.78 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  90%|█████████ | 9/10 [00:05<00:00,  1.78 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples: 100%|██████████| 10/10 [00:05<00:00,  1.89 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples: 100%|██████████| 10/10 [00:05<00:00,  1.81 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:   0%|          | 0/15 [00:00<?, ? Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:   7%|▋         | 1/15 [00:00<00:07,  1.78 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  13%|█▎        | 2/15 [00:01<00:07,  1.78 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  20%|██        | 3/15 [00:01<00:06,  1.78 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  27%|██▋       | 4/15 [00:02<00:06,  1.78 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  33%|███▎      | 5/15 [00:02<00:05,  1.77 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  40%|████      | 6/15 [00:03<00:05,  1.77 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  47%|████▋     | 7/15 [00:03<00:04,  1.77 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  53%|█████▎    | 8/15 [00:04<00:03,  1.77 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  60%|██████    | 9/15 [00:05<00:03,  1.77 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  67%|██████▋   | 10/15 [00:05<00:02,  1.78 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  73%|███████▎  | 11/15 [00:06<00:02,  1.78 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  80%|████████  | 12/15 [00:06<00:01,  1.77 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  87%|████████▋ | 13/15 [00:07<00:01,  1.76 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  93%|█████████▎| 14/15 [00:07<00:00,  1.76 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples: 100%|██████████| 15/15 [00:07<00:00,  1.89 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:   0%|          | 0/12 [00:00<?, ? Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:   8%|▊         | 1/12 [00:00<00:06,  1.78 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  17%|█▋        | 2/12 [00:01<00:05,  1.78 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  25%|██▌       | 3/12 [00:01<00:05,  1.78 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  33%|███▎      | 4/12 [00:02<00:04,  1.77 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  42%|████▏     | 5/12 [00:02<00:03,  1.78 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  50%|█████     | 6/12 [00:03<00:03,  1.77 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  58%|█████▊    | 7/12 [00:03<00:02,  1.77 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  67%|██████▋   | 8/12 [00:04<00:02,  1.76 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  75%|███████▌  | 9/12 [00:05<00:01,  1.76 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  83%|████████▎ | 10/12 [00:05<00:01,  1.77 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  92%|█████████▏| 11/12 [00:06<00:00,  1.77 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples: 100%|██████████| 12/12 [00:06<00:00,  1.85 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples: 100%|██████████| 12/12 [00:06<00:00,  1.79 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:   0%|          | 0/17 [00:00<?, ? Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:   6%|▌         | 1/17 [00:00<00:09,  1.78 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  12%|█▏        | 2/17 [00:01<00:08,  1.77 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  18%|█▊        | 3/17 [00:01<00:07,  1.77 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  24%|██▎       | 4/17 [00:02<00:07,  1.77 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  29%|██▉       | 5/17 [00:02<00:06,  1.77 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  35%|███▌      | 6/17 [00:03<00:06,  1.77 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  41%|████      | 7/17 [00:03<00:05,  1.77 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  47%|████▋     | 8/17 [00:04<00:05,  1.77 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  53%|█████▎    | 9/17 [00:05<00:04,  1.77 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  59%|█████▉    | 10/17 [00:05<00:03,  1.77 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  65%|██████▍   | 11/17 [00:06<00:03,  1.77 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  71%|███████   | 12/17 [00:06<00:02,  1.77 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  76%|███████▋  | 13/17 [00:07<00:02,  1.76 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  82%|████████▏ | 14/17 [00:07<00:01,  1.76 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  88%|████████▊ | 15/17 [00:08<00:01,  1.77 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  94%|█████████▍| 16/17 [00:09<00:00,  1.77 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples: 100%|██████████| 17/17 [00:09<00:00,  2.22 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples: 100%|██████████| 17/17 [00:09<00:00,  1.84 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:   0%|          | 0/14 [00:00<?, ? Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:   7%|▋         | 1/14 [00:00<00:07,  1.77 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  14%|█▍        | 2/14 [00:01<00:06,  1.78 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  21%|██▏       | 3/14 [00:01<00:06,  1.78 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  29%|██▊       | 4/14 [00:02<00:05,  1.78 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  36%|███▌      | 5/14 [00:02<00:05,  1.78 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  43%|████▎     | 6/14 [00:03<00:04,  1.78 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  50%|█████     | 7/14 [00:03<00:03,  1.77 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  57%|█████▋    | 8/14 [00:04<00:03,  1.77 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  64%|██████▍   | 9/14 [00:05<00:02,  1.76 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  71%|███████▏  | 10/14 [00:05<00:02,  1.77 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  79%|███████▊  | 11/14 [00:06<00:01,  1.77 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  86%|████████▌ | 12/14 [00:06<00:01,  1.76 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  93%|█████████▎| 13/14 [00:07<00:00,  1.77 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples: 100%|██████████| 14/14 [00:07<00:00,  1.89 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:   0%|          | 0/13 [00:00<?, ? Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:   8%|▊         | 1/13 [00:00<00:06,  1.77 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  15%|█▌        | 2/13 [00:01<00:06,  1.77 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  23%|██▎       | 3/13 [00:01<00:05,  1.77 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  31%|███       | 4/13 [00:02<00:05,  1.77 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  38%|███▊      | 5/13 [00:02<00:04,  1.77 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  46%|████▌     | 6/13 [00:03<00:03,  1.77 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  54%|█████▍    | 7/13 [00:03<00:03,  1.77 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  62%|██████▏   | 8/13 [00:04<00:02,  1.77 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  69%|██████▉   | 9/13 [00:05<00:02,  1.77 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  77%|███████▋  | 10/13 [00:05<00:01,  1.77 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  85%|████████▍ | 11/13 [00:06<00:01,  1.77 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  92%|█████████▏| 12/13 [00:06<00:00,  1.77 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples: 100%|██████████| 13/13 [00:07<00:00,  1.98 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples: 100%|██████████| 13/13 [00:07<00:00,  1.82 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:   0%|          | 0/10 [00:00<?, ? Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  10%|█         | 1/10 [00:00<00:05,  1.78 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  20%|██        | 2/10 [00:01<00:04,  1.78 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  30%|███       | 3/10 [00:01<00:03,  1.77 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  40%|████      | 4/10 [00:02<00:03,  1.77 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  50%|█████     | 5/10 [00:02<00:02,  1.77 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  60%|██████    | 6/10 [00:03<00:02,  1.77 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  70%|███████   | 7/10 [00:03<00:01,  1.77 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  80%|████████  | 8/10 [00:04<00:01,  1.77 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  90%|█████████ | 9/10 [00:05<00:00,  1.77 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples: 100%|██████████| 10/10 [00:05<00:00,  1.79 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples: 100%|██████████| 10/10 [00:05<00:00,  1.78 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:   0%|          | 0/9 [00:00<?, ? Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  11%|█         | 1/9 [00:00<00:04,  1.77 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  22%|██▏       | 2/9 [00:01<00:03,  1.78 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  33%|███▎      | 3/9 [00:01<00:03,  1.78 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  44%|████▍     | 4/9 [00:02<00:02,  1.78 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  56%|█████▌    | 5/9 [00:02<00:02,  1.78 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  67%|██████▋   | 6/9 [00:03<00:01,  1.78 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  78%|███████▊  | 7/9 [00:03<00:01,  1.78 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  89%|████████▉ | 8/9 [00:04<00:00,  1.78 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples: 100%|██████████| 9/9 [00:04<00:00,  2.17 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples: 100%|██████████| 9/9 [00:04<00:00,  1.90 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:   0%|          | 0/14 [00:00<?, ? Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:   7%|▋         | 1/14 [00:00<00:07,  1.78 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  14%|█▍        | 2/14 [00:01<00:06,  1.78 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  21%|██▏       | 3/14 [00:01<00:06,  1.77 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  29%|██▊       | 4/14 [00:02<00:05,  1.77 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  36%|███▌      | 5/14 [00:02<00:05,  1.77 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  43%|████▎     | 6/14 [00:03<00:04,  1.77 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  50%|█████     | 7/14 [00:03<00:03,  1.77 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  57%|█████▋    | 8/14 [00:04<00:03,  1.77 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  64%|██████▍   | 9/14 [00:05<00:02,  1.77 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  71%|███████▏  | 10/14 [00:05<00:02,  1.77 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  79%|███████▊  | 11/14 [00:06<00:01,  1.77 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  86%|████████▌ | 12/14 [00:06<00:01,  1.77 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  93%|█████████▎| 13/14 [00:07<00:00,  1.77 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples: 100%|██████████| 14/14 [00:07<00:00,  1.89 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:   0%|          | 0/14 [00:00<?, ? Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:   7%|▋         | 1/14 [00:00<00:07,  1.78 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  14%|█▍        | 2/14 [00:01<00:06,  1.78 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  21%|██▏       | 3/14 [00:01<00:06,  1.78 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  29%|██▊       | 4/14 [00:02<00:05,  1.77 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  36%|███▌      | 5/14 [00:02<00:05,  1.77 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  43%|████▎     | 6/14 [00:03<00:04,  1.77 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  50%|█████     | 7/14 [00:03<00:03,  1.77 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  57%|█████▋    | 8/14 [00:04<00:03,  1.78 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  64%|██████▍   | 9/14 [00:05<00:02,  1.78 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  71%|███████▏  | 10/14 [00:05<00:02,  1.77 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  79%|███████▊  | 11/14 [00:06<00:01,  1.77 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  86%|████████▌ | 12/14 [00:06<00:01,  1.76 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  93%|█████████▎| 13/14 [00:07<00:00,  1.76 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples: 100%|██████████| 14/14 [00:07<00:00,  1.88 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:   0%|          | 0/18 [00:00<?, ? Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:   6%|▌         | 1/18 [00:00<00:09,  1.77 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  11%|█         | 2/18 [00:01<00:09,  1.77 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  17%|█▋        | 3/18 [00:01<00:08,  1.77 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  22%|██▏       | 4/18 [00:02<00:07,  1.77 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  28%|██▊       | 5/18 [00:02<00:07,  1.77 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  33%|███▎      | 6/18 [00:03<00:06,  1.77 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  39%|███▉      | 7/18 [00:03<00:06,  1.77 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  44%|████▍     | 8/18 [00:04<00:05,  1.77 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  50%|█████     | 9/18 [00:05<00:05,  1.77 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  56%|█████▌    | 10/18 [00:05<00:04,  1.77 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  61%|██████    | 11/18 [00:06<00:03,  1.76 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  67%|██████▋   | 12/18 [00:06<00:03,  1.77 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  72%|███████▏  | 13/18 [00:07<00:02,  1.77 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  78%|███████▊  | 14/18 [00:07<00:02,  1.77 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  83%|████████▎ | 15/18 [00:08<00:01,  1.77 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  89%|████████▉ | 16/18 [00:09<00:01,  1.77 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  94%|█████████▍| 17/18 [00:09<00:00,  1.77 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples: 100%|██████████| 18/18 [00:09<00:00,  1.87 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:   0%|          | 0/14 [00:00<?, ? Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:   7%|▋         | 1/14 [00:00<00:07,  1.77 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  14%|█▍        | 2/14 [00:01<00:06,  1.77 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  21%|██▏       | 3/14 [00:01<00:06,  1.77 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  29%|██▊       | 4/14 [00:02<00:05,  1.77 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  36%|███▌      | 5/14 [00:02<00:05,  1.77 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  43%|████▎     | 6/14 [00:03<00:04,  1.77 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  50%|█████     | 7/14 [00:03<00:03,  1.77 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  57%|█████▋    | 8/14 [00:04<00:03,  1.77 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  64%|██████▍   | 9/14 [00:05<00:02,  1.76 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  71%|███████▏  | 10/14 [00:05<00:02,  1.77 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  79%|███████▊  | 11/14 [00:06<00:01,  1.77 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  86%|████████▌ | 12/14 [00:06<00:01,  1.76 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  93%|█████████▎| 13/14 [00:07<00:00,  1.77 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples: 100%|██████████| 14/14 [00:07<00:00,  2.28 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples: 100%|██████████| 14/14 [00:07<00:00,  1.87 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:   0%|          | 0/11 [00:00<?, ? Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:   9%|▉         | 1/11 [00:00<00:05,  1.77 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  18%|█▊        | 2/11 [00:01<00:05,  1.77 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  27%|██▋       | 3/11 [00:01<00:04,  1.77 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  36%|███▋      | 4/11 [00:02<00:03,  1.78 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  45%|████▌     | 5/11 [00:02<00:03,  1.78 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  55%|█████▍    | 6/11 [00:03<00:02,  1.77 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  64%|██████▎   | 7/11 [00:03<00:02,  1.77 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  73%|███████▎  | 8/11 [00:04<00:01,  1.77 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  82%|████████▏ | 9/11 [00:05<00:01,  1.77 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  91%|█████████ | 10/11 [00:05<00:00,  1.77 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples: 100%|██████████| 11/11 [00:06<00:00,  1.91 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples: 100%|██████████| 11/11 [00:06<00:00,  1.81 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:   0%|          | 0/15 [00:00<?, ? Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:   7%|▋         | 1/15 [00:00<00:07,  1.76 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  13%|█▎        | 2/15 [00:01<00:07,  1.77 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  20%|██        | 3/15 [00:01<00:06,  1.77 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  27%|██▋       | 4/15 [00:02<00:06,  1.77 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  33%|███▎      | 5/15 [00:02<00:05,  1.77 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  40%|████      | 6/15 [00:03<00:05,  1.77 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  47%|████▋     | 7/15 [00:03<00:04,  1.76 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  53%|█████▎    | 8/15 [00:04<00:03,  1.76 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  60%|██████    | 9/15 [00:05<00:03,  1.76 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  67%|██████▋   | 10/15 [00:05<00:02,  1.76 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  73%|███████▎  | 11/15 [00:06<00:02,  1.77 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  80%|████████  | 12/15 [00:06<00:01,  1.77 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  87%|████████▋ | 13/15 [00:07<00:01,  1.77 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  93%|█████████▎| 14/15 [00:07<00:00,  1.77 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples: 100%|██████████| 15/15 [00:08<00:00,  2.29 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples: 100%|██████████| 15/15 [00:08<00:00,  1.86 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:   0%|          | 0/11 [00:00<?, ? Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:   9%|▉         | 1/11 [00:00<00:05,  1.77 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  18%|█▊        | 2/11 [00:01<00:05,  1.77 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  27%|██▋       | 3/11 [00:01<00:04,  1.78 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  36%|███▋      | 4/11 [00:02<00:03,  1.78 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  45%|████▌     | 5/11 [00:02<00:03,  1.78 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  55%|█████▍    | 6/11 [00:03<00:02,  1.78 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  64%|██████▎   | 7/11 [00:03<00:02,  1.78 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  73%|███████▎  | 8/11 [00:04<00:01,  1.78 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  82%|████████▏ | 9/11 [00:05<00:01,  1.78 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples:  91%|█████████ | 10/11 [00:05<00:00,  1.78 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Inferencing Samples: 100%|██████████| 11/11 [00:05<00:00,  1.94 Batches/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/07/2022 19:39:47 - INFO - farm.data_handler.utils -   Written Squad predictions to: /opt/app-root/data/processed/preds.json\n"
     ]
    }
   ],
   "source": [
    "# run inference on validation dataset\n",
    "results = model.inference_from_file(\n",
    "    file=test_data_path,\n",
    "    return_json=False,\n",
    ")\n",
    "result_squad = [x.to_squad_eval() for x in results]\n",
    "\n",
    "preds_data_path = str(config.BASE_PROCESSED_DATA)+'/preds.json'\n",
    "\n",
    "write_squad_predictions(\n",
    "    predictions=result_squad,\n",
    "    predictions_filename=test_data_path,\n",
    "    out_filename=preds_data_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d2ebeca4-2d6b-443e-a61c-cb1d314cfc1a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-07T19:39:47.920479Z",
     "iopub.status.busy": "2022-10-07T19:39:47.920255Z",
     "iopub.status.idle": "2022-10-07T19:39:48.449932Z",
     "shell.execute_reply": "2022-10-07T19:39:48.449194Z"
    },
    "papermill": {
     "duration": 0.575234,
     "end_time": "2022-10-07T19:39:48.452141",
     "exception": false,
     "start_time": "2022-10-07T19:39:47.876907",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>exact</th>\n",
       "      <th>f1</th>\n",
       "      <th>total</th>\n",
       "      <th>HasAns_exact</th>\n",
       "      <th>HasAns_f1</th>\n",
       "      <th>HasAns_total</th>\n",
       "      <th>NoAns_exact</th>\n",
       "      <th>NoAns_f1</th>\n",
       "      <th>NoAns_total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>99.088272</td>\n",
       "      <td>99.218777</td>\n",
       "      <td>16891</td>\n",
       "      <td>45.357143</td>\n",
       "      <td>53.229875</td>\n",
       "      <td>280</td>\n",
       "      <td>99.99398</td>\n",
       "      <td>99.99398</td>\n",
       "      <td>16611</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       exact         f1  total  HasAns_exact  HasAns_f1  HasAns_total  \\\n",
       "0  99.088272  99.218777  16891     45.357143  53.229875           280   \n",
       "\n",
       "   NoAns_exact  NoAns_f1  NoAns_total  \n",
       "0     99.99398  99.99398        16611  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(test_data_path) as f:\n",
    "    dataset_json = json.load(f)\n",
    "    dataset = dataset_json['data']\n",
    "\n",
    "with open(preds_data_path) as f:\n",
    "    preds = json.load(f)\n",
    "\n",
    "# NOTE: in predictions, the keys are strings but need to be converted to ints\n",
    "# if we want to use squad evaluation file provided by farm\n",
    "preds = {int(k): v for k, v in preds.items()}\n",
    "\n",
    "na_probs = {k: 0.0 for k in preds}\n",
    "\n",
    "# maps qid to True/False\n",
    "qid_to_has_ans = squad_evaluation.make_qid_to_has_ans(dataset)\n",
    "has_ans_qids = [k for k, v in qid_to_has_ans.items() if v]\n",
    "no_ans_qids = [k for k, v in qid_to_has_ans.items() if not v]\n",
    "\n",
    "# get raw scores\n",
    "exact_raw, f1_raw = squad_evaluation.get_raw_scores_extended(dataset, preds)\n",
    "\n",
    "# apply thresholds\n",
    "exact_thresh = squad_evaluation.apply_no_ans_threshold(\n",
    "    exact_raw,\n",
    "    na_probs,\n",
    "    qid_to_has_ans,\n",
    "    1,\n",
    ")\n",
    "f1_thresh = squad_evaluation.apply_no_ans_threshold(\n",
    "    f1_raw,\n",
    "    na_probs,\n",
    "    qid_to_has_ans,\n",
    "    1,\n",
    ")\n",
    "\n",
    "# create results dict\n",
    "results_squad = squad_evaluation.make_eval_dict(exact_thresh, f1_thresh)\n",
    "if has_ans_qids:\n",
    "    has_ans_eval = squad_evaluation.make_eval_dict(exact_thresh, f1_thresh, qid_list=has_ans_qids)\n",
    "    squad_evaluation.merge_eval(results_squad, has_ans_eval, 'HasAns')\n",
    "if no_ans_qids:\n",
    "    no_ans_eval = squad_evaluation.make_eval_dict(exact_thresh, f1_thresh, qid_list=no_ans_qids)\n",
    "    squad_evaluation.merge_eval(results_squad, no_ans_eval, 'NoAns')\n",
    "\n",
    "# covert to df\n",
    "scores_df = pd.DataFrame(results_squad, index=[0])\n",
    "scores_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0c65a0-c542-46fe-8e8b-1a114e98fe33",
   "metadata": {
    "papermill": {
     "duration": 0.056394,
     "end_time": "2022-10-07T19:39:48.550803",
     "exception": false,
     "start_time": "2022-10-07T19:39:48.494409",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Conclusion\n",
    "The f1 scores for the relevance and the kpi extraction models were printed respectively. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 397.594238,
   "end_time": "2022-10-07T19:39:51.412655",
   "environment_variables": {},
   "exception": null,
   "input_path": "benchmarks_performance.ipynb",
   "output_path": "benchmarks_performance-output.ipynb",
   "parameters": {},
   "start_time": "2022-10-07T19:33:13.818417",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
