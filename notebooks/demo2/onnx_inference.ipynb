{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "529c927e-8d86-4b2f-ad43-38afe629eaf0",
   "metadata": {},
   "source": [
    "# Inference with ONNX runtime\n",
    "In this notebook, we explore Open Neural Network Exchange (ONNX) runtime and converting models in ONNX format. [ONNX Runtime](https://onnxruntime.ai/) is a cross-platform inference and training machine-learning accelerator. We are exploring that for quantization and for inferencing with smaller pruned models. For quantization, onnx supports conversion of floating point 32 values to int 8 values. We try that with our distilbert model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b362255a-d774-4236-9818-fb3d92eae7ba",
   "metadata": {
    "papermill": {
     "duration": 3.30074,
     "end_time": "2022-10-07T19:33:18.885511",
     "exception": false,
     "start_time": "2022-10-07T19:33:15.584771",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "from dotenv import load_dotenv\n",
    "from datasets import Dataset, DatasetDict\n",
    "import pandas as pd\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from src.data.s3_communication import S3Communication\n",
    "import config\n",
    "from transformers import AutoTokenizer\n",
    "from torch import cuda\n",
    "import transformers\n",
    "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "import transformers.convert_graph_to_onnx as onnx_convert\n",
    "from pathlib import Path\n",
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "from sklearn.metrics import recall_score, precision_score, f1_score, accuracy_score\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "local_model_path = '/opt/app-root/src/aicoe-osc-demo/models/transformers/RELEVANCE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e9424c3-db56-45ca-88bf-cfb9b2dbfd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load credentials\n",
    "dotenv_dir = os.environ.get(\n",
    "    \"CREDENTIAL_DOTENV_DIR\", os.environ.get(\"PWD\", \"/opt/app-root/src\")\n",
    ")\n",
    "dotenv_path = pathlib.Path(dotenv_dir) / \"credentials.env\"\n",
    "if os.path.exists(dotenv_path):\n",
    "    load_dotenv(dotenv_path=dotenv_path, override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa196757-de7b-4084-a311-5e8030227247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init s3 connector\n",
    "s3c = S3Communication(\n",
    "    s3_endpoint_url=os.getenv(\"S3_ENDPOINT\"),\n",
    "    aws_access_key_id=os.getenv(\"AWS_ACCESS_KEY_ID\"),\n",
    "    aws_secret_access_key=os.getenv(\"AWS_SECRET_ACCESS_KEY\"),\n",
    "    s3_bucket=os.getenv(\"S3_BUCKET\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbeadcd6-14c0-490e-9887-0aed8abf6a5e",
   "metadata": {
    "papermill": {
     "duration": 0.003808,
     "end_time": "2022-10-07T19:33:19.004776",
     "exception": false,
     "start_time": "2022-10-07T19:33:19.000968",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Retrieve the test dataset and the trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7321eaef-8d78-46a4-b2ff-f284b0ac919b",
   "metadata": {
    "papermill": {
     "duration": 0.886377,
     "end_time": "2022-10-07T19:33:19.895045",
     "exception": false,
     "start_time": "2022-10-07T19:33:19.008668",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3c.download_files_in_prefix_to_dir(\n",
    "    config.BASE_TRAIN_TEST_DATASET_S3_PREFIX,\n",
    "    config.BASE_PROCESSED_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d141c078-46c8-4394-8963-ab99b2487c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_path = str(config.BASE_PROCESSED_DATA)+'/rel_test_split.csv'\n",
    "test_data = pd.read_csv(test_data_path, index_col=0)\n",
    "test_data.rename(columns={'text': 'question', 'text_b':'sentence'}, inplace=True)\n",
    "\n",
    "train_data_path = str(config.BASE_PROCESSED_DATA)+'/rel_train_split.csv'\n",
    "train_data = pd.read_csv(train_data_path, index_col=0)\n",
    "train_data.rename(columns={'text': 'question', 'text_b':'sentence'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a01437c-9bf1-4878-914a-1eafad07ad73",
   "metadata": {},
   "outputs": [],
   "source": [
    "trds = Dataset.from_pandas(train_data)\n",
    "teds = Dataset.from_pandas(test_data.drop('label', axis=1))\n",
    "\n",
    "climate_dataset = DatasetDict()\n",
    "\n",
    "climate_dataset['train'] = trds\n",
    "climate_dataset['test'] = teds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f7f69b5b-3cd4-4b98-b18a-af7e8c765090",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batches(data_df, batch_size=32):\n",
    "    encoded_dataset = list()\n",
    "    batch = list()\n",
    "    for df, row in data_df.iterrows():\n",
    "        if len(batch) < batch_size:\n",
    "            batch.append([row['question'], row['sentence']])\n",
    "        else:\n",
    "            encoded_dataset.append(tokenizer(batch,\n",
    "                                             truncation=True,\n",
    "                                             return_tensors='pt',\n",
    "                                             padding=True))\n",
    "            batch = [[row['question'], row['sentence']]]\n",
    "\n",
    "    if batch:\n",
    "        encoded_dataset.append(tokenizer(batch,\n",
    "                                         truncation=True,\n",
    "                                         return_tensors='pt',\n",
    "                                         padding=True))\n",
    "    return encoded_dataset\n",
    "\n",
    "\n",
    "encoded_dataset = create_batches(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "99ebd886-6cbc-423f-ab45-7067acacfaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(local_model_path, use_fast=True)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(local_model_path).to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "73438af7-f3a9-4012-8a32-2d8696dcbf7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = transformers.pipeline(\"text-classification\",model=model,tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa73e86a-7590-4a79-855c-89e5f43099e5",
   "metadata": {},
   "source": [
    "# Convert to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f8ff14f9-72fe-4f38-8252-50030f7578d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using framework PyTorch: 1.6.0\n",
      "Found input input_ids with shape: {0: 'batch', 1: 'sequence'}\n",
      "Found input attention_mask with shape: {0: 'batch', 1: 'sequence'}\n",
      "Found output output_0 with shape: {0: 'batch'}\n",
      "Ensuring inputs are in correct order\n",
      "head_mask is not present in the generated input list.\n",
      "Generated inputs order: ['input_ids', 'attention_mask']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/app-root/lib64/python3.8/site-packages/transformers/models/distilbert/modeling_distilbert.py:122: TracerWarning: Converting a tensor to a Python index might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  position_ids = self.position_ids[:, :seq_length]\n",
      "/opt/app-root/lib64/python3.8/site-packages/transformers/models/distilbert/modeling_distilbert.py:213: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  mask, torch.tensor(torch.finfo(scores.dtype).min)\n"
     ]
    }
   ],
   "source": [
    "output_path = Path(\"onnx-model/classifier.onnx\")\n",
    "onnx_convert.convert_pytorch(pipeline, opset=11, output=output_path, use_external_format=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3e521f47-4829-4b3e-9e19-e0b50a195286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignore MatMul due to non constant B: /[MatMul_47]\n",
      "Ignore MatMul due to non constant B: /[MatMul_60]\n",
      "Ignore MatMul due to non constant B: /[MatMul_129]\n",
      "Ignore MatMul due to non constant B: /[MatMul_142]\n",
      "Ignore MatMul due to non constant B: /[MatMul_211]\n",
      "Ignore MatMul due to non constant B: /[MatMul_224]\n",
      "Ignore MatMul due to non constant B: /[MatMul_293]\n",
      "Ignore MatMul due to non constant B: /[MatMul_306]\n",
      "Ignore MatMul due to non constant B: /[MatMul_375]\n",
      "Ignore MatMul due to non constant B: /[MatMul_388]\n",
      "Ignore MatMul due to non constant B: /[MatMul_457]\n",
      "Ignore MatMul due to non constant B: /[MatMul_470]\n"
     ]
    }
   ],
   "source": [
    "output_int8_path = Path(\"onnx-model/classifier_int8.onnx\")\n",
    "quantize_dynamic(output_path, output_int8_path,\n",
    "                 weight_type=QuantType.QUInt8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ba383dff-dfb9-4dbf-8c70-d51470fb2a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = ort.InferenceSession(output_path.as_posix())\n",
    "session_int8 = ort.InferenceSession(output_int8_path.as_posix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "ad0bd434-42cb-4fc9-9907-a719d5adba92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(encoded_dataset):\n",
    "    out, out_int8 = list(), list()\n",
    "    for batch in encoded_dataset:\n",
    "        input_ids = batch['input_ids'].numpy()\n",
    "        attention_mask = batch['attention_mask'].numpy()\n",
    "        input_feed = {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "        }\n",
    "        out.extend(session.run(input_feed=input_feed, output_names=['output_0'])[0])\n",
    "        out_int8.extend(session_int8.run(input_feed=input_feed,  output_names=['output_0'])[0])\n",
    "    return out, out_int8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "683edf6c-2c13-4b78-9989-a8eaabbdfc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "out, out_int8 = predict(encoded_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c0af8ac0-b566-403e-b7de-94f18c637ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data[\"pred\"] = np.argmax(out, axis=-1)\n",
    "test_data[\"pred_int8\"] = np.argmax(out_int8, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "13f401d5-535b-4d44-8459-c44851c4fe93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def score(test_data, pred_type=\"pred\"):\n",
    "    #evalute performance\n",
    "    groups = test_data.groupby(\"question\")\n",
    "    scores = {}\n",
    "    for group, data in groups:\n",
    "        pred = data[pred_type]\n",
    "        true = data.label\n",
    "        scores[group] = {}\n",
    "        scores[group][\"accuracy\"] = accuracy_score(true, pred)\n",
    "        scores[group][\"f1_score\"] = f1_score(true, pred)\n",
    "        scores[group][\"recall_score\"] = recall_score(true, pred)\n",
    "        scores[group][\"precision_score\"] = precision_score(true, pred)\n",
    "        scores[group][\"support\"] = len(pred)\n",
    "\n",
    "    # kpi wise performance metrics\n",
    "    scores_df = pd.DataFrame(scores)\n",
    "    return scores_df.loc['f1_score'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "14285eed-3e55-4462-af6b-c4b9e3192bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/app-root/lib64/python3.8/site-packages/sklearn/metrics/_classification.py:1580: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/opt/app-root/lib64/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/app-root/lib64/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9179571080911388"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "f3013a8e-98e2-4596-b3e1-249a75ee7c8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/app-root/lib64/python3.8/site-packages/sklearn/metrics/_classification.py:1580: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/opt/app-root/lib64/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/app-root/lib64/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9068992132430389"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score(test_data, pred_type='pred_int8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a0035a-565a-4b5b-a1ae-d6beeb844fed",
   "metadata": {},
   "source": [
    "So we see here that the qunatized distilbert model gives a lesser f1 score of about 90.69% whereas the normal distilbert model gives 91.79%. However the model size of the quantized model is 65.1Mb compared to 255.4Mb of the original model. With losing 1.1% in f1 score, we get almost 4 times size compression."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
